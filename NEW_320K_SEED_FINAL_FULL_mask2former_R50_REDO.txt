==========================================
SLURM_JOB_ID = 2470938
SLURM_NODELIST = gnode070
SLURM_JOB_GPUS = 1
==========================================
08/06 02:16:35 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.23 (main, Jun  5 2025, 13:40:20) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 268722126
    GPU 0: NVIDIA GeForce RTX 2080 Ti
    CUDA_HOME: /opt/cuda-12.1/
    NVCC: Cuda compilation tools, release 12.1, V12.1.66
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.1.2
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.2
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: True
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 268722126
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

08/06 02:16:36 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=2, enable=False)
crop_size = (
    512,
    1024,
)
data_preprocessor = dict(
    bgr_to_rgb=True,
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    pad_val=0,
    seg_pad_val=255,
    size=(
        512,
        1024,
    ),
    std=[
        58.395,
        57.12,
        57.375,
    ],
    test_cfg=dict(size_divisor=32),
    type='SegDataPreProcessor')
data_root = '/scratch/seg_benchmark/splits_flat/'
dataset_type = 'YourDataset_BIG'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=40000,
        save_best='mIoU',
        type='CheckpointHook'),
    logger=dict(interval=50, log_metric_by_epoch=False, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'))
default_scope = 'mmseg'
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=True,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
img_ratios = [
    0.5,
    0.75,
    1.0,
    1.25,
    1.5,
    1.75,
]
img_suffix = '.jpg'
launcher = 'none'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=False)
model = dict(
    backbone=dict(
        deep_stem=False,
        depth=50,
        frozen_stages=-1,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=False, type='SyncBN'),
        num_stages=4,
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        style='pytorch',
        type='ResNet'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_val=0,
        seg_pad_val=255,
        size=(
            512,
            1024,
        ),
        std=[
            58.395,
            57.12,
            57.375,
        ],
        test_cfg=dict(size_divisor=32),
        type='SegDataPreProcessor'),
    decode_head=dict(
        align_corners=False,
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                0.1,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='mmdet.DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=True),
        num_classes=51,
        num_queries=100,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                init_cfg=None,
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=1024,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        im2col_step=64,
                        init_cfg=None,
                        norm_cfg=None,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            init_cfg=None,
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='mmdet.MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        train_cfg=dict(
            assigner=dict(
                match_costs=[
                    dict(type='mmdet.ClassificationCost', weight=2.0),
                    dict(
                        type='mmdet.CrossEntropyLossCost',
                        use_sigmoid=True,
                        weight=5.0),
                    dict(
                        eps=1.0,
                        pred_act=True,
                        type='mmdet.DiceCost',
                        weight=5.0),
                ],
                type='mmdet.HungarianAssigner'),
            importance_sample_ratio=0.75,
            num_points=12544,
            oversample_ratio=3.0,
            sampler=dict(type='mmdet.MaskPseudoSampler')),
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    attn_drop=0.0,
                    batch_first=True,
                    dropout_layer=None,
                    embed_dims=256,
                    num_heads=8,
                    proj_drop=0.0),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    add_identity=True,
                    dropout_layer=None,
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    attn_drop=0.0,
                    batch_first=True,
                    dropout_layer=None,
                    embed_dims=256,
                    num_heads=8,
                    proj_drop=0.0)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(mode='whole'),
    train_cfg=dict(),
    type='EncoderDecoder')
num_classes = 51
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(decay_mult=1.0, lr_mult=0.1),
            level_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_feat=dict(decay_mult=0.0, lr_mult=1.0)),
        norm_decay_mult=0.0),
    type='OptimWrapper')
optimizer = dict(
    betas=(
        0.9,
        0.999,
    ),
    eps=1e-08,
    lr=0.0001,
    type='AdamW',
    weight_decay=0.05)
param_scheduler = [
    dict(
        begin=0,
        by_epoch=False,
        end=320000,
        eta_min=0,
        power=0.9,
        type='PolyLR'),
]
randomness = dict(seed=268722126)
resume = False
seg_map_suffix = '.png'
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_prefix=dict(img_path='test/images', seg_map_path='test/masks'),
        data_root='/scratch/seg_benchmark/splits_flat/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                2048,
                1024,
            ), type='Resize'),
            dict(type='LoadAnnotations'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=8,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    classwise=True, iou_metrics=[
        'mIoU',
    ], type='IoUNanAbsent')
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        2048,
        1024,
    ), type='Resize'),
    dict(type='LoadAnnotations'),
    dict(type='PackSegInputs'),
]
train_cfg = dict(
    max_iters=320000, type='IterBasedTrainLoop', val_interval=40000)
train_dataloader = dict(
    batch_size=2,
    dataset=dict(
        data_prefix=dict(img_path='train/images', seg_map_path='train/masks'),
        data_root='/scratch/seg_benchmark/splits_flat/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(
                max_size=4096,
                resize_type='ResizeShortestEdge',
                scales=[
                    512,
                    614,
                    716,
                    819,
                    921,
                    1024,
                    1126,
                    1228,
                    1331,
                    1433,
                    1536,
                    1638,
                    1740,
                    1843,
                    1945,
                    2048,
                ],
                type='RandomChoiceResize'),
            dict(
                cat_max_ratio=0.75, crop_size=(
                    512,
                    1024,
                ), type='RandomCrop'),
            dict(prob=0.5, type='RandomFlip'),
            dict(type='PhotoMetricDistortion'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='InfiniteSampler'))
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(
        max_size=4096,
        resize_type='ResizeShortestEdge',
        scales=[
            512,
            614,
            716,
            819,
            921,
            1024,
            1126,
            1228,
            1331,
            1433,
            1536,
            1638,
            1740,
            1843,
            1945,
            2048,
        ],
        type='RandomChoiceResize'),
    dict(cat_max_ratio=0.75, crop_size=(
        512,
        1024,
    ), type='RandomCrop'),
    dict(prob=0.5, type='RandomFlip'),
    dict(type='PhotoMetricDistortion'),
    dict(type='PackSegInputs'),
]
tta_model = dict(type='SegTTAModel')
tta_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        transforms=[
            [
                dict(keep_ratio=True, scale_factor=0.5, type='Resize'),
                dict(keep_ratio=True, scale_factor=0.75, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.0, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.25, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.5, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.75, type='Resize'),
            ],
            [
                dict(direction='horizontal', prob=0.0, type='RandomFlip'),
                dict(direction='horizontal', prob=1.0, type='RandomFlip'),
            ],
            [
                dict(type='LoadAnnotations'),
            ],
            [
                dict(type='PackSegInputs'),
            ],
        ],
        type='TestTimeAug'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_prefix=dict(img_path='test/images', seg_map_path='test/masks'),
        data_root='/scratch/seg_benchmark/splits_flat/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                2048,
                1024,
            ), type='Resize'),
            dict(type='LoadAnnotations'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=8,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    iou_metrics=[
        'mIoU',
    ], type='IoUMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='SegLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = '/scratch/seg_benchmark/NEW/mask2former_R50_320K'

08/06 02:16:41 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
08/06 02:16:41 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.downsample.1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.downsample.1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.downsample.1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.downsample.1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.downsample.1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.downsample.1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.2.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.2.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.2.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.2.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.2.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.2.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.downsample.1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.downsample.1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr=0.0001
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr_mult=1.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:decay_mult=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr=0.0001
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr_mult=1.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:decay_mult=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr=0.0001
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr_mult=1.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:decay_mult=0.0
08/06 02:16:42 - mmengine - WARNING - The prefix is not set in metric class IoUMetric.
08/06 02:16:42 - mmengine - INFO - load model from: torchvision://resnet50
08/06 02:16:42 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
08/06 02:16:46 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

08/06 02:16:46 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
08/06 02:16:46 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
08/06 02:16:46 - mmengine - INFO - Checkpoints will be saved to /scratch/seg_benchmark/NEW/mask2former_R50_320K.
/home2/yasharora120/miniconda3/envs/mmseg/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400441250/work/aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
08/06 02:17:16 - mmengine - INFO - Iter(train) [    50/320000]  base_lr: 9.9986e-05 lr: 9.9986e-06  eta: 2 days, 5:08:50  time: 0.4298  data_time: 0.0092  memory: 10587  grad_norm: 183.2636  loss: 92.9535  decode.loss_cls: 4.0095  decode.loss_mask: 2.2549  decode.loss_dice: 3.9745  decode.d0.loss_cls: 7.9950  decode.d0.loss_mask: 1.6893  decode.d0.loss_dice: 3.3309  decode.d1.loss_cls: 3.5442  decode.d1.loss_mask: 1.6469  decode.d1.loss_dice: 3.2318  decode.d2.loss_cls: 3.4345  decode.d2.loss_mask: 1.6396  decode.d2.loss_dice: 3.2074  decode.d3.loss_cls: 3.3822  decode.d3.loss_mask: 1.7013  decode.d3.loss_dice: 3.2107  decode.d4.loss_cls: 3.3065  decode.d4.loss_mask: 1.6747  decode.d4.loss_dice: 3.2904  decode.d5.loss_cls: 3.4944  decode.d5.loss_mask: 1.7029  decode.d5.loss_dice: 3.3384  decode.d6.loss_cls: 3.5047  decode.d6.loss_mask: 1.8024  decode.d6.loss_dice: 3.4708  decode.d7.loss_cls: 3.6863  decode.d7.loss_mask: 1.8941  decode.d7.loss_dice: 3.7199  decode.d8.loss_cls: 3.8917  decode.d8.loss_mask: 2.0631  decode.d8.loss_dice: 3.8607
08/06 02:17:37 - mmengine - INFO - Iter(train) [   100/320000]  base_lr: 9.9972e-05 lr: 9.9972e-06  eta: 1 day, 21:41:58  time: 0.4312  data_time: 0.0091  memory: 5260  grad_norm: 215.9784  loss: 71.3652  decode.loss_cls: 2.9681  decode.loss_mask: 1.3867  decode.loss_dice: 2.6143  decode.d0.loss_cls: 7.7737  decode.d0.loss_mask: 1.2754  decode.d0.loss_dice: 2.6706  decode.d1.loss_cls: 3.0295  decode.d1.loss_mask: 1.2729  decode.d1.loss_dice: 2.4263  decode.d2.loss_cls: 2.9109  decode.d2.loss_mask: 1.2391  decode.d2.loss_dice: 2.3386  decode.d3.loss_cls: 2.8028  decode.d3.loss_mask: 1.2924  decode.d3.loss_dice: 2.4304  decode.d4.loss_cls: 2.7899  decode.d4.loss_mask: 1.3313  decode.d4.loss_dice: 2.4120  decode.d5.loss_cls: 2.7976  decode.d5.loss_mask: 1.3130  decode.d5.loss_dice: 2.4458  decode.d6.loss_cls: 2.7010  decode.d6.loss_mask: 1.3893  decode.d6.loss_dice: 2.4347  decode.d7.loss_cls: 2.7975  decode.d7.loss_mask: 1.3624  decode.d7.loss_dice: 2.4555  decode.d8.loss_cls: 2.9194  decode.d8.loss_mask: 1.2798  decode.d8.loss_dice: 2.5041
08/06 02:17:59 - mmengine - INFO - Iter(train) [   150/320000]  base_lr: 9.9958e-05 lr: 9.9958e-06  eta: 1 day, 19:15:53  time: 0.4332  data_time: 0.0091  memory: 5260  grad_norm: 207.9954  loss: 61.3996  decode.loss_cls: 2.8618  decode.loss_mask: 1.1160  decode.loss_dice: 1.8397  decode.d0.loss_cls: 7.6095  decode.d0.loss_mask: 1.1976  decode.d0.loss_dice: 2.1397  decode.d1.loss_cls: 2.8133  decode.d1.loss_mask: 1.0924  decode.d1.loss_dice: 1.7774  decode.d2.loss_cls: 2.7379  decode.d2.loss_mask: 1.0689  decode.d2.loss_dice: 1.7101  decode.d3.loss_cls: 2.7320  decode.d3.loss_mask: 1.0504  decode.d3.loss_dice: 1.7103  decode.d4.loss_cls: 2.8426  decode.d4.loss_mask: 1.0488  decode.d4.loss_dice: 1.6752  decode.d5.loss_cls: 2.7305  decode.d5.loss_mask: 1.0342  decode.d5.loss_dice: 1.6862  decode.d6.loss_cls: 2.7602  decode.d6.loss_mask: 1.0882  decode.d6.loss_dice: 1.7240  decode.d7.loss_cls: 2.7922  decode.d7.loss_mask: 1.0864  decode.d7.loss_dice: 1.7569  decode.d8.loss_cls: 2.8668  decode.d8.loss_mask: 1.0805  decode.d8.loss_dice: 1.7700
08/06 02:18:20 - mmengine - INFO - Iter(train) [   200/320000]  base_lr: 9.9944e-05 lr: 9.9944e-06  eta: 1 day, 18:03:37  time: 0.4328  data_time: 0.0089  memory: 5240  grad_norm: 327.2682  loss: 56.1046  decode.loss_cls: 2.6932  decode.loss_mask: 1.1377  decode.loss_dice: 1.4273  decode.d0.loss_cls: 7.4872  decode.d0.loss_mask: 1.0732  decode.d0.loss_dice: 1.8283  decode.d1.loss_cls: 2.7273  decode.d1.loss_mask: 1.0404  decode.d1.loss_dice: 1.3811  decode.d2.loss_cls: 2.7731  decode.d2.loss_mask: 0.9285  decode.d2.loss_dice: 1.2679  decode.d3.loss_cls: 2.6820  decode.d3.loss_mask: 0.9399  decode.d3.loss_dice: 1.3039  decode.d4.loss_cls: 2.7278  decode.d4.loss_mask: 0.9964  decode.d4.loss_dice: 1.2987  decode.d5.loss_cls: 2.7129  decode.d5.loss_mask: 0.9965  decode.d5.loss_dice: 1.2972  decode.d6.loss_cls: 2.6895  decode.d6.loss_mask: 0.9868  decode.d6.loss_dice: 1.3194  decode.d7.loss_cls: 2.7328  decode.d7.loss_mask: 1.0304  decode.d7.loss_dice: 1.3704  decode.d8.loss_cls: 2.6728  decode.d8.loss_mask: 1.1481  decode.d8.loss_dice: 1.4339
08/06 02:18:42 - mmengine - INFO - Iter(train) [   250/320000]  base_lr: 9.9930e-05 lr: 9.9930e-06  eta: 1 day, 17:20:44  time: 0.4340  data_time: 0.0089  memory: 5242  grad_norm: 232.7529  loss: 56.0446  decode.loss_cls: 2.8953  decode.loss_mask: 0.9571  decode.loss_dice: 1.4374  decode.d0.loss_cls: 7.4452  decode.d0.loss_mask: 0.9181  decode.d0.loss_dice: 1.8180  decode.d1.loss_cls: 2.9368  decode.d1.loss_mask: 0.8344  decode.d1.loss_dice: 1.4301  decode.d2.loss_cls: 2.8264  decode.d2.loss_mask: 0.8098  decode.d2.loss_dice: 1.3361  decode.d3.loss_cls: 2.8802  decode.d3.loss_mask: 0.7894  decode.d3.loss_dice: 1.3366  decode.d4.loss_cls: 2.8217  decode.d4.loss_mask: 0.8263  decode.d4.loss_dice: 1.3365  decode.d5.loss_cls: 2.8927  decode.d5.loss_mask: 0.8111  decode.d5.loss_dice: 1.3257  decode.d6.loss_cls: 2.8508  decode.d6.loss_mask: 0.8623  decode.d6.loss_dice: 1.3273  decode.d7.loss_cls: 2.8742  decode.d7.loss_mask: 0.9003  decode.d7.loss_dice: 1.3541  decode.d8.loss_cls: 2.8743  decode.d8.loss_mask: 0.9522  decode.d8.loss_dice: 1.3841
08/06 02:19:04 - mmengine - INFO - Iter(train) [   300/320000]  base_lr: 9.9916e-05 lr: 9.9916e-06  eta: 1 day, 16:52:23  time: 0.4336  data_time: 0.0089  memory: 5260  grad_norm: 215.9315  loss: 48.5751  decode.loss_cls: 2.4033  decode.loss_mask: 0.9940  decode.loss_dice: 1.1254  decode.d0.loss_cls: 7.2143  decode.d0.loss_mask: 0.9395  decode.d0.loss_dice: 1.4419  decode.d1.loss_cls: 2.4498  decode.d1.loss_mask: 0.9232  decode.d1.loss_dice: 1.1072  decode.d2.loss_cls: 2.4183  decode.d2.loss_mask: 0.8796  decode.d2.loss_dice: 0.9888  decode.d3.loss_cls: 2.3668  decode.d3.loss_mask: 0.8663  decode.d3.loss_dice: 0.9776  decode.d4.loss_cls: 2.4288  decode.d4.loss_mask: 0.8135  decode.d4.loss_dice: 0.9171  decode.d5.loss_cls: 2.4691  decode.d5.loss_mask: 0.8479  decode.d5.loss_dice: 0.9691  decode.d6.loss_cls: 2.4191  decode.d6.loss_mask: 0.8615  decode.d6.loss_dice: 0.9662  decode.d7.loss_cls: 2.4396  decode.d7.loss_mask: 0.8729  decode.d7.loss_dice: 1.0304  decode.d8.loss_cls: 2.4398  decode.d8.loss_mask: 0.9621  decode.d8.loss_dice: 1.0420
08/06 02:19:26 - mmengine - INFO - Iter(train) [   350/320000]  base_lr: 9.9902e-05 lr: 9.9902e-06  eta: 1 day, 16:32:54  time: 0.4356  data_time: 0.0091  memory: 5240  grad_norm: 182.9954  loss: 45.7380  decode.loss_cls: 2.5844  decode.loss_mask: 0.6825  decode.loss_dice: 0.9401  decode.d0.loss_cls: 6.9975  decode.d0.loss_mask: 0.8084  decode.d0.loss_dice: 1.3737  decode.d1.loss_cls: 2.5919  decode.d1.loss_mask: 0.6437  decode.d1.loss_dice: 1.0046  decode.d2.loss_cls: 2.4930  decode.d2.loss_mask: 0.6133  decode.d2.loss_dice: 0.9224  decode.d3.loss_cls: 2.5312  decode.d3.loss_mask: 0.6084  decode.d3.loss_dice: 0.8996  decode.d4.loss_cls: 2.6254  decode.d4.loss_mask: 0.5850  decode.d4.loss_dice: 0.8600  decode.d5.loss_cls: 2.5525  decode.d5.loss_mask: 0.6046  decode.d5.loss_dice: 0.8392  decode.d6.loss_cls: 2.5647  decode.d6.loss_mask: 0.5748  decode.d6.loss_dice: 0.8335  decode.d7.loss_cls: 2.5378  decode.d7.loss_mask: 0.5742  decode.d7.loss_dice: 0.8348  decode.d8.loss_cls: 2.5135  decode.d8.loss_mask: 0.6311  decode.d8.loss_dice: 0.9119
08/06 02:19:47 - mmengine - INFO - Iter(train) [   400/320000]  base_lr: 9.9888e-05 lr: 9.9888e-06  eta: 1 day, 16:18:16  time: 0.4356  data_time: 0.0090  memory: 5260  grad_norm: 222.2104  loss: 44.7097  decode.loss_cls: 2.5530  decode.loss_mask: 0.6699  decode.loss_dice: 0.9060  decode.d0.loss_cls: 6.8951  decode.d0.loss_mask: 0.7707  decode.d0.loss_dice: 1.2566  decode.d1.loss_cls: 2.2706  decode.d1.loss_mask: 0.7307  decode.d1.loss_dice: 1.0400  decode.d2.loss_cls: 2.2823  decode.d2.loss_mask: 0.7280  decode.d2.loss_dice: 0.9051  decode.d3.loss_cls: 2.2815  decode.d3.loss_mask: 0.6778  decode.d3.loss_dice: 0.8909  decode.d4.loss_cls: 2.2974  decode.d4.loss_mask: 0.6684  decode.d4.loss_dice: 0.8856  decode.d5.loss_cls: 2.3571  decode.d5.loss_mask: 0.6423  decode.d5.loss_dice: 0.8809  decode.d6.loss_cls: 2.4441  decode.d6.loss_mask: 0.6558  decode.d6.loss_dice: 0.8729  decode.d7.loss_cls: 2.4548  decode.d7.loss_mask: 0.6698  decode.d7.loss_dice: 0.8970  decode.d8.loss_cls: 2.5243  decode.d8.loss_mask: 0.6924  decode.d8.loss_dice: 0.9084
08/06 02:20:09 - mmengine - INFO - Iter(train) [   450/320000]  base_lr: 9.9874e-05 lr: 9.9874e-06  eta: 1 day, 16:06:49  time: 0.4351  data_time: 0.0091  memory: 5242  grad_norm: 246.9214  loss: 42.9736  decode.loss_cls: 2.3538  decode.loss_mask: 0.6646  decode.loss_dice: 0.7871  decode.d0.loss_cls: 6.7473  decode.d0.loss_mask: 0.7802  decode.d0.loss_dice: 1.1377  decode.d1.loss_cls: 2.3159  decode.d1.loss_mask: 0.6882  decode.d1.loss_dice: 0.8548  decode.d2.loss_cls: 2.3369  decode.d2.loss_mask: 0.7009  decode.d2.loss_dice: 0.7915  decode.d3.loss_cls: 2.3135  decode.d3.loss_mask: 0.6891  decode.d3.loss_dice: 0.7914  decode.d4.loss_cls: 2.3250  decode.d4.loss_mask: 0.6652  decode.d4.loss_dice: 0.7368  decode.d5.loss_cls: 2.3030  decode.d5.loss_mask: 0.6752  decode.d5.loss_dice: 0.8018  decode.d6.loss_cls: 2.3544  decode.d6.loss_mask: 0.6397  decode.d6.loss_dice: 0.7861  decode.d7.loss_cls: 2.3319  decode.d7.loss_mask: 0.7142  decode.d7.loss_dice: 0.8269  decode.d8.loss_cls: 2.3822  decode.d8.loss_mask: 0.6403  decode.d8.loss_dice: 0.8381
08/06 02:20:31 - mmengine - INFO - Iter(train) [   500/320000]  base_lr: 9.9860e-05 lr: 9.9860e-06  eta: 1 day, 15:58:00  time: 0.4338  data_time: 0.0091  memory: 5240  grad_norm: 222.3095  loss: 38.5922  decode.loss_cls: 2.0201  decode.loss_mask: 0.6720  decode.loss_dice: 0.6671  decode.d0.loss_cls: 6.5202  decode.d0.loss_mask: 0.6838  decode.d0.loss_dice: 0.9248  decode.d1.loss_cls: 2.1407  decode.d1.loss_mask: 0.6789  decode.d1.loss_dice: 0.7193  decode.d2.loss_cls: 1.9956  decode.d2.loss_mask: 0.6994  decode.d2.loss_dice: 0.6898  decode.d3.loss_cls: 2.0305  decode.d3.loss_mask: 0.6568  decode.d3.loss_dice: 0.6559  decode.d4.loss_cls: 2.0103  decode.d4.loss_mask: 0.6386  decode.d4.loss_dice: 0.6696  decode.d5.loss_cls: 2.0159  decode.d5.loss_mask: 0.6390  decode.d5.loss_dice: 0.6930  decode.d6.loss_cls: 2.0705  decode.d6.loss_mask: 0.6859  decode.d6.loss_dice: 0.6870  decode.d7.loss_cls: 2.0223  decode.d7.loss_mask: 0.6527  decode.d7.loss_dice: 0.6625  decode.d8.loss_cls: 2.0794  decode.d8.loss_mask: 0.6687  decode.d8.loss_dice: 0.6418
08/06 02:20:53 - mmengine - INFO - Iter(train) [   550/320000]  base_lr: 9.9846e-05 lr: 9.9846e-06  eta: 1 day, 15:50:15  time: 0.4351  data_time: 0.0092  memory: 5242  grad_norm: 196.3281  loss: 39.6669  decode.loss_cls: 2.1541  decode.loss_mask: 0.5553  decode.loss_dice: 0.7230  decode.d0.loss_cls: 6.4388  decode.d0.loss_mask: 0.6153  decode.d0.loss_dice: 0.9351  decode.d1.loss_cls: 2.3242  decode.d1.loss_mask: 0.5643  decode.d1.loss_dice: 0.7141  decode.d2.loss_cls: 2.2910  decode.d2.loss_mask: 0.5139  decode.d2.loss_dice: 0.6422  decode.d3.loss_cls: 2.2927  decode.d3.loss_mask: 0.4884  decode.d3.loss_dice: 0.5899  decode.d4.loss_cls: 2.2336  decode.d4.loss_mask: 0.5251  decode.d4.loss_dice: 0.6612  decode.d5.loss_cls: 2.2904  decode.d5.loss_mask: 0.5835  decode.d5.loss_dice: 0.7987  decode.d6.loss_cls: 2.2725  decode.d6.loss_mask: 0.5874  decode.d6.loss_dice: 0.7690  decode.d7.loss_cls: 2.2722  decode.d7.loss_mask: 0.5686  decode.d7.loss_dice: 0.7431  decode.d8.loss_cls: 2.2186  decode.d8.loss_mask: 0.5655  decode.d8.loss_dice: 0.7353
08/06 02:21:14 - mmengine - INFO - Iter(train) [   600/320000]  base_lr: 9.9832e-05 lr: 9.9832e-06  eta: 1 day, 15:43:53  time: 0.4350  data_time: 0.0090  memory: 5224  grad_norm: 154.8336  loss: 37.7889  decode.loss_cls: 2.1964  decode.loss_mask: 0.5243  decode.loss_dice: 0.5378  decode.d0.loss_cls: 6.3210  decode.d0.loss_mask: 0.6327  decode.d0.loss_dice: 0.7749  decode.d1.loss_cls: 2.3479  decode.d1.loss_mask: 0.5788  decode.d1.loss_dice: 0.6060  decode.d2.loss_cls: 2.2972  decode.d2.loss_mask: 0.5216  decode.d2.loss_dice: 0.5272  decode.d3.loss_cls: 2.2872  decode.d3.loss_mask: 0.5118  decode.d3.loss_dice: 0.4823  decode.d4.loss_cls: 2.3233  decode.d4.loss_mask: 0.5270  decode.d4.loss_dice: 0.5098  decode.d5.loss_cls: 2.3088  decode.d5.loss_mask: 0.5222  decode.d5.loss_dice: 0.5318  decode.d6.loss_cls: 2.2552  decode.d6.loss_mask: 0.5093  decode.d6.loss_dice: 0.5225  decode.d7.loss_cls: 2.2471  decode.d7.loss_mask: 0.5275  decode.d7.loss_dice: 0.5420  decode.d8.loss_cls: 2.2374  decode.d8.loss_mask: 0.5351  decode.d8.loss_dice: 0.5427
08/06 02:21:36 - mmengine - INFO - Iter(train) [   650/320000]  base_lr: 9.9817e-05 lr: 9.9817e-06  eta: 1 day, 15:38:27  time: 0.4362  data_time: 0.0090  memory: 5260  grad_norm: 171.3614  loss: 36.2365  decode.loss_cls: 2.1068  decode.loss_mask: 0.4905  decode.loss_dice: 0.6159  decode.d0.loss_cls: 6.1561  decode.d0.loss_mask: 0.5338  decode.d0.loss_dice: 0.8613  decode.d1.loss_cls: 2.0378  decode.d1.loss_mask: 0.4779  decode.d1.loss_dice: 0.6196  decode.d2.loss_cls: 2.0800  decode.d2.loss_mask: 0.4896  decode.d2.loss_dice: 0.6053  decode.d3.loss_cls: 2.0044  decode.d3.loss_mask: 0.4594  decode.d3.loss_dice: 0.6096  decode.d4.loss_cls: 2.0427  decode.d4.loss_mask: 0.4647  decode.d4.loss_dice: 0.5934  decode.d5.loss_cls: 2.0410  decode.d5.loss_mask: 0.4676  decode.d5.loss_dice: 0.6611  decode.d6.loss_cls: 2.0875  decode.d6.loss_mask: 0.4867  decode.d6.loss_dice: 0.6736  decode.d7.loss_cls: 2.1637  decode.d7.loss_mask: 0.4674  decode.d7.loss_dice: 0.6399  decode.d8.loss_cls: 2.1439  decode.d8.loss_mask: 0.5034  decode.d8.loss_dice: 0.6519
08/06 02:21:58 - mmengine - INFO - Iter(train) [   700/320000]  base_lr: 9.9803e-05 lr: 9.9803e-06  eta: 1 day, 15:33:39  time: 0.4356  data_time: 0.0089  memory: 5258  grad_norm: 220.4612  loss: 35.8167  decode.loss_cls: 1.8655  decode.loss_mask: 0.6378  decode.loss_dice: 0.6827  decode.d0.loss_cls: 6.1555  decode.d0.loss_mask: 0.7346  decode.d0.loss_dice: 0.9269  decode.d1.loss_cls: 2.1187  decode.d1.loss_mask: 0.5765  decode.d1.loss_dice: 0.6335  decode.d2.loss_cls: 1.9362  decode.d2.loss_mask: 0.5492  decode.d2.loss_dice: 0.6070  decode.d3.loss_cls: 1.8908  decode.d3.loss_mask: 0.5246  decode.d3.loss_dice: 0.5723  decode.d4.loss_cls: 1.8860  decode.d4.loss_mask: 0.5429  decode.d4.loss_dice: 0.5334  decode.d5.loss_cls: 1.8957  decode.d5.loss_mask: 0.5776  decode.d5.loss_dice: 0.5956  decode.d6.loss_cls: 1.9038  decode.d6.loss_mask: 0.6375  decode.d6.loss_dice: 0.6371  decode.d7.loss_cls: 1.9430  decode.d7.loss_mask: 0.5751  decode.d7.loss_dice: 0.5928  decode.d8.loss_cls: 1.9016  decode.d8.loss_mask: 0.5727  decode.d8.loss_dice: 0.6103
08/06 02:22:20 - mmengine - INFO - Iter(train) [   750/320000]  base_lr: 9.9789e-05 lr: 9.9789e-06  eta: 1 day, 15:29:41  time: 0.4365  data_time: 0.0091  memory: 5242  grad_norm: 199.2333  loss: 33.4879  decode.loss_cls: 1.9464  decode.loss_mask: 0.4901  decode.loss_dice: 0.5218  decode.d0.loss_cls: 5.8628  decode.d0.loss_mask: 0.5100  decode.d0.loss_dice: 0.7506  decode.d1.loss_cls: 2.0548  decode.d1.loss_mask: 0.4669  decode.d1.loss_dice: 0.5365  decode.d2.loss_cls: 1.9260  decode.d2.loss_mask: 0.4388  decode.d2.loss_dice: 0.4891  decode.d3.loss_cls: 1.9664  decode.d3.loss_mask: 0.4372  decode.d3.loss_dice: 0.4801  decode.d4.loss_cls: 1.9393  decode.d4.loss_mask: 0.4624  decode.d4.loss_dice: 0.5106  decode.d5.loss_cls: 2.0047  decode.d5.loss_mask: 0.4340  decode.d5.loss_dice: 0.4895  decode.d6.loss_cls: 1.9154  decode.d6.loss_mask: 0.4460  decode.d6.loss_dice: 0.4907  decode.d7.loss_cls: 1.9479  decode.d7.loss_mask: 0.5013  decode.d7.loss_dice: 0.5157  decode.d8.loss_cls: 1.9570  decode.d8.loss_mask: 0.4821  decode.d8.loss_dice: 0.5140
08/06 02:22:42 - mmengine - INFO - Iter(train) [   800/320000]  base_lr: 9.9775e-05 lr: 9.9775e-06  eta: 1 day, 15:26:20  time: 0.4366  data_time: 0.0089  memory: 5260  grad_norm: 157.6750  loss: 33.6729  decode.loss_cls: 1.8129  decode.loss_mask: 0.5412  decode.loss_dice: 0.5793  decode.d0.loss_cls: 5.6713  decode.d0.loss_mask: 0.6259  decode.d0.loss_dice: 0.7500  decode.d1.loss_cls: 1.9753  decode.d1.loss_mask: 0.5318  decode.d1.loss_dice: 0.5759  decode.d2.loss_cls: 1.7821  decode.d2.loss_mask: 0.5452  decode.d2.loss_dice: 0.6023  decode.d3.loss_cls: 1.8763  decode.d3.loss_mask: 0.5200  decode.d3.loss_dice: 0.5469  decode.d4.loss_cls: 1.8226  decode.d4.loss_mask: 0.5197  decode.d4.loss_dice: 0.5447  decode.d5.loss_cls: 1.8687  decode.d5.loss_mask: 0.5432  decode.d5.loss_dice: 0.5756  decode.d6.loss_cls: 1.9098  decode.d6.loss_mask: 0.5257  decode.d6.loss_dice: 0.5371  decode.d7.loss_cls: 1.8946  decode.d7.loss_mask: 0.5229  decode.d7.loss_dice: 0.5211  decode.d8.loss_cls: 1.8474  decode.d8.loss_mask: 0.5142  decode.d8.loss_dice: 0.5893
08/06 02:23:03 - mmengine - INFO - Iter(train) [   850/320000]  base_lr: 9.9761e-05 lr: 9.9761e-06  eta: 1 day, 15:23:10  time: 0.4355  data_time: 0.0090  memory: 5260  grad_norm: 176.2705  loss: 31.6168  decode.loss_cls: 1.6664  decode.loss_mask: 0.4749  decode.loss_dice: 0.5830  decode.d0.loss_cls: 5.4938  decode.d0.loss_mask: 0.5204  decode.d0.loss_dice: 0.7616  decode.d1.loss_cls: 1.7838  decode.d1.loss_mask: 0.5042  decode.d1.loss_dice: 0.5813  decode.d2.loss_cls: 1.7463  decode.d2.loss_mask: 0.4895  decode.d2.loss_dice: 0.5435  decode.d3.loss_cls: 1.7290  decode.d3.loss_mask: 0.4516  decode.d3.loss_dice: 0.5358  decode.d4.loss_cls: 1.7122  decode.d4.loss_mask: 0.4794  decode.d4.loss_dice: 0.5845  decode.d5.loss_cls: 1.6645  decode.d5.loss_mask: 0.4946  decode.d5.loss_dice: 0.5825  decode.d6.loss_cls: 1.6647  decode.d6.loss_mask: 0.4955  decode.d6.loss_dice: 0.5874  decode.d7.loss_cls: 1.7110  decode.d7.loss_mask: 0.4673  decode.d7.loss_dice: 0.5783  decode.d8.loss_cls: 1.6461  decode.d8.loss_mask: 0.4964  decode.d8.loss_dice: 0.5874
08/06 02:23:25 - mmengine - INFO - Iter(train) [   900/320000]  base_lr: 9.9747e-05 lr: 9.9747e-06  eta: 1 day, 15:20:17  time: 0.4350  data_time: 0.0088  memory: 5240  grad_norm: 229.7566  loss: 32.8998  decode.loss_cls: 1.6971  decode.loss_mask: 0.7154  decode.loss_dice: 0.5291  decode.d0.loss_cls: 5.3676  decode.d0.loss_mask: 0.6906  decode.d0.loss_dice: 0.7350  decode.d1.loss_cls: 1.9383  decode.d1.loss_mask: 0.5797  decode.d1.loss_dice: 0.5253  decode.d2.loss_cls: 1.8863  decode.d2.loss_mask: 0.4981  decode.d2.loss_dice: 0.4705  decode.d3.loss_cls: 1.8174  decode.d3.loss_mask: 0.5272  decode.d3.loss_dice: 0.4822  decode.d4.loss_cls: 1.8181  decode.d4.loss_mask: 0.5995  decode.d4.loss_dice: 0.5098  decode.d5.loss_cls: 1.7552  decode.d5.loss_mask: 0.6276  decode.d5.loss_dice: 0.4987  decode.d6.loss_cls: 1.7604  decode.d6.loss_mask: 0.6279  decode.d6.loss_dice: 0.5147  decode.d7.loss_cls: 1.6744  decode.d7.loss_mask: 0.6228  decode.d7.loss_dice: 0.5270  decode.d8.loss_cls: 1.7283  decode.d8.loss_mask: 0.6548  decode.d8.loss_dice: 0.5208
08/06 02:23:47 - mmengine - INFO - Iter(train) [   950/320000]  base_lr: 9.9733e-05 lr: 9.9733e-06  eta: 1 day, 15:17:43  time: 0.4362  data_time: 0.0089  memory: 5242  grad_norm: 116.2299  loss: 30.3387  decode.loss_cls: 1.7791  decode.loss_mask: 0.3373  decode.loss_dice: 0.5043  decode.d0.loss_cls: 5.2613  decode.d0.loss_mask: 0.3342  decode.d0.loss_dice: 0.5967  decode.d1.loss_cls: 2.0679  decode.d1.loss_mask: 0.3202  decode.d1.loss_dice: 0.4491  decode.d2.loss_cls: 1.9195  decode.d2.loss_mask: 0.3351  decode.d2.loss_dice: 0.4389  decode.d3.loss_cls: 1.9761  decode.d3.loss_mask: 0.3212  decode.d3.loss_dice: 0.4375  decode.d4.loss_cls: 1.8736  decode.d4.loss_mask: 0.3215  decode.d4.loss_dice: 0.4606  decode.d5.loss_cls: 1.8577  decode.d5.loss_mask: 0.3263  decode.d5.loss_dice: 0.4710  decode.d6.loss_cls: 1.8399  decode.d6.loss_mask: 0.3077  decode.d6.loss_dice: 0.4480  decode.d7.loss_cls: 1.9110  decode.d7.loss_mask: 0.3163  decode.d7.loss_dice: 0.4503  decode.d8.loss_cls: 1.8511  decode.d8.loss_mask: 0.3393  decode.d8.loss_dice: 0.4862
08/06 02:24:09 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 02:24:09 - mmengine - INFO - Iter(train) [  1000/320000]  base_lr: 9.9719e-05 lr: 9.9719e-06  eta: 1 day, 15:15:23  time: 0.4358  data_time: 0.0090  memory: 5236  grad_norm: 167.0411  loss: 29.2251  decode.loss_cls: 1.4432  decode.loss_mask: 0.6000  decode.loss_dice: 0.5273  decode.d0.loss_cls: 5.0145  decode.d0.loss_mask: 0.6056  decode.d0.loss_dice: 0.6232  decode.d1.loss_cls: 1.7209  decode.d1.loss_mask: 0.5714  decode.d1.loss_dice: 0.5051  decode.d2.loss_cls: 1.4502  decode.d2.loss_mask: 0.5395  decode.d2.loss_dice: 0.4837  decode.d3.loss_cls: 1.4857  decode.d3.loss_mask: 0.5297  decode.d3.loss_dice: 0.4849  decode.d4.loss_cls: 1.4642  decode.d4.loss_mask: 0.5458  decode.d4.loss_dice: 0.4713  decode.d5.loss_cls: 1.4655  decode.d5.loss_mask: 0.5311  decode.d5.loss_dice: 0.4570  decode.d6.loss_cls: 1.4598  decode.d6.loss_mask: 0.5715  decode.d6.loss_dice: 0.4987  decode.d7.loss_cls: 1.5165  decode.d7.loss_mask: 0.5876  decode.d7.loss_dice: 0.5199  decode.d8.loss_cls: 1.5005  decode.d8.loss_mask: 0.5675  decode.d8.loss_dice: 0.4834
08/06 02:24:31 - mmengine - INFO - Iter(train) [  1050/320000]  base_lr: 9.9705e-05 lr: 9.9705e-06  eta: 1 day, 15:13:15  time: 0.4367  data_time: 0.0088  memory: 5224  grad_norm: 127.5374  loss: 31.5408  decode.loss_cls: 1.8635  decode.loss_mask: 0.3817  decode.loss_dice: 0.5060  decode.d0.loss_cls: 5.0408  decode.d0.loss_mask: 0.5431  decode.d0.loss_dice: 0.7186  decode.d1.loss_cls: 2.1286  decode.d1.loss_mask: 0.3931  decode.d1.loss_dice: 0.4546  decode.d2.loss_cls: 1.9662  decode.d2.loss_mask: 0.3769  decode.d2.loss_dice: 0.4090  decode.d3.loss_cls: 2.0088  decode.d3.loss_mask: 0.3811  decode.d3.loss_dice: 0.4317  decode.d4.loss_cls: 2.0267  decode.d4.loss_mask: 0.3782  decode.d4.loss_dice: 0.4237  decode.d5.loss_cls: 2.0097  decode.d5.loss_mask: 0.3842  decode.d5.loss_dice: 0.4335  decode.d6.loss_cls: 1.9694  decode.d6.loss_mask: 0.3695  decode.d6.loss_dice: 0.4207  decode.d7.loss_cls: 1.9894  decode.d7.loss_mask: 0.3790  decode.d7.loss_dice: 0.4366  decode.d8.loss_cls: 1.9371  decode.d8.loss_mask: 0.3701  decode.d8.loss_dice: 0.4095
08/06 02:24:52 - mmengine - INFO - Iter(train) [  1100/320000]  base_lr: 9.9691e-05 lr: 9.9691e-06  eta: 1 day, 15:11:24  time: 0.4365  data_time: 0.0087  memory: 5260  grad_norm: 146.5344  loss: 27.8397  decode.loss_cls: 1.5463  decode.loss_mask: 0.3721  decode.loss_dice: 0.4440  decode.d0.loss_cls: 4.8055  decode.d0.loss_mask: 0.4289  decode.d0.loss_dice: 0.6357  decode.d1.loss_cls: 1.8014  decode.d1.loss_mask: 0.3768  decode.d1.loss_dice: 0.4584  decode.d2.loss_cls: 1.5961  decode.d2.loss_mask: 0.3493  decode.d2.loss_dice: 0.4242  decode.d3.loss_cls: 1.6817  decode.d3.loss_mask: 0.3325  decode.d3.loss_dice: 0.4236  decode.d4.loss_cls: 1.6432  decode.d4.loss_mask: 0.3455  decode.d4.loss_dice: 0.4543  decode.d5.loss_cls: 1.5992  decode.d5.loss_mask: 0.3625  decode.d5.loss_dice: 0.4626  decode.d6.loss_cls: 1.6635  decode.d6.loss_mask: 0.3721  decode.d6.loss_dice: 0.4580  decode.d7.loss_cls: 1.5800  decode.d7.loss_mask: 0.3678  decode.d7.loss_dice: 0.4449  decode.d8.loss_cls: 1.5770  decode.d8.loss_mask: 0.3781  decode.d8.loss_dice: 0.4547
08/06 02:25:14 - mmengine - INFO - Iter(train) [  1150/320000]  base_lr: 9.9677e-05 lr: 9.9677e-06  eta: 1 day, 15:09:41  time: 0.4366  data_time: 0.0090  memory: 5260  grad_norm: 169.7483  loss: 31.5674  decode.loss_cls: 1.8668  decode.loss_mask: 0.4314  decode.loss_dice: 0.5279  decode.d0.loss_cls: 4.7312  decode.d0.loss_mask: 0.4745  decode.d0.loss_dice: 0.7377  decode.d1.loss_cls: 1.9972  decode.d1.loss_mask: 0.4685  decode.d1.loss_dice: 0.5413  decode.d2.loss_cls: 1.8208  decode.d2.loss_mask: 0.4747  decode.d2.loss_dice: 0.5596  decode.d3.loss_cls: 1.8695  decode.d3.loss_mask: 0.4707  decode.d3.loss_dice: 0.5126  decode.d4.loss_cls: 1.8878  decode.d4.loss_mask: 0.4597  decode.d4.loss_dice: 0.5129  decode.d5.loss_cls: 1.8502  decode.d5.loss_mask: 0.4445  decode.d5.loss_dice: 0.5410  decode.d6.loss_cls: 1.8091  decode.d6.loss_mask: 0.4502  decode.d6.loss_dice: 0.5369  decode.d7.loss_cls: 1.7726  decode.d7.loss_mask: 0.4293  decode.d7.loss_dice: 0.5341  decode.d8.loss_cls: 1.8566  decode.d8.loss_mask: 0.4480  decode.d8.loss_dice: 0.5502
08/06 02:25:36 - mmengine - INFO - Iter(train) [  1200/320000]  base_lr: 9.9663e-05 lr: 9.9663e-06  eta: 1 day, 15:08:07  time: 0.4373  data_time: 0.0088  memory: 5316  grad_norm: 158.2509  loss: 28.4689  decode.loss_cls: 1.6149  decode.loss_mask: 0.3636  decode.loss_dice: 0.4496  decode.d0.loss_cls: 4.5328  decode.d0.loss_mask: 0.4196  decode.d0.loss_dice: 0.6568  decode.d1.loss_cls: 1.7978  decode.d1.loss_mask: 0.4025  decode.d1.loss_dice: 0.4926  decode.d2.loss_cls: 1.7363  decode.d2.loss_mask: 0.3876  decode.d2.loss_dice: 0.4864  decode.d3.loss_cls: 1.7011  decode.d3.loss_mask: 0.3705  decode.d3.loss_dice: 0.4599  decode.d4.loss_cls: 1.6488  decode.d4.loss_mask: 0.3817  decode.d4.loss_dice: 0.4497  decode.d5.loss_cls: 1.6787  decode.d5.loss_mask: 0.3723  decode.d5.loss_dice: 0.4633  decode.d6.loss_cls: 1.6922  decode.d6.loss_mask: 0.3659  decode.d6.loss_dice: 0.4538  decode.d7.loss_cls: 1.7026  decode.d7.loss_mask: 0.3781  decode.d7.loss_dice: 0.4703  decode.d8.loss_cls: 1.6655  decode.d8.loss_mask: 0.3815  decode.d8.loss_dice: 0.4927
08/06 02:25:58 - mmengine - INFO - Iter(train) [  1250/320000]  base_lr: 9.9649e-05 lr: 9.9649e-06  eta: 1 day, 15:07:20  time: 0.4353  data_time: 0.0088  memory: 5240  grad_norm: 169.8421  loss: 27.5978  decode.loss_cls: 1.6697  decode.loss_mask: 0.3094  decode.loss_dice: 0.4725  decode.d0.loss_cls: 4.3859  decode.d0.loss_mask: 0.4568  decode.d0.loss_dice: 0.7481  decode.d1.loss_cls: 1.7971  decode.d1.loss_mask: 0.3762  decode.d1.loss_dice: 0.4879  decode.d2.loss_cls: 1.6435  decode.d2.loss_mask: 0.3431  decode.d2.loss_dice: 0.4496  decode.d3.loss_cls: 1.5690  decode.d3.loss_mask: 0.3260  decode.d3.loss_dice: 0.4324  decode.d4.loss_cls: 1.5690  decode.d4.loss_mask: 0.3169  decode.d4.loss_dice: 0.4366  decode.d5.loss_cls: 1.6190  decode.d5.loss_mask: 0.3131  decode.d5.loss_dice: 0.4456  decode.d6.loss_cls: 1.6865  decode.d6.loss_mask: 0.3351  decode.d6.loss_dice: 0.4495  decode.d7.loss_cls: 1.6599  decode.d7.loss_mask: 0.3207  decode.d7.loss_dice: 0.4483  decode.d8.loss_cls: 1.7456  decode.d8.loss_mask: 0.3264  decode.d8.loss_dice: 0.4585
08/06 02:26:20 - mmengine - INFO - Iter(train) [  1300/320000]  base_lr: 9.9635e-05 lr: 9.9635e-06  eta: 1 day, 15:05:44  time: 0.4362  data_time: 0.0090  memory: 5275  grad_norm: 177.7142  loss: 28.6257  decode.loss_cls: 1.5140  decode.loss_mask: 0.5069  decode.loss_dice: 0.5346  decode.d0.loss_cls: 4.2466  decode.d0.loss_mask: 0.5593  decode.d0.loss_dice: 0.7211  decode.d1.loss_cls: 1.5737  decode.d1.loss_mask: 0.5088  decode.d1.loss_dice: 0.5033  decode.d2.loss_cls: 1.5089  decode.d2.loss_mask: 0.5189  decode.d2.loss_dice: 0.5675  decode.d3.loss_cls: 1.5140  decode.d3.loss_mask: 0.5001  decode.d3.loss_dice: 0.4949  decode.d4.loss_cls: 1.5415  decode.d4.loss_mask: 0.5059  decode.d4.loss_dice: 0.4870  decode.d5.loss_cls: 1.5415  decode.d5.loss_mask: 0.4818  decode.d5.loss_dice: 0.5371  decode.d6.loss_cls: 1.5507  decode.d6.loss_mask: 0.4970  decode.d6.loss_dice: 0.5298  decode.d7.loss_cls: 1.6048  decode.d7.loss_mask: 0.4998  decode.d7.loss_dice: 0.5220  decode.d8.loss_cls: 1.4987  decode.d8.loss_mask: 0.5138  decode.d8.loss_dice: 0.5418
08/06 02:26:42 - mmengine - INFO - Iter(train) [  1350/320000]  base_lr: 9.9621e-05 lr: 9.9621e-06  eta: 1 day, 15:04:32  time: 0.4380  data_time: 0.0088  memory: 5299  grad_norm: 115.0178  loss: 25.4547  decode.loss_cls: 1.4613  decode.loss_mask: 0.3626  decode.loss_dice: 0.4029  decode.d0.loss_cls: 3.9720  decode.d0.loss_mask: 0.4062  decode.d0.loss_dice: 0.5531  decode.d1.loss_cls: 1.6826  decode.d1.loss_mask: 0.3644  decode.d1.loss_dice: 0.4094  decode.d2.loss_cls: 1.5312  decode.d2.loss_mask: 0.3566  decode.d2.loss_dice: 0.3698  decode.d3.loss_cls: 1.4403  decode.d3.loss_mask: 0.3473  decode.d3.loss_dice: 0.3594  decode.d4.loss_cls: 1.4919  decode.d4.loss_mask: 0.3483  decode.d4.loss_dice: 0.4110  decode.d5.loss_cls: 1.5066  decode.d5.loss_mask: 0.3768  decode.d5.loss_dice: 0.4221  decode.d6.loss_cls: 1.5283  decode.d6.loss_mask: 0.3744  decode.d6.loss_dice: 0.4349  decode.d7.loss_cls: 1.5223  decode.d7.loss_mask: 0.3781  decode.d7.loss_dice: 0.4108  decode.d8.loss_cls: 1.4328  decode.d8.loss_mask: 0.3767  decode.d8.loss_dice: 0.4205
08/06 02:27:04 - mmengine - INFO - Iter(train) [  1400/320000]  base_lr: 9.9606e-05 lr: 9.9606e-06  eta: 1 day, 15:03:21  time: 0.4357  data_time: 0.0090  memory: 5240  grad_norm: 142.0854  loss: 28.3856  decode.loss_cls: 1.5589  decode.loss_mask: 0.4697  decode.loss_dice: 0.4885  decode.d0.loss_cls: 3.9978  decode.d0.loss_mask: 0.5722  decode.d0.loss_dice: 0.7204  decode.d1.loss_cls: 1.9226  decode.d1.loss_mask: 0.4280  decode.d1.loss_dice: 0.4921  decode.d2.loss_cls: 1.6584  decode.d2.loss_mask: 0.4398  decode.d2.loss_dice: 0.4813  decode.d3.loss_cls: 1.6272  decode.d3.loss_mask: 0.4348  decode.d3.loss_dice: 0.4662  decode.d4.loss_cls: 1.5623  decode.d4.loss_mask: 0.4426  decode.d4.loss_dice: 0.4923  decode.d5.loss_cls: 1.5827  decode.d5.loss_mask: 0.4430  decode.d5.loss_dice: 0.4822  decode.d6.loss_cls: 1.5576  decode.d6.loss_mask: 0.4593  decode.d6.loss_dice: 0.5040  decode.d7.loss_cls: 1.6013  decode.d7.loss_mask: 0.4666  decode.d7.loss_dice: 0.4879  decode.d8.loss_cls: 1.5983  decode.d8.loss_mask: 0.4591  decode.d8.loss_dice: 0.4884
08/06 02:27:25 - mmengine - INFO - Iter(train) [  1450/320000]  base_lr: 9.9592e-05 lr: 9.9592e-06  eta: 1 day, 15:01:55  time: 0.4353  data_time: 0.0088  memory: 5242  grad_norm: 165.6602  loss: 27.5646  decode.loss_cls: 1.5693  decode.loss_mask: 0.3573  decode.loss_dice: 0.4856  decode.d0.loss_cls: 3.8352  decode.d0.loss_mask: 0.4422  decode.d0.loss_dice: 0.6512  decode.d1.loss_cls: 1.7629  decode.d1.loss_mask: 0.3796  decode.d1.loss_dice: 0.5269  decode.d2.loss_cls: 1.8075  decode.d2.loss_mask: 0.3651  decode.d2.loss_dice: 0.5008  decode.d3.loss_cls: 1.6551  decode.d3.loss_mask: 0.3840  decode.d3.loss_dice: 0.4682  decode.d4.loss_cls: 1.6104  decode.d4.loss_mask: 0.4010  decode.d4.loss_dice: 0.4759  decode.d5.loss_cls: 1.5868  decode.d5.loss_mask: 0.3773  decode.d5.loss_dice: 0.4739  decode.d6.loss_cls: 1.5721  decode.d6.loss_mask: 0.4005  decode.d6.loss_dice: 0.4962  decode.d7.loss_cls: 1.5818  decode.d7.loss_mask: 0.3982  decode.d7.loss_dice: 0.5030  decode.d8.loss_cls: 1.6158  decode.d8.loss_mask: 0.3710  decode.d8.loss_dice: 0.5100
08/06 02:27:47 - mmengine - INFO - Iter(train) [  1500/320000]  base_lr: 9.9578e-05 lr: 9.9578e-06  eta: 1 day, 15:00:32  time: 0.4351  data_time: 0.0088  memory: 5242  grad_norm: 204.4505  loss: 27.8359  decode.loss_cls: 1.4179  decode.loss_mask: 0.4431  decode.loss_dice: 0.5593  decode.d0.loss_cls: 3.5996  decode.d0.loss_mask: 0.5709  decode.d0.loss_dice: 0.7350  decode.d1.loss_cls: 1.6538  decode.d1.loss_mask: 0.4585  decode.d1.loss_dice: 0.5760  decode.d2.loss_cls: 1.4796  decode.d2.loss_mask: 0.4541  decode.d2.loss_dice: 0.5418  decode.d3.loss_cls: 1.4444  decode.d3.loss_mask: 0.4419  decode.d3.loss_dice: 0.5794  decode.d4.loss_cls: 1.4673  decode.d4.loss_mask: 0.5175  decode.d4.loss_dice: 0.6159  decode.d5.loss_cls: 1.4530  decode.d5.loss_mask: 0.4759  decode.d5.loss_dice: 0.5856  decode.d6.loss_cls: 1.4976  decode.d6.loss_mask: 0.4822  decode.d6.loss_dice: 0.5963  decode.d7.loss_cls: 1.5548  decode.d7.loss_mask: 0.5422  decode.d7.loss_dice: 0.5556  decode.d8.loss_cls: 1.5001  decode.d8.loss_mask: 0.4459  decode.d8.loss_dice: 0.5906
08/06 02:28:09 - mmengine - INFO - Iter(train) [  1550/320000]  base_lr: 9.9564e-05 lr: 9.9564e-06  eta: 1 day, 14:59:16  time: 0.4354  data_time: 0.0089  memory: 5275  grad_norm: 137.8878  loss: 23.9968  decode.loss_cls: 1.2390  decode.loss_mask: 0.4545  decode.loss_dice: 0.4298  decode.d0.loss_cls: 3.3951  decode.d0.loss_mask: 0.4885  decode.d0.loss_dice: 0.6004  decode.d1.loss_cls: 1.3940  decode.d1.loss_mask: 0.4574  decode.d1.loss_dice: 0.4426  decode.d2.loss_cls: 1.3397  decode.d2.loss_mask: 0.4383  decode.d2.loss_dice: 0.4150  decode.d3.loss_cls: 1.2684  decode.d3.loss_mask: 0.4456  decode.d3.loss_dice: 0.4200  decode.d4.loss_cls: 1.2518  decode.d4.loss_mask: 0.4316  decode.d4.loss_dice: 0.4294  decode.d5.loss_cls: 1.3029  decode.d5.loss_mask: 0.4448  decode.d5.loss_dice: 0.4548  decode.d6.loss_cls: 1.2442  decode.d6.loss_mask: 0.4459  decode.d6.loss_dice: 0.4278  decode.d7.loss_cls: 1.2785  decode.d7.loss_mask: 0.4357  decode.d7.loss_dice: 0.4490  decode.d8.loss_cls: 1.2543  decode.d8.loss_mask: 0.4591  decode.d8.loss_dice: 0.4587
08/06 02:28:31 - mmengine - INFO - Iter(train) [  1600/320000]  base_lr: 9.9550e-05 lr: 9.9550e-06  eta: 1 day, 14:58:05  time: 0.4357  data_time: 0.0089  memory: 5260  grad_norm: 128.9029  loss: 27.9528  decode.loss_cls: 1.6304  decode.loss_mask: 0.4361  decode.loss_dice: 0.5238  decode.d0.loss_cls: 3.4428  decode.d0.loss_mask: 0.4743  decode.d0.loss_dice: 0.6274  decode.d1.loss_cls: 1.8204  decode.d1.loss_mask: 0.4548  decode.d1.loss_dice: 0.5107  decode.d2.loss_cls: 1.6976  decode.d2.loss_mask: 0.4408  decode.d2.loss_dice: 0.4884  decode.d3.loss_cls: 1.6567  decode.d3.loss_mask: 0.4220  decode.d3.loss_dice: 0.5062  decode.d4.loss_cls: 1.6757  decode.d4.loss_mask: 0.4120  decode.d4.loss_dice: 0.5097  decode.d5.loss_cls: 1.6688  decode.d5.loss_mask: 0.4129  decode.d5.loss_dice: 0.4546  decode.d6.loss_cls: 1.6583  decode.d6.loss_mask: 0.4177  decode.d6.loss_dice: 0.4956  decode.d7.loss_cls: 1.7145  decode.d7.loss_mask: 0.4046  decode.d7.loss_dice: 0.5019  decode.d8.loss_cls: 1.6072  decode.d8.loss_mask: 0.4160  decode.d8.loss_dice: 0.4710
08/06 02:28:52 - mmengine - INFO - Iter(train) [  1650/320000]  base_lr: 9.9536e-05 lr: 9.9536e-06  eta: 1 day, 14:56:52  time: 0.4353  data_time: 0.0089  memory: 5260  grad_norm: 197.9220  loss: 28.0375  decode.loss_cls: 1.6239  decode.loss_mask: 0.4907  decode.loss_dice: 0.5255  decode.d0.loss_cls: 3.4233  decode.d0.loss_mask: 0.4835  decode.d0.loss_dice: 0.6982  decode.d1.loss_cls: 1.6134  decode.d1.loss_mask: 0.5069  decode.d1.loss_dice: 0.5345  decode.d2.loss_cls: 1.4792  decode.d2.loss_mask: 0.4958  decode.d2.loss_dice: 0.5297  decode.d3.loss_cls: 1.6632  decode.d3.loss_mask: 0.4728  decode.d3.loss_dice: 0.5316  decode.d4.loss_cls: 1.5934  decode.d4.loss_mask: 0.4751  decode.d4.loss_dice: 0.5163  decode.d5.loss_cls: 1.5163  decode.d5.loss_mask: 0.5131  decode.d5.loss_dice: 0.5427  decode.d6.loss_cls: 1.5703  decode.d6.loss_mask: 0.4992  decode.d6.loss_dice: 0.5086  decode.d7.loss_cls: 1.5809  decode.d7.loss_mask: 0.5200  decode.d7.loss_dice: 0.5447  decode.d8.loss_cls: 1.5153  decode.d8.loss_mask: 0.5131  decode.d8.loss_dice: 0.5564
08/06 02:29:14 - mmengine - INFO - Iter(train) [  1700/320000]  base_lr: 9.9522e-05 lr: 9.9522e-06  eta: 1 day, 14:55:43  time: 0.4357  data_time: 0.0089  memory: 5260  grad_norm: 193.7012  loss: 23.5595  decode.loss_cls: 1.2197  decode.loss_mask: 0.3325  decode.loss_dice: 0.4654  decode.d0.loss_cls: 3.2405  decode.d0.loss_mask: 0.4361  decode.d0.loss_dice: 0.7363  decode.d1.loss_cls: 1.4334  decode.d1.loss_mask: 0.3146  decode.d1.loss_dice: 0.4661  decode.d2.loss_cls: 1.3268  decode.d2.loss_mask: 0.3398  decode.d2.loss_dice: 0.4576  decode.d3.loss_cls: 1.1630  decode.d3.loss_mask: 0.3915  decode.d3.loss_dice: 0.4668  decode.d4.loss_cls: 1.1955  decode.d4.loss_mask: 0.3848  decode.d4.loss_dice: 0.5046  decode.d5.loss_cls: 1.2646  decode.d5.loss_mask: 0.4008  decode.d5.loss_dice: 0.5186  decode.d6.loss_cls: 1.3257  decode.d6.loss_mask: 0.3272  decode.d6.loss_dice: 0.5141  decode.d7.loss_cls: 1.3627  decode.d7.loss_mask: 0.3571  decode.d7.loss_dice: 0.4929  decode.d8.loss_cls: 1.3489  decode.d8.loss_mask: 0.3121  decode.d8.loss_dice: 0.4597
08/06 02:29:36 - mmengine - INFO - Iter(train) [  1750/320000]  base_lr: 9.9508e-05 lr: 9.9508e-06  eta: 1 day, 14:54:39  time: 0.4358  data_time: 0.0090  memory: 5242  grad_norm: 156.8919  loss: 27.2584  decode.loss_cls: 1.6264  decode.loss_mask: 0.4367  decode.loss_dice: 0.5396  decode.d0.loss_cls: 3.1577  decode.d0.loss_mask: 0.4643  decode.d0.loss_dice: 0.6698  decode.d1.loss_cls: 1.6328  decode.d1.loss_mask: 0.4396  decode.d1.loss_dice: 0.5095  decode.d2.loss_cls: 1.5660  decode.d2.loss_mask: 0.4231  decode.d2.loss_dice: 0.5212  decode.d3.loss_cls: 1.5460  decode.d3.loss_mask: 0.4035  decode.d3.loss_dice: 0.4886  decode.d4.loss_cls: 1.5846  decode.d4.loss_mask: 0.4243  decode.d4.loss_dice: 0.5068  decode.d5.loss_cls: 1.6429  decode.d5.loss_mask: 0.4141  decode.d5.loss_dice: 0.5223  decode.d6.loss_cls: 1.6752  decode.d6.loss_mask: 0.4232  decode.d6.loss_dice: 0.5112  decode.d7.loss_cls: 1.6419  decode.d7.loss_mask: 0.4512  decode.d7.loss_dice: 0.4972  decode.d8.loss_cls: 1.5933  decode.d8.loss_mask: 0.4201  decode.d8.loss_dice: 0.5251
08/06 02:29:58 - mmengine - INFO - Iter(train) [  1800/320000]  base_lr: 9.9494e-05 lr: 9.9494e-06  eta: 1 day, 14:53:38  time: 0.4358  data_time: 0.0089  memory: 5260  grad_norm: 152.9022  loss: 23.6255  decode.loss_cls: 1.4974  decode.loss_mask: 0.3267  decode.loss_dice: 0.4011  decode.d0.loss_cls: 3.1079  decode.d0.loss_mask: 0.3855  decode.d0.loss_dice: 0.6084  decode.d1.loss_cls: 1.6089  decode.d1.loss_mask: 0.3401  decode.d1.loss_dice: 0.4528  decode.d2.loss_cls: 1.4784  decode.d2.loss_mask: 0.2843  decode.d2.loss_dice: 0.3833  decode.d3.loss_cls: 1.4296  decode.d3.loss_mask: 0.2996  decode.d3.loss_dice: 0.3931  decode.d4.loss_cls: 1.3129  decode.d4.loss_mask: 0.3022  decode.d4.loss_dice: 0.3826  decode.d5.loss_cls: 1.3752  decode.d5.loss_mask: 0.2993  decode.d5.loss_dice: 0.4111  decode.d6.loss_cls: 1.4460  decode.d6.loss_mask: 0.3194  decode.d6.loss_dice: 0.4068  decode.d7.loss_cls: 1.4473  decode.d7.loss_mask: 0.3282  decode.d7.loss_dice: 0.4236  decode.d8.loss_cls: 1.4021  decode.d8.loss_mask: 0.3535  decode.d8.loss_dice: 0.4183
08/06 02:30:20 - mmengine - INFO - Iter(train) [  1850/320000]  base_lr: 9.9480e-05 lr: 9.9480e-06  eta: 1 day, 14:52:37  time: 0.4353  data_time: 0.0089  memory: 5224  grad_norm: 219.3753  loss: 24.2578  decode.loss_cls: 1.3966  decode.loss_mask: 0.4035  decode.loss_dice: 0.4503  decode.d0.loss_cls: 3.0592  decode.d0.loss_mask: 0.4544  decode.d0.loss_dice: 0.5761  decode.d1.loss_cls: 1.6927  decode.d1.loss_mask: 0.3836  decode.d1.loss_dice: 0.4105  decode.d2.loss_cls: 1.4532  decode.d2.loss_mask: 0.3783  decode.d2.loss_dice: 0.3921  decode.d3.loss_cls: 1.3925  decode.d3.loss_mask: 0.3681  decode.d3.loss_dice: 0.3905  decode.d4.loss_cls: 1.4512  decode.d4.loss_mask: 0.3820  decode.d4.loss_dice: 0.3778  decode.d5.loss_cls: 1.3509  decode.d5.loss_mask: 0.4026  decode.d5.loss_dice: 0.4059  decode.d6.loss_cls: 1.4474  decode.d6.loss_mask: 0.3925  decode.d6.loss_dice: 0.4110  decode.d7.loss_cls: 1.4388  decode.d7.loss_mask: 0.3687  decode.d7.loss_dice: 0.3804  decode.d8.loss_cls: 1.4634  decode.d8.loss_mask: 0.3824  decode.d8.loss_dice: 0.4010
08/06 02:30:41 - mmengine - INFO - Iter(train) [  1900/320000]  base_lr: 9.9466e-05 lr: 9.9466e-06  eta: 1 day, 14:51:41  time: 0.4360  data_time: 0.0090  memory: 5260  grad_norm: 173.0859  loss: 23.1160  decode.loss_cls: 1.1683  decode.loss_mask: 0.4693  decode.loss_dice: 0.4571  decode.d0.loss_cls: 2.6841  decode.d0.loss_mask: 0.4419  decode.d0.loss_dice: 0.5593  decode.d1.loss_cls: 1.4930  decode.d1.loss_mask: 0.4377  decode.d1.loss_dice: 0.4197  decode.d2.loss_cls: 1.3663  decode.d2.loss_mask: 0.4490  decode.d2.loss_dice: 0.4369  decode.d3.loss_cls: 1.3343  decode.d3.loss_mask: 0.4377  decode.d3.loss_dice: 0.4369  decode.d4.loss_cls: 1.3283  decode.d4.loss_mask: 0.4253  decode.d4.loss_dice: 0.3932  decode.d5.loss_cls: 1.3218  decode.d5.loss_mask: 0.4395  decode.d5.loss_dice: 0.4187  decode.d6.loss_cls: 1.2114  decode.d6.loss_mask: 0.4159  decode.d6.loss_dice: 0.4419  decode.d7.loss_cls: 1.1946  decode.d7.loss_mask: 0.4357  decode.d7.loss_dice: 0.4525  decode.d8.loss_cls: 1.2010  decode.d8.loss_mask: 0.4235  decode.d8.loss_dice: 0.4213
08/06 02:31:03 - mmengine - INFO - Iter(train) [  1950/320000]  base_lr: 9.9452e-05 lr: 9.9452e-06  eta: 1 day, 14:50:44  time: 0.4356  data_time: 0.0090  memory: 5240  grad_norm: 172.7655  loss: 26.1480  decode.loss_cls: 1.4890  decode.loss_mask: 0.4568  decode.loss_dice: 0.4700  decode.d0.loss_cls: 2.8524  decode.d0.loss_mask: 0.5422  decode.d0.loss_dice: 0.6099  decode.d1.loss_cls: 1.7148  decode.d1.loss_mask: 0.4238  decode.d1.loss_dice: 0.4772  decode.d2.loss_cls: 1.5659  decode.d2.loss_mask: 0.4517  decode.d2.loss_dice: 0.4688  decode.d3.loss_cls: 1.4949  decode.d3.loss_mask: 0.4631  decode.d3.loss_dice: 0.4715  decode.d4.loss_cls: 1.4560  decode.d4.loss_mask: 0.4400  decode.d4.loss_dice: 0.4744  decode.d5.loss_cls: 1.5675  decode.d5.loss_mask: 0.4692  decode.d5.loss_dice: 0.4481  decode.d6.loss_cls: 1.5785  decode.d6.loss_mask: 0.4230  decode.d6.loss_dice: 0.4514  decode.d7.loss_cls: 1.4829  decode.d7.loss_mask: 0.4907  decode.d7.loss_dice: 0.4718  decode.d8.loss_cls: 1.5318  decode.d8.loss_mask: 0.4530  decode.d8.loss_dice: 0.4576
08/06 02:31:25 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 02:31:25 - mmengine - INFO - Iter(train) [  2000/320000]  base_lr: 9.9438e-05 lr: 9.9438e-06  eta: 1 day, 14:49:46  time: 0.4349  data_time: 0.0088  memory: 5223  grad_norm: 126.0393  loss: 23.5812  decode.loss_cls: 1.4557  decode.loss_mask: 0.3434  decode.loss_dice: 0.4272  decode.d0.loss_cls: 2.7079  decode.d0.loss_mask: 0.3815  decode.d0.loss_dice: 0.5613  decode.d1.loss_cls: 1.5905  decode.d1.loss_mask: 0.3517  decode.d1.loss_dice: 0.4153  decode.d2.loss_cls: 1.4138  decode.d2.loss_mask: 0.3479  decode.d2.loss_dice: 0.4244  decode.d3.loss_cls: 1.3318  decode.d3.loss_mask: 0.3708  decode.d3.loss_dice: 0.4222  decode.d4.loss_cls: 1.3563  decode.d4.loss_mask: 0.3673  decode.d4.loss_dice: 0.4412  decode.d5.loss_cls: 1.4282  decode.d5.loss_mask: 0.3511  decode.d5.loss_dice: 0.4356  decode.d6.loss_cls: 1.4085  decode.d6.loss_mask: 0.3544  decode.d6.loss_dice: 0.4340  decode.d7.loss_cls: 1.4660  decode.d7.loss_mask: 0.3686  decode.d7.loss_dice: 0.4610  decode.d8.loss_cls: 1.3713  decode.d8.loss_mask: 0.3514  decode.d8.loss_dice: 0.4408
08/06 02:31:47 - mmengine - INFO - Iter(train) [  2050/320000]  base_lr: 9.9424e-05 lr: 9.9424e-06  eta: 1 day, 14:48:57  time: 0.4361  data_time: 0.0091  memory: 5258  grad_norm: 210.9326  loss: 21.1251  decode.loss_cls: 1.0822  decode.loss_mask: 0.4054  decode.loss_dice: 0.4070  decode.d0.loss_cls: 2.4422  decode.d0.loss_mask: 0.4512  decode.d0.loss_dice: 0.5226  decode.d1.loss_cls: 1.3593  decode.d1.loss_mask: 0.3877  decode.d1.loss_dice: 0.4335  decode.d2.loss_cls: 1.1684  decode.d2.loss_mask: 0.3890  decode.d2.loss_dice: 0.4093  decode.d3.loss_cls: 1.1507  decode.d3.loss_mask: 0.3709  decode.d3.loss_dice: 0.3776  decode.d4.loss_cls: 1.1850  decode.d4.loss_mask: 0.3715  decode.d4.loss_dice: 0.3738  decode.d5.loss_cls: 1.1379  decode.d5.loss_mask: 0.3873  decode.d5.loss_dice: 0.3585  decode.d6.loss_cls: 1.2057  decode.d6.loss_mask: 0.3999  decode.d6.loss_dice: 0.3887  decode.d7.loss_cls: 1.2124  decode.d7.loss_mask: 0.4214  decode.d7.loss_dice: 0.3936  decode.d8.loss_cls: 1.1166  decode.d8.loss_mask: 0.4144  decode.d8.loss_dice: 0.4013
08/06 02:32:09 - mmengine - INFO - Iter(train) [  2100/320000]  base_lr: 9.9409e-05 lr: 9.9409e-06  eta: 1 day, 14:48:14  time: 0.4356  data_time: 0.0089  memory: 5240  grad_norm: 182.1534  loss: 22.5305  decode.loss_cls: 1.1525  decode.loss_mask: 0.4184  decode.loss_dice: 0.4258  decode.d0.loss_cls: 2.5105  decode.d0.loss_mask: 0.4944  decode.d0.loss_dice: 0.5792  decode.d1.loss_cls: 1.4849  decode.d1.loss_mask: 0.4561  decode.d1.loss_dice: 0.4465  decode.d2.loss_cls: 1.2349  decode.d2.loss_mask: 0.4698  decode.d2.loss_dice: 0.4182  decode.d3.loss_cls: 1.2712  decode.d3.loss_mask: 0.4176  decode.d3.loss_dice: 0.3970  decode.d4.loss_cls: 1.2123  decode.d4.loss_mask: 0.4746  decode.d4.loss_dice: 0.3893  decode.d5.loss_cls: 1.2637  decode.d5.loss_mask: 0.4209  decode.d5.loss_dice: 0.4173  decode.d6.loss_cls: 1.2311  decode.d6.loss_mask: 0.4295  decode.d6.loss_dice: 0.4168  decode.d7.loss_cls: 1.2005  decode.d7.loss_mask: 0.4327  decode.d7.loss_dice: 0.4131  decode.d8.loss_cls: 1.2067  decode.d8.loss_mask: 0.4363  decode.d8.loss_dice: 0.4090
08/06 02:32:30 - mmengine - INFO - Iter(train) [  2150/320000]  base_lr: 9.9395e-05 lr: 9.9395e-06  eta: 1 day, 14:47:23  time: 0.4355  data_time: 0.0090  memory: 5299  grad_norm: 166.4910  loss: 22.3042  decode.loss_cls: 1.0569  decode.loss_mask: 0.5080  decode.loss_dice: 0.4419  decode.d0.loss_cls: 2.4363  decode.d0.loss_mask: 0.5082  decode.d0.loss_dice: 0.5825  decode.d1.loss_cls: 1.2966  decode.d1.loss_mask: 0.4715  decode.d1.loss_dice: 0.5008  decode.d2.loss_cls: 1.1783  decode.d2.loss_mask: 0.5017  decode.d2.loss_dice: 0.4793  decode.d3.loss_cls: 1.1673  decode.d3.loss_mask: 0.5115  decode.d3.loss_dice: 0.4619  decode.d4.loss_cls: 1.0772  decode.d4.loss_mask: 0.4896  decode.d4.loss_dice: 0.4805  decode.d5.loss_cls: 1.0382  decode.d5.loss_mask: 0.5036  decode.d5.loss_dice: 0.4502  decode.d6.loss_cls: 1.0231  decode.d6.loss_mask: 0.5051  decode.d6.loss_dice: 0.5034  decode.d7.loss_cls: 1.0368  decode.d7.loss_mask: 0.5376  decode.d7.loss_dice: 0.5093  decode.d8.loss_cls: 1.0160  decode.d8.loss_mask: 0.5423  decode.d8.loss_dice: 0.4887
08/06 02:32:52 - mmengine - INFO - Iter(train) [  2200/320000]  base_lr: 9.9381e-05 lr: 9.9381e-06  eta: 1 day, 14:46:31  time: 0.4357  data_time: 0.0090  memory: 5275  grad_norm: 140.1581  loss: 25.4138  decode.loss_cls: 1.4729  decode.loss_mask: 0.3682  decode.loss_dice: 0.5064  decode.d0.loss_cls: 2.6254  decode.d0.loss_mask: 0.4327  decode.d0.loss_dice: 0.6544  decode.d1.loss_cls: 1.7489  decode.d1.loss_mask: 0.3615  decode.d1.loss_dice: 0.5311  decode.d2.loss_cls: 1.5602  decode.d2.loss_mask: 0.3693  decode.d2.loss_dice: 0.4963  decode.d3.loss_cls: 1.5015  decode.d3.loss_mask: 0.3425  decode.d3.loss_dice: 0.4777  decode.d4.loss_cls: 1.5239  decode.d4.loss_mask: 0.3376  decode.d4.loss_dice: 0.4867  decode.d5.loss_cls: 1.5090  decode.d5.loss_mask: 0.3497  decode.d5.loss_dice: 0.5055  decode.d6.loss_cls: 1.5219  decode.d6.loss_mask: 0.3613  decode.d6.loss_dice: 0.4905  decode.d7.loss_cls: 1.6030  decode.d7.loss_mask: 0.3766  decode.d7.loss_dice: 0.5062  decode.d8.loss_cls: 1.5198  decode.d8.loss_mask: 0.3655  decode.d8.loss_dice: 0.5077
08/06 02:33:14 - mmengine - INFO - Iter(train) [  2250/320000]  base_lr: 9.9367e-05 lr: 9.9367e-06  eta: 1 day, 14:45:42  time: 0.4354  data_time: 0.0089  memory: 5260  grad_norm: 122.7294  loss: 19.0695  decode.loss_cls: 1.1461  decode.loss_mask: 0.2784  decode.loss_dice: 0.3121  decode.d0.loss_cls: 2.3268  decode.d0.loss_mask: 0.3377  decode.d0.loss_dice: 0.4608  decode.d1.loss_cls: 1.3620  decode.d1.loss_mask: 0.2909  decode.d1.loss_dice: 0.3332  decode.d2.loss_cls: 1.2111  decode.d2.loss_mask: 0.2812  decode.d2.loss_dice: 0.3283  decode.d3.loss_cls: 1.1344  decode.d3.loss_mask: 0.2869  decode.d3.loss_dice: 0.3218  decode.d4.loss_cls: 1.0734  decode.d4.loss_mask: 0.2849  decode.d4.loss_dice: 0.3288  decode.d5.loss_cls: 1.0932  decode.d5.loss_mask: 0.2855  decode.d5.loss_dice: 0.3395  decode.d6.loss_cls: 1.1346  decode.d6.loss_mask: 0.2866  decode.d6.loss_dice: 0.3241  decode.d7.loss_cls: 1.1648  decode.d7.loss_mask: 0.2835  decode.d7.loss_dice: 0.3274  decode.d8.loss_cls: 1.1257  decode.d8.loss_mask: 0.2840  decode.d8.loss_dice: 0.3214
08/06 02:33:36 - mmengine - INFO - Iter(train) [  2300/320000]  base_lr: 9.9353e-05 lr: 9.9353e-06  eta: 1 day, 14:44:53  time: 0.4354  data_time: 0.0088  memory: 5240  grad_norm: 155.3742  loss: 21.1746  decode.loss_cls: 1.2444  decode.loss_mask: 0.3390  decode.loss_dice: 0.4658  decode.d0.loss_cls: 2.2086  decode.d0.loss_mask: 0.3200  decode.d0.loss_dice: 0.5299  decode.d1.loss_cls: 1.2963  decode.d1.loss_mask: 0.3407  decode.d1.loss_dice: 0.4728  decode.d2.loss_cls: 1.1280  decode.d2.loss_mask: 0.3428  decode.d2.loss_dice: 0.4726  decode.d3.loss_cls: 1.1544  decode.d3.loss_mask: 0.3385  decode.d3.loss_dice: 0.4349  decode.d4.loss_cls: 1.2153  decode.d4.loss_mask: 0.3261  decode.d4.loss_dice: 0.4359  decode.d5.loss_cls: 1.1637  decode.d5.loss_mask: 0.3304  decode.d5.loss_dice: 0.4652  decode.d6.loss_cls: 1.2543  decode.d6.loss_mask: 0.3298  decode.d6.loss_dice: 0.4712  decode.d7.loss_cls: 1.2487  decode.d7.loss_mask: 0.3372  decode.d7.loss_dice: 0.5234  decode.d8.loss_cls: 1.1656  decode.d8.loss_mask: 0.3396  decode.d8.loss_dice: 0.4793
08/06 02:33:57 - mmengine - INFO - Iter(train) [  2350/320000]  base_lr: 9.9339e-05 lr: 9.9339e-06  eta: 1 day, 14:44:08  time: 0.4360  data_time: 0.0092  memory: 5297  grad_norm: 183.5764  loss: 21.9009  decode.loss_cls: 1.2410  decode.loss_mask: 0.4047  decode.loss_dice: 0.4130  decode.d0.loss_cls: 2.2616  decode.d0.loss_mask: 0.4579  decode.d0.loss_dice: 0.5037  decode.d1.loss_cls: 1.3717  decode.d1.loss_mask: 0.3900  decode.d1.loss_dice: 0.4081  decode.d2.loss_cls: 1.2041  decode.d2.loss_mask: 0.3718  decode.d2.loss_dice: 0.3901  decode.d3.loss_cls: 1.2220  decode.d3.loss_mask: 0.3618  decode.d3.loss_dice: 0.3976  decode.d4.loss_cls: 1.2511  decode.d4.loss_mask: 0.3746  decode.d4.loss_dice: 0.4188  decode.d5.loss_cls: 1.2659  decode.d5.loss_mask: 0.3886  decode.d5.loss_dice: 0.4090  decode.d6.loss_cls: 1.3054  decode.d6.loss_mask: 0.3924  decode.d6.loss_dice: 0.4277  decode.d7.loss_cls: 1.3520  decode.d7.loss_mask: 0.3838  decode.d7.loss_dice: 0.4345  decode.d8.loss_cls: 1.2682  decode.d8.loss_mask: 0.3915  decode.d8.loss_dice: 0.4386
08/06 02:34:19 - mmengine - INFO - Iter(train) [  2400/320000]  base_lr: 9.9325e-05 lr: 9.9325e-06  eta: 1 day, 14:43:21  time: 0.4357  data_time: 0.0089  memory: 5275  grad_norm: 260.2106  loss: 24.4188  decode.loss_cls: 1.3201  decode.loss_mask: 0.4644  decode.loss_dice: 0.4121  decode.d0.loss_cls: 2.3247  decode.d0.loss_mask: 0.3959  decode.d0.loss_dice: 0.5258  decode.d1.loss_cls: 1.4181  decode.d1.loss_mask: 0.5202  decode.d1.loss_dice: 0.4145  decode.d2.loss_cls: 1.3320  decode.d2.loss_mask: 0.5001  decode.d2.loss_dice: 0.4422  decode.d3.loss_cls: 1.4642  decode.d3.loss_mask: 0.4998  decode.d3.loss_dice: 0.4058  decode.d4.loss_cls: 1.4369  decode.d4.loss_mask: 0.5351  decode.d4.loss_dice: 0.4488  decode.d5.loss_cls: 1.4270  decode.d5.loss_mask: 0.4899  decode.d5.loss_dice: 0.4250  decode.d6.loss_cls: 1.4145  decode.d6.loss_mask: 0.5490  decode.d6.loss_dice: 0.4447  decode.d7.loss_cls: 1.3592  decode.d7.loss_mask: 0.5599  decode.d7.loss_dice: 0.4458  decode.d8.loss_cls: 1.3556  decode.d8.loss_mask: 0.6074  decode.d8.loss_dice: 0.4801
08/06 02:34:41 - mmengine - INFO - Iter(train) [  2450/320000]  base_lr: 9.9311e-05 lr: 9.9311e-06  eta: 1 day, 14:42:37  time: 0.4362  data_time: 0.0089  memory: 5242  grad_norm: 158.0795  loss: 24.6269  decode.loss_cls: 1.4720  decode.loss_mask: 0.3563  decode.loss_dice: 0.4939  decode.d0.loss_cls: 2.3517  decode.d0.loss_mask: 0.3390  decode.d0.loss_dice: 0.6079  decode.d1.loss_cls: 1.5763  decode.d1.loss_mask: 0.3365  decode.d1.loss_dice: 0.5172  decode.d2.loss_cls: 1.4425  decode.d2.loss_mask: 0.3457  decode.d2.loss_dice: 0.5001  decode.d3.loss_cls: 1.4627  decode.d3.loss_mask: 0.3546  decode.d3.loss_dice: 0.5066  decode.d4.loss_cls: 1.4762  decode.d4.loss_mask: 0.3684  decode.d4.loss_dice: 0.5321  decode.d5.loss_cls: 1.5305  decode.d5.loss_mask: 0.3790  decode.d5.loss_dice: 0.5462  decode.d6.loss_cls: 1.4951  decode.d6.loss_mask: 0.3522  decode.d6.loss_dice: 0.5330  decode.d7.loss_cls: 1.5280  decode.d7.loss_mask: 0.3542  decode.d7.loss_dice: 0.5415  decode.d8.loss_cls: 1.4610  decode.d8.loss_mask: 0.3589  decode.d8.loss_dice: 0.5077
08/06 02:35:03 - mmengine - INFO - Iter(train) [  2500/320000]  base_lr: 9.9297e-05 lr: 9.9297e-06  eta: 1 day, 14:41:55  time: 0.4356  data_time: 0.0089  memory: 5260  grad_norm: 281.0882  loss: 21.9669  decode.loss_cls: 1.2384  decode.loss_mask: 0.4346  decode.loss_dice: 0.4393  decode.d0.loss_cls: 2.1506  decode.d0.loss_mask: 0.4667  decode.d0.loss_dice: 0.5169  decode.d1.loss_cls: 1.4068  decode.d1.loss_mask: 0.4251  decode.d1.loss_dice: 0.3957  decode.d2.loss_cls: 1.2361  decode.d2.loss_mask: 0.3786  decode.d2.loss_dice: 0.4001  decode.d3.loss_cls: 1.2841  decode.d3.loss_mask: 0.3796  decode.d3.loss_dice: 0.3928  decode.d4.loss_cls: 1.2384  decode.d4.loss_mask: 0.3814  decode.d4.loss_dice: 0.3874  decode.d5.loss_cls: 1.2425  decode.d5.loss_mask: 0.3921  decode.d5.loss_dice: 0.4154  decode.d6.loss_cls: 1.2695  decode.d6.loss_mask: 0.3806  decode.d6.loss_dice: 0.4165  decode.d7.loss_cls: 1.3035  decode.d7.loss_mask: 0.4062  decode.d7.loss_dice: 0.4539  decode.d8.loss_cls: 1.2979  decode.d8.loss_mask: 0.3858  decode.d8.loss_dice: 0.4502
08/06 02:35:25 - mmengine - INFO - Iter(train) [  2550/320000]  base_lr: 9.9283e-05 lr: 9.9283e-06  eta: 1 day, 14:41:14  time: 0.4362  data_time: 0.0089  memory: 5242  grad_norm: 157.4707  loss: 21.2082  decode.loss_cls: 1.1525  decode.loss_mask: 0.2923  decode.loss_dice: 0.4752  decode.d0.loss_cls: 2.1429  decode.d0.loss_mask: 0.2847  decode.d0.loss_dice: 0.5587  decode.d1.loss_cls: 1.3764  decode.d1.loss_mask: 0.2872  decode.d1.loss_dice: 0.4898  decode.d2.loss_cls: 1.2304  decode.d2.loss_mask: 0.2905  decode.d2.loss_dice: 0.5036  decode.d3.loss_cls: 1.2096  decode.d3.loss_mask: 0.3036  decode.d3.loss_dice: 0.5081  decode.d4.loss_cls: 1.1765  decode.d4.loss_mask: 0.3122  decode.d4.loss_dice: 0.5277  decode.d5.loss_cls: 1.2033  decode.d5.loss_mask: 0.3071  decode.d5.loss_dice: 0.5127  decode.d6.loss_cls: 1.2030  decode.d6.loss_mask: 0.3139  decode.d6.loss_dice: 0.5169  decode.d7.loss_cls: 1.1849  decode.d7.loss_mask: 0.3032  decode.d7.loss_dice: 0.5228  decode.d8.loss_cls: 1.1489  decode.d8.loss_mask: 0.3120  decode.d8.loss_dice: 0.5577
08/06 02:35:46 - mmengine - INFO - Iter(train) [  2600/320000]  base_lr: 9.9269e-05 lr: 9.9269e-06  eta: 1 day, 14:40:33  time: 0.4366  data_time: 0.0089  memory: 5242  grad_norm: 125.4297  loss: 18.9443  decode.loss_cls: 1.1110  decode.loss_mask: 0.2927  decode.loss_dice: 0.3555  decode.d0.loss_cls: 2.1152  decode.d0.loss_mask: 0.3541  decode.d0.loss_dice: 0.5275  decode.d1.loss_cls: 1.2355  decode.d1.loss_mask: 0.3399  decode.d1.loss_dice: 0.4114  decode.d2.loss_cls: 1.0570  decode.d2.loss_mask: 0.3210  decode.d2.loss_dice: 0.4088  decode.d3.loss_cls: 0.9708  decode.d3.loss_mask: 0.2955  decode.d3.loss_dice: 0.4008  decode.d4.loss_cls: 1.0488  decode.d4.loss_mask: 0.3064  decode.d4.loss_dice: 0.3902  decode.d5.loss_cls: 1.0891  decode.d5.loss_mask: 0.3118  decode.d5.loss_dice: 0.3789  decode.d6.loss_cls: 1.0222  decode.d6.loss_mask: 0.3118  decode.d6.loss_dice: 0.3640  decode.d7.loss_cls: 1.1297  decode.d7.loss_mask: 0.3045  decode.d7.loss_dice: 0.3518  decode.d8.loss_cls: 1.0651  decode.d8.loss_mask: 0.3148  decode.d8.loss_dice: 0.3584
08/06 02:36:08 - mmengine - INFO - Iter(train) [  2650/320000]  base_lr: 9.9255e-05 lr: 9.9255e-06  eta: 1 day, 14:39:54  time: 0.4353  data_time: 0.0089  memory: 5260  grad_norm: 151.8859  loss: 22.0297  decode.loss_cls: 1.2915  decode.loss_mask: 0.3573  decode.loss_dice: 0.3747  decode.d0.loss_cls: 2.2519  decode.d0.loss_mask: 0.4395  decode.d0.loss_dice: 0.4968  decode.d1.loss_cls: 1.5447  decode.d1.loss_mask: 0.3786  decode.d1.loss_dice: 0.4233  decode.d2.loss_cls: 1.2962  decode.d2.loss_mask: 0.3871  decode.d2.loss_dice: 0.4292  decode.d3.loss_cls: 1.2463  decode.d3.loss_mask: 0.3975  decode.d3.loss_dice: 0.4222  decode.d4.loss_cls: 1.2441  decode.d4.loss_mask: 0.3911  decode.d4.loss_dice: 0.4147  decode.d5.loss_cls: 1.2919  decode.d5.loss_mask: 0.3894  decode.d5.loss_dice: 0.4203  decode.d6.loss_cls: 1.2768  decode.d6.loss_mask: 0.3538  decode.d6.loss_dice: 0.3893  decode.d7.loss_cls: 1.2610  decode.d7.loss_mask: 0.3709  decode.d7.loss_dice: 0.3820  decode.d8.loss_cls: 1.2900  decode.d8.loss_mask: 0.3962  decode.d8.loss_dice: 0.4214
08/06 02:36:30 - mmengine - INFO - Iter(train) [  2700/320000]  base_lr: 9.9241e-05 lr: 9.9241e-06  eta: 1 day, 14:39:17  time: 0.4366  data_time: 0.0090  memory: 5275  grad_norm: 189.0981  loss: 18.1749  decode.loss_cls: 1.0457  decode.loss_mask: 0.3248  decode.loss_dice: 0.3329  decode.d0.loss_cls: 1.9282  decode.d0.loss_mask: 0.3579  decode.d0.loss_dice: 0.4176  decode.d1.loss_cls: 1.1914  decode.d1.loss_mask: 0.3450  decode.d1.loss_dice: 0.3578  decode.d2.loss_cls: 1.0182  decode.d2.loss_mask: 0.3323  decode.d2.loss_dice: 0.3299  decode.d3.loss_cls: 1.0435  decode.d3.loss_mask: 0.3449  decode.d3.loss_dice: 0.3328  decode.d4.loss_cls: 1.0351  decode.d4.loss_mask: 0.3698  decode.d4.loss_dice: 0.3473  decode.d5.loss_cls: 1.0136  decode.d5.loss_mask: 0.3648  decode.d5.loss_dice: 0.3316  decode.d6.loss_cls: 0.9805  decode.d6.loss_mask: 0.3388  decode.d6.loss_dice: 0.3419  decode.d7.loss_cls: 1.0014  decode.d7.loss_mask: 0.3343  decode.d7.loss_dice: 0.3579  decode.d8.loss_cls: 1.0057  decode.d8.loss_mask: 0.3274  decode.d8.loss_dice: 0.3219
08/06 02:36:52 - mmengine - INFO - Iter(train) [  2750/320000]  base_lr: 9.9227e-05 lr: 9.9227e-06  eta: 1 day, 14:38:42  time: 0.4363  data_time: 0.0092  memory: 5223  grad_norm: 147.9829  loss: 18.5319  decode.loss_cls: 0.9088  decode.loss_mask: 0.3926  decode.loss_dice: 0.4299  decode.d0.loss_cls: 1.8522  decode.d0.loss_mask: 0.4159  decode.d0.loss_dice: 0.5510  decode.d1.loss_cls: 1.0959  decode.d1.loss_mask: 0.3824  decode.d1.loss_dice: 0.4378  decode.d2.loss_cls: 0.8918  decode.d2.loss_mask: 0.3907  decode.d2.loss_dice: 0.4323  decode.d3.loss_cls: 0.9601  decode.d3.loss_mask: 0.3645  decode.d3.loss_dice: 0.3854  decode.d4.loss_cls: 0.9520  decode.d4.loss_mask: 0.3783  decode.d4.loss_dice: 0.3956  decode.d5.loss_cls: 0.9695  decode.d5.loss_mask: 0.3727  decode.d5.loss_dice: 0.4073  decode.d6.loss_cls: 0.8987  decode.d6.loss_mask: 0.3904  decode.d6.loss_dice: 0.4184  decode.d7.loss_cls: 0.9055  decode.d7.loss_mask: 0.3927  decode.d7.loss_dice: 0.4469  decode.d8.loss_cls: 0.9150  decode.d8.loss_mask: 0.3839  decode.d8.loss_dice: 0.4137
08/06 02:37:14 - mmengine - INFO - Iter(train) [  2800/320000]  base_lr: 9.9212e-05 lr: 9.9212e-06  eta: 1 day, 14:38:08  time: 0.4377  data_time: 0.0089  memory: 5236  grad_norm: 122.5971  loss: 17.8250  decode.loss_cls: 0.8552  decode.loss_mask: 0.3644  decode.loss_dice: 0.4114  decode.d0.loss_cls: 1.6876  decode.d0.loss_mask: 0.3743  decode.d0.loss_dice: 0.5150  decode.d1.loss_cls: 1.1714  decode.d1.loss_mask: 0.3492  decode.d1.loss_dice: 0.4529  decode.d2.loss_cls: 0.9354  decode.d2.loss_mask: 0.3619  decode.d2.loss_dice: 0.4634  decode.d3.loss_cls: 0.9527  decode.d3.loss_mask: 0.3414  decode.d3.loss_dice: 0.4113  decode.d4.loss_cls: 0.8303  decode.d4.loss_mask: 0.3518  decode.d4.loss_dice: 0.4243  decode.d5.loss_cls: 0.8469  decode.d5.loss_mask: 0.3529  decode.d5.loss_dice: 0.4233  decode.d6.loss_cls: 0.8865  decode.d6.loss_mask: 0.3419  decode.d6.loss_dice: 0.4004  decode.d7.loss_cls: 0.8998  decode.d7.loss_mask: 0.3508  decode.d7.loss_dice: 0.4107  decode.d8.loss_cls: 0.8898  decode.d8.loss_mask: 0.3576  decode.d8.loss_dice: 0.4103
08/06 02:37:35 - mmengine - INFO - Iter(train) [  2850/320000]  base_lr: 9.9198e-05 lr: 9.9198e-06  eta: 1 day, 14:37:36  time: 0.4369  data_time: 0.0090  memory: 5242  grad_norm: 209.3316  loss: 20.9000  decode.loss_cls: 1.0973  decode.loss_mask: 0.3984  decode.loss_dice: 0.4507  decode.d0.loss_cls: 2.0622  decode.d0.loss_mask: 0.4006  decode.d0.loss_dice: 0.5682  decode.d1.loss_cls: 1.3783  decode.d1.loss_mask: 0.3869  decode.d1.loss_dice: 0.4348  decode.d2.loss_cls: 1.1332  decode.d2.loss_mask: 0.3761  decode.d2.loss_dice: 0.4525  decode.d3.loss_cls: 1.1664  decode.d3.loss_mask: 0.3876  decode.d3.loss_dice: 0.4585  decode.d4.loss_cls: 1.1214  decode.d4.loss_mask: 0.3684  decode.d4.loss_dice: 0.4369  decode.d5.loss_cls: 1.1168  decode.d5.loss_mask: 0.3861  decode.d5.loss_dice: 0.4594  decode.d6.loss_cls: 1.0811  decode.d6.loss_mask: 0.4108  decode.d6.loss_dice: 0.4638  decode.d7.loss_cls: 1.0888  decode.d7.loss_mask: 0.3875  decode.d7.loss_dice: 0.4501  decode.d8.loss_cls: 1.1461  decode.d8.loss_mask: 0.3854  decode.d8.loss_dice: 0.4455
08/06 02:37:57 - mmengine - INFO - Iter(train) [  2900/320000]  base_lr: 9.9184e-05 lr: 9.9184e-06  eta: 1 day, 14:37:27  time: 0.4362  data_time: 0.0090  memory: 5242  grad_norm: 266.7872  loss: 20.7397  decode.loss_cls: 1.0174  decode.loss_mask: 0.4962  decode.loss_dice: 0.4798  decode.d0.loss_cls: 1.9344  decode.d0.loss_mask: 0.5107  decode.d0.loss_dice: 0.5749  decode.d1.loss_cls: 1.2550  decode.d1.loss_mask: 0.4615  decode.d1.loss_dice: 0.4824  decode.d2.loss_cls: 1.0421  decode.d2.loss_mask: 0.4850  decode.d2.loss_dice: 0.4584  decode.d3.loss_cls: 1.0251  decode.d3.loss_mask: 0.4773  decode.d3.loss_dice: 0.4563  decode.d4.loss_cls: 0.9793  decode.d4.loss_mask: 0.4862  decode.d4.loss_dice: 0.4662  decode.d5.loss_cls: 1.0014  decode.d5.loss_mask: 0.4524  decode.d5.loss_dice: 0.4689  decode.d6.loss_cls: 0.9406  decode.d6.loss_mask: 0.4530  decode.d6.loss_dice: 0.4870  decode.d7.loss_cls: 1.0229  decode.d7.loss_mask: 0.4412  decode.d7.loss_dice: 0.4706  decode.d8.loss_cls: 0.9597  decode.d8.loss_mask: 0.4907  decode.d8.loss_dice: 0.4634
08/06 02:38:19 - mmengine - INFO - Iter(train) [  2950/320000]  base_lr: 9.9170e-05 lr: 9.9170e-06  eta: 1 day, 14:36:57  time: 0.4365  data_time: 0.0090  memory: 5260  grad_norm: 160.3413  loss: 19.7996  decode.loss_cls: 1.0362  decode.loss_mask: 0.3749  decode.loss_dice: 0.4227  decode.d0.loss_cls: 2.0292  decode.d0.loss_mask: 0.3342  decode.d0.loss_dice: 0.5694  decode.d1.loss_cls: 1.2541  decode.d1.loss_mask: 0.3164  decode.d1.loss_dice: 0.4681  decode.d2.loss_cls: 1.0160  decode.d2.loss_mask: 0.2956  decode.d2.loss_dice: 0.4040  decode.d3.loss_cls: 1.0498  decode.d3.loss_mask: 0.3525  decode.d3.loss_dice: 0.4234  decode.d4.loss_cls: 1.0599  decode.d4.loss_mask: 0.3448  decode.d4.loss_dice: 0.4442  decode.d5.loss_cls: 1.0230  decode.d5.loss_mask: 0.4262  decode.d5.loss_dice: 0.4342  decode.d6.loss_cls: 1.0207  decode.d6.loss_mask: 0.4400  decode.d6.loss_dice: 0.4312  decode.d7.loss_cls: 1.0740  decode.d7.loss_mask: 0.4181  decode.d7.loss_dice: 0.4857  decode.d8.loss_cls: 1.0636  decode.d8.loss_mask: 0.3708  decode.d8.loss_dice: 0.4167
08/06 02:38:41 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 02:38:41 - mmengine - INFO - Iter(train) [  3000/320000]  base_lr: 9.9156e-05 lr: 9.9156e-06  eta: 1 day, 14:36:24  time: 0.4365  data_time: 0.0087  memory: 5275  grad_norm: 153.5276  loss: 19.0304  decode.loss_cls: 1.2189  decode.loss_mask: 0.2568  decode.loss_dice: 0.3640  decode.d0.loss_cls: 2.0645  decode.d0.loss_mask: 0.3146  decode.d0.loss_dice: 0.4696  decode.d1.loss_cls: 1.4382  decode.d1.loss_mask: 0.2588  decode.d1.loss_dice: 0.3552  decode.d2.loss_cls: 1.2708  decode.d2.loss_mask: 0.2444  decode.d2.loss_dice: 0.3303  decode.d3.loss_cls: 1.1784  decode.d3.loss_mask: 0.2402  decode.d3.loss_dice: 0.3303  decode.d4.loss_cls: 1.1493  decode.d4.loss_mask: 0.2415  decode.d4.loss_dice: 0.3396  decode.d5.loss_cls: 1.1183  decode.d5.loss_mask: 0.2483  decode.d5.loss_dice: 0.3528  decode.d6.loss_cls: 1.1088  decode.d6.loss_mask: 0.2503  decode.d6.loss_dice: 0.3639  decode.d7.loss_cls: 1.1854  decode.d7.loss_mask: 0.2599  decode.d7.loss_dice: 0.3640  decode.d8.loss_cls: 1.1256  decode.d8.loss_mask: 0.2447  decode.d8.loss_dice: 0.3431
08/06 02:39:03 - mmengine - INFO - Iter(train) [  3050/320000]  base_lr: 9.9142e-05 lr: 9.9142e-06  eta: 1 day, 14:35:54  time: 0.4377  data_time: 0.0091  memory: 5260  grad_norm: 122.1484  loss: 17.0549  decode.loss_cls: 0.9014  decode.loss_mask: 0.2679  decode.loss_dice: 0.3070  decode.d0.loss_cls: 1.9468  decode.d0.loss_mask: 0.3299  decode.d0.loss_dice: 0.4435  decode.d1.loss_cls: 1.2507  decode.d1.loss_mask: 0.2733  decode.d1.loss_dice: 0.3297  decode.d2.loss_cls: 0.9934  decode.d2.loss_mask: 0.2794  decode.d2.loss_dice: 0.3397  decode.d3.loss_cls: 0.9700  decode.d3.loss_mask: 0.3122  decode.d3.loss_dice: 0.3301  decode.d4.loss_cls: 0.9773  decode.d4.loss_mask: 0.2738  decode.d4.loss_dice: 0.3163  decode.d5.loss_cls: 1.0022  decode.d5.loss_mask: 0.2707  decode.d5.loss_dice: 0.3212  decode.d6.loss_cls: 0.9059  decode.d6.loss_mask: 0.3103  decode.d6.loss_dice: 0.3404  decode.d7.loss_cls: 0.9537  decode.d7.loss_mask: 0.2768  decode.d7.loss_dice: 0.3033  decode.d8.loss_cls: 0.9371  decode.d8.loss_mask: 0.2710  decode.d8.loss_dice: 0.3200
08/06 02:39:25 - mmengine - INFO - Iter(train) [  3100/320000]  base_lr: 9.9128e-05 lr: 9.9128e-06  eta: 1 day, 14:35:23  time: 0.4365  data_time: 0.0089  memory: 5236  grad_norm: 188.8790  loss: 18.8788  decode.loss_cls: 0.9175  decode.loss_mask: 0.4236  decode.loss_dice: 0.5123  decode.d0.loss_cls: 1.7378  decode.d0.loss_mask: 0.3978  decode.d0.loss_dice: 0.5343  decode.d1.loss_cls: 1.0478  decode.d1.loss_mask: 0.4220  decode.d1.loss_dice: 0.4710  decode.d2.loss_cls: 0.8593  decode.d2.loss_mask: 0.4029  decode.d2.loss_dice: 0.4592  decode.d3.loss_cls: 0.8696  decode.d3.loss_mask: 0.4236  decode.d3.loss_dice: 0.4678  decode.d4.loss_cls: 0.8609  decode.d4.loss_mask: 0.4137  decode.d4.loss_dice: 0.4583  decode.d5.loss_cls: 0.8541  decode.d5.loss_mask: 0.4271  decode.d5.loss_dice: 0.4874  decode.d6.loss_cls: 0.8951  decode.d6.loss_mask: 0.4192  decode.d6.loss_dice: 0.4974  decode.d7.loss_cls: 0.8787  decode.d7.loss_mask: 0.4253  decode.d7.loss_dice: 0.5069  decode.d8.loss_cls: 0.8883  decode.d8.loss_mask: 0.4170  decode.d8.loss_dice: 0.5028
08/06 02:39:47 - mmengine - INFO - Iter(train) [  3150/320000]  base_lr: 9.9114e-05 lr: 9.9114e-06  eta: 1 day, 14:34:51  time: 0.4354  data_time: 0.0087  memory: 5260  grad_norm: 170.7158  loss: 17.8461  decode.loss_cls: 0.8839  decode.loss_mask: 0.3987  decode.loss_dice: 0.4231  decode.d0.loss_cls: 1.6078  decode.d0.loss_mask: 0.3819  decode.d0.loss_dice: 0.5051  decode.d1.loss_cls: 0.9785  decode.d1.loss_mask: 0.3828  decode.d1.loss_dice: 0.4360  decode.d2.loss_cls: 0.8692  decode.d2.loss_mask: 0.3922  decode.d2.loss_dice: 0.4324  decode.d3.loss_cls: 0.8558  decode.d3.loss_mask: 0.3877  decode.d3.loss_dice: 0.4282  decode.d4.loss_cls: 0.9014  decode.d4.loss_mask: 0.3838  decode.d4.loss_dice: 0.4306  decode.d5.loss_cls: 0.8813  decode.d5.loss_mask: 0.3933  decode.d5.loss_dice: 0.4236  decode.d6.loss_cls: 0.8698  decode.d6.loss_mask: 0.3794  decode.d6.loss_dice: 0.3967  decode.d7.loss_cls: 0.9183  decode.d7.loss_mask: 0.3933  decode.d7.loss_dice: 0.4229  decode.d8.loss_cls: 0.8927  decode.d8.loss_mask: 0.3888  decode.d8.loss_dice: 0.4067
08/06 02:39:49 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 02:40:08 - mmengine - INFO - Iter(train) [  3200/320000]  base_lr: 9.9100e-05 lr: 9.9100e-06  eta: 1 day, 14:34:20  time: 0.4367  data_time: 0.0090  memory: 5242  grad_norm: 134.4741  loss: 20.1223  decode.loss_cls: 1.0675  decode.loss_mask: 0.3907  decode.loss_dice: 0.4273  decode.d0.loss_cls: 1.9444  decode.d0.loss_mask: 0.3864  decode.d0.loss_dice: 0.5082  decode.d1.loss_cls: 1.1845  decode.d1.loss_mask: 0.3896  decode.d1.loss_dice: 0.4490  decode.d2.loss_cls: 1.1123  decode.d2.loss_mask: 0.3618  decode.d2.loss_dice: 0.4498  decode.d3.loss_cls: 1.1121  decode.d3.loss_mask: 0.3776  decode.d3.loss_dice: 0.4758  decode.d4.loss_cls: 1.0776  decode.d4.loss_mask: 0.3955  decode.d4.loss_dice: 0.4549  decode.d5.loss_cls: 1.0280  decode.d5.loss_mask: 0.3943  decode.d5.loss_dice: 0.4416  decode.d6.loss_cls: 1.0843  decode.d6.loss_mask: 0.3963  decode.d6.loss_dice: 0.4517  decode.d7.loss_cls: 1.0618  decode.d7.loss_mask: 0.3927  decode.d7.loss_dice: 0.4424  decode.d8.loss_cls: 1.0691  decode.d8.loss_mask: 0.3743  decode.d8.loss_dice: 0.4209
08/06 02:40:30 - mmengine - INFO - Iter(train) [  3250/320000]  base_lr: 9.9086e-05 lr: 9.9086e-06  eta: 1 day, 14:33:49  time: 0.4374  data_time: 0.0089  memory: 5242  grad_norm: 137.8478  loss: 18.6345  decode.loss_cls: 0.9668  decode.loss_mask: 0.3606  decode.loss_dice: 0.4028  decode.d0.loss_cls: 1.8167  decode.d0.loss_mask: 0.3904  decode.d0.loss_dice: 0.5243  decode.d1.loss_cls: 1.1182  decode.d1.loss_mask: 0.3310  decode.d1.loss_dice: 0.4182  decode.d2.loss_cls: 1.0669  decode.d2.loss_mask: 0.3124  decode.d2.loss_dice: 0.3762  decode.d3.loss_cls: 1.0605  decode.d3.loss_mask: 0.3124  decode.d3.loss_dice: 0.3771  decode.d4.loss_cls: 1.0961  decode.d4.loss_mask: 0.3668  decode.d4.loss_dice: 0.3941  decode.d5.loss_cls: 0.9958  decode.d5.loss_mask: 0.3458  decode.d5.loss_dice: 0.4126  decode.d6.loss_cls: 0.9590  decode.d6.loss_mask: 0.3543  decode.d6.loss_dice: 0.4141  decode.d7.loss_cls: 0.9859  decode.d7.loss_mask: 0.3790  decode.d7.loss_dice: 0.4076  decode.d8.loss_cls: 0.8840  decode.d8.loss_mask: 0.3764  decode.d8.loss_dice: 0.4286
08/06 02:40:52 - mmengine - INFO - Iter(train) [  3300/320000]  base_lr: 9.9072e-05 lr: 9.9072e-06  eta: 1 day, 14:33:17  time: 0.4360  data_time: 0.0087  memory: 5260  grad_norm: 162.3457  loss: 17.1719  decode.loss_cls: 0.9185  decode.loss_mask: 0.3578  decode.loss_dice: 0.3509  decode.d0.loss_cls: 1.8922  decode.d0.loss_mask: 0.3496  decode.d0.loss_dice: 0.4503  decode.d1.loss_cls: 1.1739  decode.d1.loss_mask: 0.3539  decode.d1.loss_dice: 0.3523  decode.d2.loss_cls: 0.9619  decode.d2.loss_mask: 0.3330  decode.d2.loss_dice: 0.3434  decode.d3.loss_cls: 0.8396  decode.d3.loss_mask: 0.3351  decode.d3.loss_dice: 0.3236  decode.d4.loss_cls: 0.8672  decode.d4.loss_mask: 0.3420  decode.d4.loss_dice: 0.3283  decode.d5.loss_cls: 0.9550  decode.d5.loss_mask: 0.3554  decode.d5.loss_dice: 0.3394  decode.d6.loss_cls: 0.8783  decode.d6.loss_mask: 0.3281  decode.d6.loss_dice: 0.3121  decode.d7.loss_cls: 0.8807  decode.d7.loss_mask: 0.3416  decode.d7.loss_dice: 0.3267  decode.d8.loss_cls: 0.8934  decode.d8.loss_mask: 0.3410  decode.d8.loss_dice: 0.3466
08/06 02:41:14 - mmengine - INFO - Iter(train) [  3350/320000]  base_lr: 9.9058e-05 lr: 9.9058e-06  eta: 1 day, 14:32:46  time: 0.4358  data_time: 0.0088  memory: 5299  grad_norm: 176.6026  loss: 19.2893  decode.loss_cls: 0.9383  decode.loss_mask: 0.5314  decode.loss_dice: 0.4399  decode.d0.loss_cls: 1.8112  decode.d0.loss_mask: 0.4409  decode.d0.loss_dice: 0.4431  decode.d1.loss_cls: 1.2912  decode.d1.loss_mask: 0.4493  decode.d1.loss_dice: 0.4352  decode.d2.loss_cls: 0.9927  decode.d2.loss_mask: 0.4230  decode.d2.loss_dice: 0.4113  decode.d3.loss_cls: 0.8993  decode.d3.loss_mask: 0.4098  decode.d3.loss_dice: 0.4090  decode.d4.loss_cls: 0.8772  decode.d4.loss_mask: 0.4495  decode.d4.loss_dice: 0.4197  decode.d5.loss_cls: 0.8229  decode.d5.loss_mask: 0.4770  decode.d5.loss_dice: 0.4375  decode.d6.loss_cls: 0.8749  decode.d6.loss_mask: 0.4813  decode.d6.loss_dice: 0.4284  decode.d7.loss_cls: 0.8750  decode.d7.loss_mask: 0.5055  decode.d7.loss_dice: 0.4435  decode.d8.loss_cls: 0.9404  decode.d8.loss_mask: 0.5003  decode.d8.loss_dice: 0.4305
08/06 02:41:36 - mmengine - INFO - Iter(train) [  3400/320000]  base_lr: 9.9044e-05 lr: 9.9044e-06  eta: 1 day, 14:32:11  time: 0.4351  data_time: 0.0086  memory: 5242  grad_norm: 154.5101  loss: 16.6996  decode.loss_cls: 0.8744  decode.loss_mask: 0.3249  decode.loss_dice: 0.3858  decode.d0.loss_cls: 1.6003  decode.d0.loss_mask: 0.3454  decode.d0.loss_dice: 0.5028  decode.d1.loss_cls: 0.9059  decode.d1.loss_mask: 0.3237  decode.d1.loss_dice: 0.4010  decode.d2.loss_cls: 0.8708  decode.d2.loss_mask: 0.3355  decode.d2.loss_dice: 0.4058  decode.d3.loss_cls: 0.8491  decode.d3.loss_mask: 0.3173  decode.d3.loss_dice: 0.4084  decode.d4.loss_cls: 0.9081  decode.d4.loss_mask: 0.3167  decode.d4.loss_dice: 0.4135  decode.d5.loss_cls: 0.8253  decode.d5.loss_mask: 0.3168  decode.d5.loss_dice: 0.4081  decode.d6.loss_cls: 0.8201  decode.d6.loss_mask: 0.3156  decode.d6.loss_dice: 0.3838  decode.d7.loss_cls: 0.8249  decode.d7.loss_mask: 0.3203  decode.d7.loss_dice: 0.3945  decode.d8.loss_cls: 0.8798  decode.d8.loss_mask: 0.3244  decode.d8.loss_dice: 0.3967
08/06 02:41:57 - mmengine - INFO - Iter(train) [  3450/320000]  base_lr: 9.9029e-05 lr: 9.9029e-06  eta: 1 day, 14:31:39  time: 0.4360  data_time: 0.0087  memory: 5242  grad_norm: 139.1182  loss: 13.2756  decode.loss_cls: 0.5302  decode.loss_mask: 0.2764  decode.loss_dice: 0.3497  decode.d0.loss_cls: 1.4460  decode.d0.loss_mask: 0.3401  decode.d0.loss_dice: 0.4661  decode.d1.loss_cls: 0.8436  decode.d1.loss_mask: 0.2750  decode.d1.loss_dice: 0.3722  decode.d2.loss_cls: 0.6783  decode.d2.loss_mask: 0.2641  decode.d2.loss_dice: 0.3048  decode.d3.loss_cls: 0.5526  decode.d3.loss_mask: 0.2718  decode.d3.loss_dice: 0.3483  decode.d4.loss_cls: 0.5489  decode.d4.loss_mask: 0.2630  decode.d4.loss_dice: 0.3508  decode.d5.loss_cls: 0.5731  decode.d5.loss_mask: 0.2737  decode.d5.loss_dice: 0.3562  decode.d6.loss_cls: 0.5629  decode.d6.loss_mask: 0.2694  decode.d6.loss_dice: 0.3626  decode.d7.loss_cls: 0.5457  decode.d7.loss_mask: 0.2706  decode.d7.loss_dice: 0.3762  decode.d8.loss_cls: 0.5772  decode.d8.loss_mask: 0.2763  decode.d8.loss_dice: 0.3499
08/06 02:42:19 - mmengine - INFO - Iter(train) [  3500/320000]  base_lr: 9.9015e-05 lr: 9.9015e-06  eta: 1 day, 14:31:09  time: 0.4359  data_time: 0.0086  memory: 5299  grad_norm: 187.9484  loss: 21.0580  decode.loss_cls: 1.0992  decode.loss_mask: 0.4901  decode.loss_dice: 0.3954  decode.d0.loss_cls: 1.9748  decode.d0.loss_mask: 0.4088  decode.d0.loss_dice: 0.4570  decode.d1.loss_cls: 1.2819  decode.d1.loss_mask: 0.3644  decode.d1.loss_dice: 0.3543  decode.d2.loss_cls: 1.1406  decode.d2.loss_mask: 0.4392  decode.d2.loss_dice: 0.3770  decode.d3.loss_cls: 1.2479  decode.d3.loss_mask: 0.3779  decode.d3.loss_dice: 0.3714  decode.d4.loss_cls: 1.3008  decode.d4.loss_mask: 0.3958  decode.d4.loss_dice: 0.3986  decode.d5.loss_cls: 1.2533  decode.d5.loss_mask: 0.4712  decode.d5.loss_dice: 0.3873  decode.d6.loss_cls: 1.2102  decode.d6.loss_mask: 0.3953  decode.d6.loss_dice: 0.3951  decode.d7.loss_cls: 1.2098  decode.d7.loss_mask: 0.4142  decode.d7.loss_dice: 0.3917  decode.d8.loss_cls: 1.1612  decode.d8.loss_mask: 0.5064  decode.d8.loss_dice: 0.3874
08/06 02:42:41 - mmengine - INFO - Iter(train) [  3550/320000]  base_lr: 9.9001e-05 lr: 9.9001e-06  eta: 1 day, 14:30:41  time: 0.4366  data_time: 0.0090  memory: 5275  grad_norm: 159.7399  loss: 18.3237  decode.loss_cls: 0.9012  decode.loss_mask: 0.3144  decode.loss_dice: 0.4702  decode.d0.loss_cls: 1.6582  decode.d0.loss_mask: 0.3448  decode.d0.loss_dice: 0.5608  decode.d1.loss_cls: 1.1119  decode.d1.loss_mask: 0.3155  decode.d1.loss_dice: 0.4983  decode.d2.loss_cls: 0.9695  decode.d2.loss_mask: 0.3076  decode.d2.loss_dice: 0.4627  decode.d3.loss_cls: 0.9054  decode.d3.loss_mask: 0.3373  decode.d3.loss_dice: 0.4690  decode.d4.loss_cls: 0.9533  decode.d4.loss_mask: 0.2893  decode.d4.loss_dice: 0.4266  decode.d5.loss_cls: 0.8933  decode.d5.loss_mask: 0.3430  decode.d5.loss_dice: 0.4802  decode.d6.loss_cls: 0.8906  decode.d6.loss_mask: 0.3401  decode.d6.loss_dice: 0.5018  decode.d7.loss_cls: 1.0097  decode.d7.loss_mask: 0.3400  decode.d7.loss_dice: 0.4818  decode.d8.loss_cls: 0.9411  decode.d8.loss_mask: 0.3359  decode.d8.loss_dice: 0.4703
08/06 02:43:03 - mmengine - INFO - Iter(train) [  3600/320000]  base_lr: 9.8987e-05 lr: 9.8987e-06  eta: 1 day, 14:30:12  time: 0.4368  data_time: 0.0090  memory: 5208  grad_norm: 104.1831  loss: 15.5666  decode.loss_cls: 0.8749  decode.loss_mask: 0.3105  decode.loss_dice: 0.3162  decode.d0.loss_cls: 1.6797  decode.d0.loss_mask: 0.3160  decode.d0.loss_dice: 0.3348  decode.d1.loss_cls: 0.9801  decode.d1.loss_mask: 0.3140  decode.d1.loss_dice: 0.3046  decode.d2.loss_cls: 0.8981  decode.d2.loss_mask: 0.3045  decode.d2.loss_dice: 0.3111  decode.d3.loss_cls: 0.8161  decode.d3.loss_mask: 0.3095  decode.d3.loss_dice: 0.3070  decode.d4.loss_cls: 0.8013  decode.d4.loss_mask: 0.3179  decode.d4.loss_dice: 0.3165  decode.d5.loss_cls: 0.8508  decode.d5.loss_mask: 0.3141  decode.d5.loss_dice: 0.3050  decode.d6.loss_cls: 0.7649  decode.d6.loss_mask: 0.3224  decode.d6.loss_dice: 0.3318  decode.d7.loss_cls: 0.8062  decode.d7.loss_mask: 0.3199  decode.d7.loss_dice: 0.3306  decode.d8.loss_cls: 0.7782  decode.d8.loss_mask: 0.3108  decode.d8.loss_dice: 0.3190
08/06 02:43:25 - mmengine - INFO - Iter(train) [  3650/320000]  base_lr: 9.8973e-05 lr: 9.8973e-06  eta: 1 day, 14:29:45  time: 0.4372  data_time: 0.0091  memory: 5275  grad_norm: 94.4768  loss: 16.9315  decode.loss_cls: 0.9100  decode.loss_mask: 0.2705  decode.loss_dice: 0.3624  decode.d0.loss_cls: 1.5407  decode.d0.loss_mask: 0.3177  decode.d0.loss_dice: 0.4702  decode.d1.loss_cls: 1.1052  decode.d1.loss_mask: 0.2764  decode.d1.loss_dice: 0.4033  decode.d2.loss_cls: 1.1073  decode.d2.loss_mask: 0.2713  decode.d2.loss_dice: 0.3818  decode.d3.loss_cls: 0.9341  decode.d3.loss_mask: 0.2694  decode.d3.loss_dice: 0.3863  decode.d4.loss_cls: 0.9309  decode.d4.loss_mask: 0.2746  decode.d4.loss_dice: 0.3540  decode.d5.loss_cls: 0.9932  decode.d5.loss_mask: 0.2721  decode.d5.loss_dice: 0.3630  decode.d6.loss_cls: 0.9569  decode.d6.loss_mask: 0.2705  decode.d6.loss_dice: 0.3686  decode.d7.loss_cls: 0.9539  decode.d7.loss_mask: 0.2658  decode.d7.loss_dice: 0.3512  decode.d8.loss_cls: 0.9549  decode.d8.loss_mask: 0.2667  decode.d8.loss_dice: 0.3486
08/06 02:43:47 - mmengine - INFO - Iter(train) [  3700/320000]  base_lr: 9.8959e-05 lr: 9.8959e-06  eta: 1 day, 14:29:17  time: 0.4372  data_time: 0.0090  memory: 5240  grad_norm: 153.6857  loss: 19.1663  decode.loss_cls: 0.9868  decode.loss_mask: 0.3950  decode.loss_dice: 0.3540  decode.d0.loss_cls: 1.8524  decode.d0.loss_mask: 0.3653  decode.d0.loss_dice: 0.4720  decode.d1.loss_cls: 1.2586  decode.d1.loss_mask: 0.3767  decode.d1.loss_dice: 0.3850  decode.d2.loss_cls: 1.0319  decode.d2.loss_mask: 0.3661  decode.d2.loss_dice: 0.3984  decode.d3.loss_cls: 1.0933  decode.d3.loss_mask: 0.3641  decode.d3.loss_dice: 0.3721  decode.d4.loss_cls: 1.0452  decode.d4.loss_mask: 0.3798  decode.d4.loss_dice: 0.4014  decode.d5.loss_cls: 1.0842  decode.d5.loss_mask: 0.4445  decode.d5.loss_dice: 0.3902  decode.d6.loss_cls: 0.9992  decode.d6.loss_mask: 0.4077  decode.d6.loss_dice: 0.3756  decode.d7.loss_cls: 0.9857  decode.d7.loss_mask: 0.4093  decode.d7.loss_dice: 0.3541  decode.d8.loss_cls: 1.0250  decode.d8.loss_mask: 0.4229  decode.d8.loss_dice: 0.3698
08/06 02:44:08 - mmengine - INFO - Iter(train) [  3750/320000]  base_lr: 9.8945e-05 lr: 9.8945e-06  eta: 1 day, 14:28:52  time: 0.4374  data_time: 0.0090  memory: 5240  grad_norm: 122.9731  loss: 16.4976  decode.loss_cls: 0.8622  decode.loss_mask: 0.3138  decode.loss_dice: 0.3896  decode.d0.loss_cls: 1.6646  decode.d0.loss_mask: 0.3126  decode.d0.loss_dice: 0.4242  decode.d1.loss_cls: 1.1221  decode.d1.loss_mask: 0.3071  decode.d1.loss_dice: 0.3546  decode.d2.loss_cls: 0.8716  decode.d2.loss_mask: 0.3160  decode.d2.loss_dice: 0.3617  decode.d3.loss_cls: 0.8152  decode.d3.loss_mask: 0.3105  decode.d3.loss_dice: 0.3831  decode.d4.loss_cls: 0.8524  decode.d4.loss_mask: 0.3312  decode.d4.loss_dice: 0.3865  decode.d5.loss_cls: 0.7874  decode.d5.loss_mask: 0.3619  decode.d5.loss_dice: 0.3858  decode.d6.loss_cls: 0.7441  decode.d6.loss_mask: 0.3543  decode.d6.loss_dice: 0.3908  decode.d7.loss_cls: 0.8763  decode.d7.loss_mask: 0.3180  decode.d7.loss_dice: 0.3687  decode.d8.loss_cls: 0.8232  decode.d8.loss_mask: 0.3174  decode.d8.loss_dice: 0.3908
08/06 02:44:30 - mmengine - INFO - Iter(train) [  3800/320000]  base_lr: 9.8931e-05 lr: 9.8931e-06  eta: 1 day, 14:28:25  time: 0.4372  data_time: 0.0091  memory: 5275  grad_norm: 180.8493  loss: 16.7668  decode.loss_cls: 0.8435  decode.loss_mask: 0.3146  decode.loss_dice: 0.4069  decode.d0.loss_cls: 1.7005  decode.d0.loss_mask: 0.3343  decode.d0.loss_dice: 0.5340  decode.d1.loss_cls: 1.0172  decode.d1.loss_mask: 0.2836  decode.d1.loss_dice: 0.3786  decode.d2.loss_cls: 0.8957  decode.d2.loss_mask: 0.2978  decode.d2.loss_dice: 0.4087  decode.d3.loss_cls: 0.8870  decode.d3.loss_mask: 0.2805  decode.d3.loss_dice: 0.3728  decode.d4.loss_cls: 0.8635  decode.d4.loss_mask: 0.3090  decode.d4.loss_dice: 0.3865  decode.d5.loss_cls: 0.8266  decode.d5.loss_mask: 0.3144  decode.d5.loss_dice: 0.3970  decode.d6.loss_cls: 0.8374  decode.d6.loss_mask: 0.3081  decode.d6.loss_dice: 0.3713  decode.d7.loss_cls: 0.9018  decode.d7.loss_mask: 0.3169  decode.d7.loss_dice: 0.4025  decode.d8.loss_cls: 0.8607  decode.d8.loss_mask: 0.3114  decode.d8.loss_dice: 0.4040
08/06 02:44:52 - mmengine - INFO - Iter(train) [  3850/320000]  base_lr: 9.8917e-05 lr: 9.8917e-06  eta: 1 day, 14:27:58  time: 0.4362  data_time: 0.0091  memory: 5260  grad_norm: 194.5312  loss: 18.2913  decode.loss_cls: 0.8007  decode.loss_mask: 0.4243  decode.loss_dice: 0.4029  decode.d0.loss_cls: 1.5947  decode.d0.loss_mask: 0.4606  decode.d0.loss_dice: 0.4636  decode.d1.loss_cls: 1.0504  decode.d1.loss_mask: 0.4659  decode.d1.loss_dice: 0.3737  decode.d2.loss_cls: 0.9266  decode.d2.loss_mask: 0.4282  decode.d2.loss_dice: 0.3901  decode.d3.loss_cls: 0.8508  decode.d3.loss_mask: 0.4283  decode.d3.loss_dice: 0.3822  decode.d4.loss_cls: 0.9018  decode.d4.loss_mask: 0.4868  decode.d4.loss_dice: 0.4250  decode.d5.loss_cls: 0.8983  decode.d5.loss_mask: 0.4429  decode.d5.loss_dice: 0.4003  decode.d6.loss_cls: 0.9071  decode.d6.loss_mask: 0.4750  decode.d6.loss_dice: 0.4124  decode.d7.loss_cls: 0.8756  decode.d7.loss_mask: 0.4869  decode.d7.loss_dice: 0.3804  decode.d8.loss_cls: 0.7853  decode.d8.loss_mask: 0.5378  decode.d8.loss_dice: 0.4325
08/06 02:45:14 - mmengine - INFO - Iter(train) [  3900/320000]  base_lr: 9.8903e-05 lr: 9.8903e-06  eta: 1 day, 14:27:30  time: 0.4362  data_time: 0.0091  memory: 5240  grad_norm: 165.5418  loss: 16.5522  decode.loss_cls: 0.7971  decode.loss_mask: 0.3423  decode.loss_dice: 0.4551  decode.d0.loss_cls: 1.5283  decode.d0.loss_mask: 0.3497  decode.d0.loss_dice: 0.4733  decode.d1.loss_cls: 0.9150  decode.d1.loss_mask: 0.3406  decode.d1.loss_dice: 0.4212  decode.d2.loss_cls: 0.7205  decode.d2.loss_mask: 0.3448  decode.d2.loss_dice: 0.4591  decode.d3.loss_cls: 0.7443  decode.d3.loss_mask: 0.3422  decode.d3.loss_dice: 0.4534  decode.d4.loss_cls: 0.7716  decode.d4.loss_mask: 0.3455  decode.d4.loss_dice: 0.4779  decode.d5.loss_cls: 0.7764  decode.d5.loss_mask: 0.3404  decode.d5.loss_dice: 0.4510  decode.d6.loss_cls: 0.7616  decode.d6.loss_mask: 0.3349  decode.d6.loss_dice: 0.4522  decode.d7.loss_cls: 0.7942  decode.d7.loss_mask: 0.3402  decode.d7.loss_dice: 0.4451  decode.d8.loss_cls: 0.7794  decode.d8.loss_mask: 0.3373  decode.d8.loss_dice: 0.4576
08/06 02:45:36 - mmengine - INFO - Iter(train) [  3950/320000]  base_lr: 9.8889e-05 lr: 9.8889e-06  eta: 1 day, 14:27:03  time: 0.4366  data_time: 0.0090  memory: 5242  grad_norm: 182.1318  loss: 17.3918  decode.loss_cls: 0.7819  decode.loss_mask: 0.3804  decode.loss_dice: 0.4432  decode.d0.loss_cls: 1.8660  decode.d0.loss_mask: 0.4590  decode.d0.loss_dice: 0.5479  decode.d1.loss_cls: 1.0097  decode.d1.loss_mask: 0.3593  decode.d1.loss_dice: 0.4566  decode.d2.loss_cls: 0.8050  decode.d2.loss_mask: 0.3530  decode.d2.loss_dice: 0.4402  decode.d3.loss_cls: 0.7822  decode.d3.loss_mask: 0.3599  decode.d3.loss_dice: 0.4321  decode.d4.loss_cls: 0.8296  decode.d4.loss_mask: 0.3679  decode.d4.loss_dice: 0.4442  decode.d5.loss_cls: 0.7601  decode.d5.loss_mask: 0.3698  decode.d5.loss_dice: 0.4481  decode.d6.loss_cls: 0.8333  decode.d6.loss_mask: 0.3487  decode.d6.loss_dice: 0.4202  decode.d7.loss_cls: 0.7656  decode.d7.loss_mask: 0.3500  decode.d7.loss_dice: 0.4137  decode.d8.loss_cls: 0.7451  decode.d8.loss_mask: 0.3787  decode.d8.loss_dice: 0.4402
08/06 02:45:58 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 02:45:58 - mmengine - INFO - Iter(train) [  4000/320000]  base_lr: 9.8875e-05 lr: 9.8875e-06  eta: 1 day, 14:26:37  time: 0.4368  data_time: 0.0091  memory: 5242  grad_norm: 138.6506  loss: 14.9905  decode.loss_cls: 0.5888  decode.loss_mask: 0.3452  decode.loss_dice: 0.4228  decode.d0.loss_cls: 1.4190  decode.d0.loss_mask: 0.3528  decode.d0.loss_dice: 0.4575  decode.d1.loss_cls: 0.8075  decode.d1.loss_mask: 0.3476  decode.d1.loss_dice: 0.4236  decode.d2.loss_cls: 0.6837  decode.d2.loss_mask: 0.3414  decode.d2.loss_dice: 0.4472  decode.d3.loss_cls: 0.6112  decode.d3.loss_mask: 0.3437  decode.d3.loss_dice: 0.4123  decode.d4.loss_cls: 0.5690  decode.d4.loss_mask: 0.3502  decode.d4.loss_dice: 0.4272  decode.d5.loss_cls: 0.6026  decode.d5.loss_mask: 0.3441  decode.d5.loss_dice: 0.4074  decode.d6.loss_cls: 0.6400  decode.d6.loss_mask: 0.3477  decode.d6.loss_dice: 0.4454  decode.d7.loss_cls: 0.6874  decode.d7.loss_mask: 0.3534  decode.d7.loss_dice: 0.4307  decode.d8.loss_cls: 0.6090  decode.d8.loss_mask: 0.3506  decode.d8.loss_dice: 0.4217
08/06 02:46:19 - mmengine - INFO - Iter(train) [  4050/320000]  base_lr: 9.8860e-05 lr: 9.8860e-06  eta: 1 day, 14:26:10  time: 0.4369  data_time: 0.0088  memory: 5242  grad_norm: 164.3612  loss: 14.0967  decode.loss_cls: 0.6597  decode.loss_mask: 0.2241  decode.loss_dice: 0.3530  decode.d0.loss_cls: 1.7405  decode.d0.loss_mask: 0.2215  decode.d0.loss_dice: 0.4357  decode.d1.loss_cls: 0.9760  decode.d1.loss_mask: 0.2096  decode.d1.loss_dice: 0.3429  decode.d2.loss_cls: 0.7358  decode.d2.loss_mask: 0.2139  decode.d2.loss_dice: 0.3591  decode.d3.loss_cls: 0.6717  decode.d3.loss_mask: 0.2140  decode.d3.loss_dice: 0.3605  decode.d4.loss_cls: 0.7608  decode.d4.loss_mask: 0.2108  decode.d4.loss_dice: 0.3397  decode.d5.loss_cls: 0.7089  decode.d5.loss_mask: 0.2159  decode.d5.loss_dice: 0.3622  decode.d6.loss_cls: 0.7516  decode.d6.loss_mask: 0.2135  decode.d6.loss_dice: 0.3469  decode.d7.loss_cls: 0.7004  decode.d7.loss_mask: 0.2191  decode.d7.loss_dice: 0.3547  decode.d8.loss_cls: 0.6272  decode.d8.loss_mask: 0.2138  decode.d8.loss_dice: 0.3532
08/06 02:46:41 - mmengine - INFO - Iter(train) [  4100/320000]  base_lr: 9.8846e-05 lr: 9.8846e-06  eta: 1 day, 14:25:44  time: 0.4363  data_time: 0.0090  memory: 5224  grad_norm: 229.9645  loss: 18.2265  decode.loss_cls: 0.9299  decode.loss_mask: 0.3128  decode.loss_dice: 0.4528  decode.d0.loss_cls: 1.7011  decode.d0.loss_mask: 0.3320  decode.d0.loss_dice: 0.5107  decode.d1.loss_cls: 1.1771  decode.d1.loss_mask: 0.3050  decode.d1.loss_dice: 0.4633  decode.d2.loss_cls: 0.9560  decode.d2.loss_mask: 0.3025  decode.d2.loss_dice: 0.4635  decode.d3.loss_cls: 0.9185  decode.d3.loss_mask: 0.2981  decode.d3.loss_dice: 0.4373  decode.d4.loss_cls: 0.9026  decode.d4.loss_mask: 0.3107  decode.d4.loss_dice: 0.4598  decode.d5.loss_cls: 0.9708  decode.d5.loss_mask: 0.2938  decode.d5.loss_dice: 0.4628  decode.d6.loss_cls: 0.9667  decode.d6.loss_mask: 0.3054  decode.d6.loss_dice: 0.4836  decode.d7.loss_cls: 0.9898  decode.d7.loss_mask: 0.2997  decode.d7.loss_dice: 0.4520  decode.d8.loss_cls: 0.9758  decode.d8.loss_mask: 0.3194  decode.d8.loss_dice: 0.4730
08/06 02:47:03 - mmengine - INFO - Iter(train) [  4150/320000]  base_lr: 9.8832e-05 lr: 9.8832e-06  eta: 1 day, 14:25:16  time: 0.4370  data_time: 0.0090  memory: 5258  grad_norm: 129.0028  loss: 17.1109  decode.loss_cls: 0.8448  decode.loss_mask: 0.4084  decode.loss_dice: 0.4099  decode.d0.loss_cls: 1.7343  decode.d0.loss_mask: 0.3757  decode.d0.loss_dice: 0.4465  decode.d1.loss_cls: 1.0392  decode.d1.loss_mask: 0.3567  decode.d1.loss_dice: 0.3674  decode.d2.loss_cls: 0.9725  decode.d2.loss_mask: 0.3491  decode.d2.loss_dice: 0.3487  decode.d3.loss_cls: 0.9245  decode.d3.loss_mask: 0.3449  decode.d3.loss_dice: 0.3523  decode.d4.loss_cls: 0.7949  decode.d4.loss_mask: 0.3586  decode.d4.loss_dice: 0.3619  decode.d5.loss_cls: 0.8299  decode.d5.loss_mask: 0.3482  decode.d5.loss_dice: 0.3506  decode.d6.loss_cls: 0.8680  decode.d6.loss_mask: 0.3534  decode.d6.loss_dice: 0.3840  decode.d7.loss_cls: 0.8388  decode.d7.loss_mask: 0.4081  decode.d7.loss_dice: 0.3662  decode.d8.loss_cls: 0.8329  decode.d8.loss_mask: 0.3678  decode.d8.loss_dice: 0.3727
08/06 02:47:25 - mmengine - INFO - Iter(train) [  4200/320000]  base_lr: 9.8818e-05 lr: 9.8818e-06  eta: 1 day, 14:24:49  time: 0.4364  data_time: 0.0089  memory: 5260  grad_norm: 107.7371  loss: 12.5486  decode.loss_cls: 0.5676  decode.loss_mask: 0.2652  decode.loss_dice: 0.3077  decode.d0.loss_cls: 1.4513  decode.d0.loss_mask: 0.2799  decode.d0.loss_dice: 0.3504  decode.d1.loss_cls: 0.7086  decode.d1.loss_mask: 0.2752  decode.d1.loss_dice: 0.3285  decode.d2.loss_cls: 0.6212  decode.d2.loss_mask: 0.2636  decode.d2.loss_dice: 0.3209  decode.d3.loss_cls: 0.5709  decode.d3.loss_mask: 0.2653  decode.d3.loss_dice: 0.3139  decode.d4.loss_cls: 0.6099  decode.d4.loss_mask: 0.2604  decode.d4.loss_dice: 0.3230  decode.d5.loss_cls: 0.5731  decode.d5.loss_mask: 0.2622  decode.d5.loss_dice: 0.3189  decode.d6.loss_cls: 0.5467  decode.d6.loss_mask: 0.2606  decode.d6.loss_dice: 0.3037  decode.d7.loss_cls: 0.5253  decode.d7.loss_mask: 0.2609  decode.d7.loss_dice: 0.2954  decode.d8.loss_cls: 0.5447  decode.d8.loss_mask: 0.2664  decode.d8.loss_dice: 0.3071
08/06 02:47:47 - mmengine - INFO - Iter(train) [  4250/320000]  base_lr: 9.8804e-05 lr: 9.8804e-06  eta: 1 day, 14:24:22  time: 0.4356  data_time: 0.0090  memory: 5242  grad_norm: 92.9086  loss: 12.2036  decode.loss_cls: 0.5164  decode.loss_mask: 0.2609  decode.loss_dice: 0.3430  decode.d0.loss_cls: 1.2957  decode.d0.loss_mask: 0.2651  decode.d0.loss_dice: 0.4099  decode.d1.loss_cls: 0.5992  decode.d1.loss_mask: 0.2678  decode.d1.loss_dice: 0.3801  decode.d2.loss_cls: 0.4636  decode.d2.loss_mask: 0.2654  decode.d2.loss_dice: 0.3730  decode.d3.loss_cls: 0.4744  decode.d3.loss_mask: 0.2581  decode.d3.loss_dice: 0.3446  decode.d4.loss_cls: 0.5078  decode.d4.loss_mask: 0.2648  decode.d4.loss_dice: 0.3442  decode.d5.loss_cls: 0.4947  decode.d5.loss_mask: 0.2635  decode.d5.loss_dice: 0.3570  decode.d6.loss_cls: 0.5296  decode.d6.loss_mask: 0.2666  decode.d6.loss_dice: 0.3945  decode.d7.loss_cls: 0.5128  decode.d7.loss_mask: 0.2665  decode.d7.loss_dice: 0.3512  decode.d8.loss_cls: 0.5343  decode.d8.loss_mask: 0.2631  decode.d8.loss_dice: 0.3355
08/06 02:48:09 - mmengine - INFO - Iter(train) [  4300/320000]  base_lr: 9.8790e-05 lr: 9.8790e-06  eta: 1 day, 14:23:56  time: 0.4365  data_time: 0.0088  memory: 5260  grad_norm: 99.8522  loss: 14.3844  decode.loss_cls: 0.6917  decode.loss_mask: 0.2653  decode.loss_dice: 0.3236  decode.d0.loss_cls: 1.7397  decode.d0.loss_mask: 0.2853  decode.d0.loss_dice: 0.3758  decode.d1.loss_cls: 0.8556  decode.d1.loss_mask: 0.2881  decode.d1.loss_dice: 0.3533  decode.d2.loss_cls: 0.7954  decode.d2.loss_mask: 0.2698  decode.d2.loss_dice: 0.3352  decode.d3.loss_cls: 0.7530  decode.d3.loss_mask: 0.2547  decode.d3.loss_dice: 0.3197  decode.d4.loss_cls: 0.7954  decode.d4.loss_mask: 0.2546  decode.d4.loss_dice: 0.3258  decode.d5.loss_cls: 0.7459  decode.d5.loss_mask: 0.2464  decode.d5.loss_dice: 0.3172  decode.d6.loss_cls: 0.7096  decode.d6.loss_mask: 0.2480  decode.d6.loss_dice: 0.3216  decode.d7.loss_cls: 0.6846  decode.d7.loss_mask: 0.2590  decode.d7.loss_dice: 0.3120  decode.d8.loss_cls: 0.6587  decode.d8.loss_mask: 0.2637  decode.d8.loss_dice: 0.3357
08/06 02:48:30 - mmengine - INFO - Iter(train) [  4350/320000]  base_lr: 9.8776e-05 lr: 9.8776e-06  eta: 1 day, 14:23:29  time: 0.4355  data_time: 0.0090  memory: 5260  grad_norm: 126.5175  loss: 16.8542  decode.loss_cls: 0.8632  decode.loss_mask: 0.3435  decode.loss_dice: 0.3722  decode.d0.loss_cls: 1.6727  decode.d0.loss_mask: 0.3485  decode.d0.loss_dice: 0.4225  decode.d1.loss_cls: 1.0247  decode.d1.loss_mask: 0.3498  decode.d1.loss_dice: 0.3950  decode.d2.loss_cls: 0.8675  decode.d2.loss_mask: 0.3448  decode.d2.loss_dice: 0.3914  decode.d3.loss_cls: 0.8347  decode.d3.loss_mask: 0.3456  decode.d3.loss_dice: 0.3666  decode.d4.loss_cls: 0.8948  decode.d4.loss_mask: 0.3422  decode.d4.loss_dice: 0.3673  decode.d5.loss_cls: 0.8108  decode.d5.loss_mask: 0.3508  decode.d5.loss_dice: 0.3953  decode.d6.loss_cls: 0.8578  decode.d6.loss_mask: 0.3429  decode.d6.loss_dice: 0.3923  decode.d7.loss_cls: 0.8507  decode.d7.loss_mask: 0.3422  decode.d7.loss_dice: 0.3789  decode.d8.loss_cls: 0.8639  decode.d8.loss_mask: 0.3341  decode.d8.loss_dice: 0.3874
08/06 02:48:52 - mmengine - INFO - Iter(train) [  4400/320000]  base_lr: 9.8762e-05 lr: 9.8762e-06  eta: 1 day, 14:22:58  time: 0.4351  data_time: 0.0090  memory: 5260  grad_norm: 125.4899  loss: 13.7284  decode.loss_cls: 0.6064  decode.loss_mask: 0.2975  decode.loss_dice: 0.3763  decode.d0.loss_cls: 1.3615  decode.d0.loss_mask: 0.2867  decode.d0.loss_dice: 0.4109  decode.d1.loss_cls: 0.7640  decode.d1.loss_mask: 0.2599  decode.d1.loss_dice: 0.3520  decode.d2.loss_cls: 0.6551  decode.d2.loss_mask: 0.2619  decode.d2.loss_dice: 0.3343  decode.d3.loss_cls: 0.6671  decode.d3.loss_mask: 0.2729  decode.d3.loss_dice: 0.3304  decode.d4.loss_cls: 0.6271  decode.d4.loss_mask: 0.2829  decode.d4.loss_dice: 0.3247  decode.d5.loss_cls: 0.6811  decode.d5.loss_mask: 0.2754  decode.d5.loss_dice: 0.3433  decode.d6.loss_cls: 0.7197  decode.d6.loss_mask: 0.2738  decode.d6.loss_dice: 0.3591  decode.d7.loss_cls: 0.7002  decode.d7.loss_mask: 0.2807  decode.d7.loss_dice: 0.3722  decode.d8.loss_cls: 0.6192  decode.d8.loss_mask: 0.2894  decode.d8.loss_dice: 0.3430
08/06 02:49:14 - mmengine - INFO - Iter(train) [  4450/320000]  base_lr: 9.8748e-05 lr: 9.8748e-06  eta: 1 day, 14:22:29  time: 0.4362  data_time: 0.0089  memory: 5242  grad_norm: 203.2028  loss: 15.9986  decode.loss_cls: 0.7721  decode.loss_mask: 0.2911  decode.loss_dice: 0.4119  decode.d0.loss_cls: 1.5797  decode.d0.loss_mask: 0.3399  decode.d0.loss_dice: 0.5148  decode.d1.loss_cls: 0.9121  decode.d1.loss_mask: 0.3405  decode.d1.loss_dice: 0.4400  decode.d2.loss_cls: 0.8492  decode.d2.loss_mask: 0.2978  decode.d2.loss_dice: 0.4353  decode.d3.loss_cls: 0.6764  decode.d3.loss_mask: 0.3043  decode.d3.loss_dice: 0.4347  decode.d4.loss_cls: 0.6400  decode.d4.loss_mask: 0.3525  decode.d4.loss_dice: 0.4500  decode.d5.loss_cls: 0.8134  decode.d5.loss_mask: 0.2951  decode.d5.loss_dice: 0.4348  decode.d6.loss_cls: 0.7299  decode.d6.loss_mask: 0.3517  decode.d6.loss_dice: 0.4124  decode.d7.loss_cls: 0.7179  decode.d7.loss_mask: 0.3193  decode.d7.loss_dice: 0.4485  decode.d8.loss_cls: 0.7169  decode.d8.loss_mask: 0.3054  decode.d8.loss_dice: 0.4111
08/06 02:49:36 - mmengine - INFO - Iter(train) [  4500/320000]  base_lr: 9.8734e-05 lr: 9.8734e-06  eta: 1 day, 14:22:11  time: 0.4534  data_time: 0.0088  memory: 5240  grad_norm: 221.9317  loss: 15.2560  decode.loss_cls: 0.5756  decode.loss_mask: 0.4459  decode.loss_dice: 0.4471  decode.d0.loss_cls: 1.4164  decode.d0.loss_mask: 0.3827  decode.d0.loss_dice: 0.4516  decode.d1.loss_cls: 0.7939  decode.d1.loss_mask: 0.3677  decode.d1.loss_dice: 0.4135  decode.d2.loss_cls: 0.5467  decode.d2.loss_mask: 0.3654  decode.d2.loss_dice: 0.4399  decode.d3.loss_cls: 0.5349  decode.d3.loss_mask: 0.3427  decode.d3.loss_dice: 0.4263  decode.d4.loss_cls: 0.5375  decode.d4.loss_mask: 0.4086  decode.d4.loss_dice: 0.4555  decode.d5.loss_cls: 0.5836  decode.d5.loss_mask: 0.4215  decode.d5.loss_dice: 0.4180  decode.d6.loss_cls: 0.6153  decode.d6.loss_mask: 0.4219  decode.d6.loss_dice: 0.4304  decode.d7.loss_cls: 0.6177  decode.d7.loss_mask: 0.4610  decode.d7.loss_dice: 0.4587  decode.d8.loss_cls: 0.5717  decode.d8.loss_mask: 0.4618  decode.d8.loss_dice: 0.4425
08/06 02:49:58 - mmengine - INFO - Iter(train) [  4550/320000]  base_lr: 9.8720e-05 lr: 9.8720e-06  eta: 1 day, 14:21:40  time: 0.4352  data_time: 0.0089  memory: 5260  grad_norm: 127.6341  loss: 16.1191  decode.loss_cls: 0.8001  decode.loss_mask: 0.3827  decode.loss_dice: 0.4076  decode.d0.loss_cls: 1.6769  decode.d0.loss_mask: 0.3865  decode.d0.loss_dice: 0.4336  decode.d1.loss_cls: 1.0139  decode.d1.loss_mask: 0.3291  decode.d1.loss_dice: 0.3703  decode.d2.loss_cls: 0.7646  decode.d2.loss_mask: 0.3353  decode.d2.loss_dice: 0.3734  decode.d3.loss_cls: 0.7352  decode.d3.loss_mask: 0.3195  decode.d3.loss_dice: 0.3689  decode.d4.loss_cls: 0.7452  decode.d4.loss_mask: 0.3403  decode.d4.loss_dice: 0.3941  decode.d5.loss_cls: 0.7136  decode.d5.loss_mask: 0.3217  decode.d5.loss_dice: 0.3628  decode.d6.loss_cls: 0.7236  decode.d6.loss_mask: 0.3547  decode.d6.loss_dice: 0.4026  decode.d7.loss_cls: 0.7847  decode.d7.loss_mask: 0.3549  decode.d7.loss_dice: 0.3947  decode.d8.loss_cls: 0.7185  decode.d8.loss_mask: 0.3936  decode.d8.loss_dice: 0.4164
08/06 02:50:19 - mmengine - INFO - Iter(train) [  4600/320000]  base_lr: 9.8706e-05 lr: 9.8706e-06  eta: 1 day, 14:21:09  time: 0.4352  data_time: 0.0087  memory: 5224  grad_norm: 133.4786  loss: 12.0451  decode.loss_cls: 0.5262  decode.loss_mask: 0.2533  decode.loss_dice: 0.3114  decode.d0.loss_cls: 1.3486  decode.d0.loss_mask: 0.2696  decode.d0.loss_dice: 0.3693  decode.d1.loss_cls: 0.5677  decode.d1.loss_mask: 0.2793  decode.d1.loss_dice: 0.3329  decode.d2.loss_cls: 0.5830  decode.d2.loss_mask: 0.2633  decode.d2.loss_dice: 0.3233  decode.d3.loss_cls: 0.4771  decode.d3.loss_mask: 0.2689  decode.d3.loss_dice: 0.3549  decode.d4.loss_cls: 0.5006  decode.d4.loss_mask: 0.2606  decode.d4.loss_dice: 0.3327  decode.d5.loss_cls: 0.5519  decode.d5.loss_mask: 0.2551  decode.d5.loss_dice: 0.3170  decode.d6.loss_cls: 0.5051  decode.d6.loss_mask: 0.2607  decode.d6.loss_dice: 0.3317  decode.d7.loss_cls: 0.5301  decode.d7.loss_mask: 0.2592  decode.d7.loss_dice: 0.3322  decode.d8.loss_cls: 0.5284  decode.d8.loss_mask: 0.2507  decode.d8.loss_dice: 0.3002
08/06 02:50:41 - mmengine - INFO - Iter(train) [  4650/320000]  base_lr: 9.8692e-05 lr: 9.8692e-06  eta: 1 day, 14:20:37  time: 0.4355  data_time: 0.0089  memory: 5260  grad_norm: 174.1447  loss: 14.9074  decode.loss_cls: 0.5423  decode.loss_mask: 0.3814  decode.loss_dice: 0.4359  decode.d0.loss_cls: 1.5512  decode.d0.loss_mask: 0.3776  decode.d0.loss_dice: 0.4485  decode.d1.loss_cls: 0.8638  decode.d1.loss_mask: 0.4059  decode.d1.loss_dice: 0.4123  decode.d2.loss_cls: 0.6975  decode.d2.loss_mask: 0.3683  decode.d2.loss_dice: 0.3901  decode.d3.loss_cls: 0.6047  decode.d3.loss_mask: 0.3662  decode.d3.loss_dice: 0.3869  decode.d4.loss_cls: 0.5945  decode.d4.loss_mask: 0.3699  decode.d4.loss_dice: 0.3934  decode.d5.loss_cls: 0.5276  decode.d5.loss_mask: 0.3763  decode.d5.loss_dice: 0.3876  decode.d6.loss_cls: 0.5357  decode.d6.loss_mask: 0.3743  decode.d6.loss_dice: 0.4179  decode.d7.loss_cls: 0.5440  decode.d7.loss_mask: 0.3914  decode.d7.loss_dice: 0.4242  decode.d8.loss_cls: 0.5601  decode.d8.loss_mask: 0.3706  decode.d8.loss_dice: 0.4074
08/06 02:51:03 - mmengine - INFO - Iter(train) [  4700/320000]  base_lr: 9.8677e-05 lr: 9.8677e-06  eta: 1 day, 14:20:07  time: 0.4349  data_time: 0.0088  memory: 5260  grad_norm: 112.2212  loss: 13.8313  decode.loss_cls: 0.6239  decode.loss_mask: 0.3073  decode.loss_dice: 0.3435  decode.d0.loss_cls: 1.4989  decode.d0.loss_mask: 0.3496  decode.d0.loss_dice: 0.4223  decode.d1.loss_cls: 0.6932  decode.d1.loss_mask: 0.3300  decode.d1.loss_dice: 0.3430  decode.d2.loss_cls: 0.6020  decode.d2.loss_mask: 0.3170  decode.d2.loss_dice: 0.3499  decode.d3.loss_cls: 0.5945  decode.d3.loss_mask: 0.3180  decode.d3.loss_dice: 0.3491  decode.d4.loss_cls: 0.5732  decode.d4.loss_mask: 0.3182  decode.d4.loss_dice: 0.3714  decode.d5.loss_cls: 0.6269  decode.d5.loss_mask: 0.3131  decode.d5.loss_dice: 0.3434  decode.d6.loss_cls: 0.6062  decode.d6.loss_mask: 0.3179  decode.d6.loss_dice: 0.3485  decode.d7.loss_cls: 0.6253  decode.d7.loss_mask: 0.3056  decode.d7.loss_dice: 0.3441  decode.d8.loss_cls: 0.6442  decode.d8.loss_mask: 0.3084  decode.d8.loss_dice: 0.3426
08/06 02:51:25 - mmengine - INFO - Iter(train) [  4750/320000]  base_lr: 9.8663e-05 lr: 9.8663e-06  eta: 1 day, 14:19:36  time: 0.4348  data_time: 0.0089  memory: 5260  grad_norm: 224.3446  loss: 13.9997  decode.loss_cls: 0.6263  decode.loss_mask: 0.3977  decode.loss_dice: 0.3285  decode.d0.loss_cls: 1.3409  decode.d0.loss_mask: 0.3969  decode.d0.loss_dice: 0.4350  decode.d1.loss_cls: 0.6347  decode.d1.loss_mask: 0.3801  decode.d1.loss_dice: 0.3647  decode.d2.loss_cls: 0.4621  decode.d2.loss_mask: 0.4149  decode.d2.loss_dice: 0.4112  decode.d3.loss_cls: 0.4449  decode.d3.loss_mask: 0.3979  decode.d3.loss_dice: 0.3386  decode.d4.loss_cls: 0.4741  decode.d4.loss_mask: 0.4007  decode.d4.loss_dice: 0.3783  decode.d5.loss_cls: 0.5150  decode.d5.loss_mask: 0.3988  decode.d5.loss_dice: 0.3799  decode.d6.loss_cls: 0.5249  decode.d6.loss_mask: 0.4165  decode.d6.loss_dice: 0.3892  decode.d7.loss_cls: 0.5868  decode.d7.loss_mask: 0.3980  decode.d7.loss_dice: 0.3784  decode.d8.loss_cls: 0.6525  decode.d8.loss_mask: 0.3909  decode.d8.loss_dice: 0.3415
08/06 02:51:46 - mmengine - INFO - Iter(train) [  4800/320000]  base_lr: 9.8649e-05 lr: 9.8649e-06  eta: 1 day, 14:19:05  time: 0.4356  data_time: 0.0088  memory: 5242  grad_norm: 111.4808  loss: 13.2045  decode.loss_cls: 0.6298  decode.loss_mask: 0.2515  decode.loss_dice: 0.3476  decode.d0.loss_cls: 1.4692  decode.d0.loss_mask: 0.2699  decode.d0.loss_dice: 0.4103  decode.d1.loss_cls: 0.7311  decode.d1.loss_mask: 0.2599  decode.d1.loss_dice: 0.3315  decode.d2.loss_cls: 0.6541  decode.d2.loss_mask: 0.2561  decode.d2.loss_dice: 0.3412  decode.d3.loss_cls: 0.6486  decode.d3.loss_mask: 0.2536  decode.d3.loss_dice: 0.3402  decode.d4.loss_cls: 0.5879  decode.d4.loss_mask: 0.2508  decode.d4.loss_dice: 0.3249  decode.d5.loss_cls: 0.6224  decode.d5.loss_mask: 0.2470  decode.d5.loss_dice: 0.3295  decode.d6.loss_cls: 0.6132  decode.d6.loss_mask: 0.2456  decode.d6.loss_dice: 0.3320  decode.d7.loss_cls: 0.6178  decode.d7.loss_mask: 0.2436  decode.d7.loss_dice: 0.3256  decode.d8.loss_cls: 0.6100  decode.d8.loss_mask: 0.2752  decode.d8.loss_dice: 0.3842
08/06 02:52:08 - mmengine - INFO - Iter(train) [  4850/320000]  base_lr: 9.8635e-05 lr: 9.8635e-06  eta: 1 day, 14:18:36  time: 0.4350  data_time: 0.0088  memory: 5260  grad_norm: 156.9117  loss: 14.4524  decode.loss_cls: 0.5615  decode.loss_mask: 0.3735  decode.loss_dice: 0.3606  decode.d0.loss_cls: 1.3696  decode.d0.loss_mask: 0.3554  decode.d0.loss_dice: 0.3719  decode.d1.loss_cls: 0.6990  decode.d1.loss_mask: 0.3573  decode.d1.loss_dice: 0.3737  decode.d2.loss_cls: 0.6883  decode.d2.loss_mask: 0.3454  decode.d2.loss_dice: 0.3572  decode.d3.loss_cls: 0.6605  decode.d3.loss_mask: 0.3583  decode.d3.loss_dice: 0.3561  decode.d4.loss_cls: 0.6362  decode.d4.loss_mask: 0.3720  decode.d4.loss_dice: 0.3677  decode.d5.loss_cls: 0.5544  decode.d5.loss_mask: 0.4414  decode.d5.loss_dice: 0.3684  decode.d6.loss_cls: 0.5523  decode.d6.loss_mask: 0.4603  decode.d6.loss_dice: 0.3827  decode.d7.loss_cls: 0.6655  decode.d7.loss_mask: 0.4085  decode.d7.loss_dice: 0.3642  decode.d8.loss_cls: 0.5394  decode.d8.loss_mask: 0.3796  decode.d8.loss_dice: 0.3715
08/06 02:52:30 - mmengine - INFO - Iter(train) [  4900/320000]  base_lr: 9.8621e-05 lr: 9.8621e-06  eta: 1 day, 14:18:07  time: 0.4354  data_time: 0.0090  memory: 5223  grad_norm: 136.9835  loss: 13.1671  decode.loss_cls: 0.5528  decode.loss_mask: 0.3161  decode.loss_dice: 0.3874  decode.d0.loss_cls: 1.2848  decode.d0.loss_mask: 0.3545  decode.d0.loss_dice: 0.4649  decode.d1.loss_cls: 0.5560  decode.d1.loss_mask: 0.3256  decode.d1.loss_dice: 0.3920  decode.d2.loss_cls: 0.5091  decode.d2.loss_mask: 0.3190  decode.d2.loss_dice: 0.3756  decode.d3.loss_cls: 0.5425  decode.d3.loss_mask: 0.3171  decode.d3.loss_dice: 0.3811  decode.d4.loss_cls: 0.5656  decode.d4.loss_mask: 0.3269  decode.d4.loss_dice: 0.3952  decode.d5.loss_cls: 0.5115  decode.d5.loss_mask: 0.3144  decode.d5.loss_dice: 0.3714  decode.d6.loss_cls: 0.5366  decode.d6.loss_mask: 0.3173  decode.d6.loss_dice: 0.3735  decode.d7.loss_cls: 0.5055  decode.d7.loss_mask: 0.3219  decode.d7.loss_dice: 0.3577  decode.d8.loss_cls: 0.5150  decode.d8.loss_mask: 0.3124  decode.d8.loss_dice: 0.3635
08/06 02:52:52 - mmengine - INFO - Iter(train) [  4950/320000]  base_lr: 9.8607e-05 lr: 9.8607e-06  eta: 1 day, 14:17:38  time: 0.4353  data_time: 0.0090  memory: 5240  grad_norm: 165.5281  loss: 14.5643  decode.loss_cls: 0.6842  decode.loss_mask: 0.3278  decode.loss_dice: 0.4044  decode.d0.loss_cls: 1.3094  decode.d0.loss_mask: 0.2960  decode.d0.loss_dice: 0.4272  decode.d1.loss_cls: 0.7834  decode.d1.loss_mask: 0.2923  decode.d1.loss_dice: 0.3658  decode.d2.loss_cls: 0.7285  decode.d2.loss_mask: 0.3048  decode.d2.loss_dice: 0.3871  decode.d3.loss_cls: 0.6234  decode.d3.loss_mask: 0.3006  decode.d3.loss_dice: 0.3741  decode.d4.loss_cls: 0.7173  decode.d4.loss_mask: 0.2951  decode.d4.loss_dice: 0.3871  decode.d5.loss_cls: 0.6992  decode.d5.loss_mask: 0.3015  decode.d5.loss_dice: 0.3730  decode.d6.loss_cls: 0.6705  decode.d6.loss_mask: 0.3236  decode.d6.loss_dice: 0.4026  decode.d7.loss_cls: 0.6623  decode.d7.loss_mask: 0.3333  decode.d7.loss_dice: 0.3818  decode.d8.loss_cls: 0.6913  decode.d8.loss_mask: 0.3205  decode.d8.loss_dice: 0.3962
08/06 02:53:14 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 02:53:14 - mmengine - INFO - Iter(train) [  5000/320000]  base_lr: 9.8593e-05 lr: 9.8593e-06  eta: 1 day, 14:17:10  time: 0.4360  data_time: 0.0089  memory: 5240  grad_norm: 133.0231  loss: 13.5409  decode.loss_cls: 0.6386  decode.loss_mask: 0.3106  decode.loss_dice: 0.3228  decode.d0.loss_cls: 1.4572  decode.d0.loss_mask: 0.3221  decode.d0.loss_dice: 0.3988  decode.d1.loss_cls: 0.7035  decode.d1.loss_mask: 0.3181  decode.d1.loss_dice: 0.3605  decode.d2.loss_cls: 0.6150  decode.d2.loss_mask: 0.3122  decode.d2.loss_dice: 0.3434  decode.d3.loss_cls: 0.5625  decode.d3.loss_mask: 0.3128  decode.d3.loss_dice: 0.3476  decode.d4.loss_cls: 0.5808  decode.d4.loss_mask: 0.3098  decode.d4.loss_dice: 0.3327  decode.d5.loss_cls: 0.5670  decode.d5.loss_mask: 0.3158  decode.d5.loss_dice: 0.3347  decode.d6.loss_cls: 0.5877  decode.d6.loss_mask: 0.3159  decode.d6.loss_dice: 0.3228  decode.d7.loss_cls: 0.6907  decode.d7.loss_mask: 0.2970  decode.d7.loss_dice: 0.3105  decode.d8.loss_cls: 0.6140  decode.d8.loss_mask: 0.3013  decode.d8.loss_dice: 0.3342
08/06 02:53:35 - mmengine - INFO - Iter(train) [  5050/320000]  base_lr: 9.8579e-05 lr: 9.8579e-06  eta: 1 day, 14:16:42  time: 0.4351  data_time: 0.0090  memory: 5260  grad_norm: 117.6668  loss: 12.6096  decode.loss_cls: 0.4709  decode.loss_mask: 0.3221  decode.loss_dice: 0.3357  decode.d0.loss_cls: 1.4541  decode.d0.loss_mask: 0.3482  decode.d0.loss_dice: 0.3912  decode.d1.loss_cls: 0.6140  decode.d1.loss_mask: 0.3382  decode.d1.loss_dice: 0.3437  decode.d2.loss_cls: 0.4879  decode.d2.loss_mask: 0.3128  decode.d2.loss_dice: 0.3208  decode.d3.loss_cls: 0.4398  decode.d3.loss_mask: 0.3291  decode.d3.loss_dice: 0.3285  decode.d4.loss_cls: 0.4959  decode.d4.loss_mask: 0.3294  decode.d4.loss_dice: 0.3252  decode.d5.loss_cls: 0.4747  decode.d5.loss_mask: 0.3227  decode.d5.loss_dice: 0.3542  decode.d6.loss_cls: 0.4708  decode.d6.loss_mask: 0.3171  decode.d6.loss_dice: 0.3507  decode.d7.loss_cls: 0.5169  decode.d7.loss_mask: 0.3374  decode.d7.loss_dice: 0.3514  decode.d8.loss_cls: 0.4785  decode.d8.loss_mask: 0.3091  decode.d8.loss_dice: 0.3385
08/06 02:53:57 - mmengine - INFO - Iter(train) [  5100/320000]  base_lr: 9.8565e-05 lr: 9.8565e-06  eta: 1 day, 14:16:14  time: 0.4352  data_time: 0.0087  memory: 5242  grad_norm: 116.6931  loss: 14.8452  decode.loss_cls: 0.8041  decode.loss_mask: 0.2445  decode.loss_dice: 0.3369  decode.d0.loss_cls: 1.5298  decode.d0.loss_mask: 0.2664  decode.d0.loss_dice: 0.4288  decode.d1.loss_cls: 0.9194  decode.d1.loss_mask: 0.2540  decode.d1.loss_dice: 0.3456  decode.d2.loss_cls: 0.7627  decode.d2.loss_mask: 0.2469  decode.d2.loss_dice: 0.3370  decode.d3.loss_cls: 0.8022  decode.d3.loss_mask: 0.2532  decode.d3.loss_dice: 0.3378  decode.d4.loss_cls: 0.7781  decode.d4.loss_mask: 0.2556  decode.d4.loss_dice: 0.3405  decode.d5.loss_cls: 0.8201  decode.d5.loss_mask: 0.2511  decode.d5.loss_dice: 0.3368  decode.d6.loss_cls: 0.8173  decode.d6.loss_mask: 0.2564  decode.d6.loss_dice: 0.3369  decode.d7.loss_cls: 0.7518  decode.d7.loss_mask: 0.2556  decode.d7.loss_dice: 0.3331  decode.d8.loss_cls: 0.8157  decode.d8.loss_mask: 0.2724  decode.d8.loss_dice: 0.3545
08/06 02:54:19 - mmengine - INFO - Iter(train) [  5150/320000]  base_lr: 9.8551e-05 lr: 9.8551e-06  eta: 1 day, 14:15:46  time: 0.4350  data_time: 0.0089  memory: 5275  grad_norm: 110.2187  loss: 10.9346  decode.loss_cls: 0.4561  decode.loss_mask: 0.2434  decode.loss_dice: 0.3010  decode.d0.loss_cls: 1.3695  decode.d0.loss_mask: 0.2419  decode.d0.loss_dice: 0.3649  decode.d1.loss_cls: 0.5344  decode.d1.loss_mask: 0.2260  decode.d1.loss_dice: 0.3053  decode.d2.loss_cls: 0.4441  decode.d2.loss_mask: 0.2181  decode.d2.loss_dice: 0.2903  decode.d3.loss_cls: 0.4806  decode.d3.loss_mask: 0.2124  decode.d3.loss_dice: 0.2850  decode.d4.loss_cls: 0.4889  decode.d4.loss_mask: 0.2149  decode.d4.loss_dice: 0.2872  decode.d5.loss_cls: 0.4682  decode.d5.loss_mask: 0.2202  decode.d5.loss_dice: 0.3069  decode.d6.loss_cls: 0.4585  decode.d6.loss_mask: 0.2211  decode.d6.loss_dice: 0.3057  decode.d7.loss_cls: 0.4958  decode.d7.loss_mask: 0.2202  decode.d7.loss_dice: 0.2932  decode.d8.loss_cls: 0.4484  decode.d8.loss_mask: 0.2348  decode.d8.loss_dice: 0.2974
08/06 02:54:41 - mmengine - INFO - Iter(train) [  5200/320000]  base_lr: 9.8537e-05 lr: 9.8537e-06  eta: 1 day, 14:15:18  time: 0.4357  data_time: 0.0089  memory: 5260  grad_norm: 191.7424  loss: 14.0851  decode.loss_cls: 0.5622  decode.loss_mask: 0.3136  decode.loss_dice: 0.3710  decode.d0.loss_cls: 1.5582  decode.d0.loss_mask: 0.3071  decode.d0.loss_dice: 0.4587  decode.d1.loss_cls: 0.7521  decode.d1.loss_mask: 0.2930  decode.d1.loss_dice: 0.3957  decode.d2.loss_cls: 0.6244  decode.d2.loss_mask: 0.2827  decode.d2.loss_dice: 0.3670  decode.d3.loss_cls: 0.6253  decode.d3.loss_mask: 0.2858  decode.d3.loss_dice: 0.3641  decode.d4.loss_cls: 0.6184  decode.d4.loss_mask: 0.2947  decode.d4.loss_dice: 0.3849  decode.d5.loss_cls: 0.6252  decode.d5.loss_mask: 0.2906  decode.d5.loss_dice: 0.4140  decode.d6.loss_cls: 0.6346  decode.d6.loss_mask: 0.2853  decode.d6.loss_dice: 0.3746  decode.d7.loss_cls: 0.6154  decode.d7.loss_mask: 0.3128  decode.d7.loss_dice: 0.4045  decode.d8.loss_cls: 0.5336  decode.d8.loss_mask: 0.3292  decode.d8.loss_dice: 0.4061
08/06 02:55:02 - mmengine - INFO - Iter(train) [  5250/320000]  base_lr: 9.8522e-05 lr: 9.8522e-06  eta: 1 day, 14:14:50  time: 0.4354  data_time: 0.0089  memory: 5260  grad_norm: 265.4273  loss: 15.8391  decode.loss_cls: 0.6496  decode.loss_mask: 0.3687  decode.loss_dice: 0.4223  decode.d0.loss_cls: 1.6583  decode.d0.loss_mask: 0.3656  decode.d0.loss_dice: 0.4963  decode.d1.loss_cls: 0.9669  decode.d1.loss_mask: 0.3447  decode.d1.loss_dice: 0.4412  decode.d2.loss_cls: 0.7403  decode.d2.loss_mask: 0.3227  decode.d2.loss_dice: 0.3777  decode.d3.loss_cls: 0.6418  decode.d3.loss_mask: 0.3279  decode.d3.loss_dice: 0.4239  decode.d4.loss_cls: 0.7063  decode.d4.loss_mask: 0.3311  decode.d4.loss_dice: 0.4279  decode.d5.loss_cls: 0.6910  decode.d5.loss_mask: 0.3456  decode.d5.loss_dice: 0.4202  decode.d6.loss_cls: 0.6814  decode.d6.loss_mask: 0.3637  decode.d6.loss_dice: 0.4055  decode.d7.loss_cls: 0.7362  decode.d7.loss_mask: 0.3627  decode.d7.loss_dice: 0.3950  decode.d8.loss_cls: 0.6664  decode.d8.loss_mask: 0.3639  decode.d8.loss_dice: 0.3945
08/06 02:55:24 - mmengine - INFO - Iter(train) [  5300/320000]  base_lr: 9.8508e-05 lr: 9.8508e-06  eta: 1 day, 14:14:22  time: 0.4358  data_time: 0.0091  memory: 5223  grad_norm: 200.1634  loss: 14.9908  decode.loss_cls: 0.5396  decode.loss_mask: 0.3979  decode.loss_dice: 0.3904  decode.d0.loss_cls: 1.2860  decode.d0.loss_mask: 0.4613  decode.d0.loss_dice: 0.4586  decode.d1.loss_cls: 0.7715  decode.d1.loss_mask: 0.4389  decode.d1.loss_dice: 0.4113  decode.d2.loss_cls: 0.4979  decode.d2.loss_mask: 0.4630  decode.d2.loss_dice: 0.4136  decode.d3.loss_cls: 0.5785  decode.d3.loss_mask: 0.4342  decode.d3.loss_dice: 0.3901  decode.d4.loss_cls: 0.5916  decode.d4.loss_mask: 0.4152  decode.d4.loss_dice: 0.3898  decode.d5.loss_cls: 0.6178  decode.d5.loss_mask: 0.4454  decode.d5.loss_dice: 0.3933  decode.d6.loss_cls: 0.5842  decode.d6.loss_mask: 0.4319  decode.d6.loss_dice: 0.4001  decode.d7.loss_cls: 0.5970  decode.d7.loss_mask: 0.4441  decode.d7.loss_dice: 0.3833  decode.d8.loss_cls: 0.5385  decode.d8.loss_mask: 0.4255  decode.d8.loss_dice: 0.4002
08/06 02:55:46 - mmengine - INFO - Iter(train) [  5350/320000]  base_lr: 9.8494e-05 lr: 9.8494e-06  eta: 1 day, 14:13:53  time: 0.4342  data_time: 0.0087  memory: 5258  grad_norm: 161.8321  loss: 13.4688  decode.loss_cls: 0.4925  decode.loss_mask: 0.4030  decode.loss_dice: 0.3649  decode.d0.loss_cls: 1.2138  decode.d0.loss_mask: 0.4501  decode.d0.loss_dice: 0.4558  decode.d1.loss_cls: 0.6136  decode.d1.loss_mask: 0.4064  decode.d1.loss_dice: 0.3940  decode.d2.loss_cls: 0.4250  decode.d2.loss_mask: 0.4160  decode.d2.loss_dice: 0.4117  decode.d3.loss_cls: 0.4242  decode.d3.loss_mask: 0.4051  decode.d3.loss_dice: 0.3986  decode.d4.loss_cls: 0.4818  decode.d4.loss_mask: 0.3925  decode.d4.loss_dice: 0.4119  decode.d5.loss_cls: 0.4715  decode.d5.loss_mask: 0.3866  decode.d5.loss_dice: 0.3920  decode.d6.loss_cls: 0.4431  decode.d6.loss_mask: 0.3907  decode.d6.loss_dice: 0.3703  decode.d7.loss_cls: 0.4605  decode.d7.loss_mask: 0.3906  decode.d7.loss_dice: 0.3613  decode.d8.loss_cls: 0.4526  decode.d8.loss_mask: 0.3987  decode.d8.loss_dice: 0.3902
08/06 02:56:08 - mmengine - INFO - Iter(train) [  5400/320000]  base_lr: 9.8480e-05 lr: 9.8480e-06  eta: 1 day, 14:13:27  time: 0.4346  data_time: 0.0090  memory: 5242  grad_norm: 127.5189  loss: 13.5501  decode.loss_cls: 0.5373  decode.loss_mask: 0.3004  decode.loss_dice: 0.3938  decode.d0.loss_cls: 1.3100  decode.d0.loss_mask: 0.3760  decode.d0.loss_dice: 0.5200  decode.d1.loss_cls: 0.6698  decode.d1.loss_mask: 0.3175  decode.d1.loss_dice: 0.3978  decode.d2.loss_cls: 0.6072  decode.d2.loss_mask: 0.3107  decode.d2.loss_dice: 0.3954  decode.d3.loss_cls: 0.5948  decode.d3.loss_mask: 0.3026  decode.d3.loss_dice: 0.3898  decode.d4.loss_cls: 0.4984  decode.d4.loss_mask: 0.3484  decode.d4.loss_dice: 0.4404  decode.d5.loss_cls: 0.4718  decode.d5.loss_mask: 0.3110  decode.d5.loss_dice: 0.4244  decode.d6.loss_cls: 0.5791  decode.d6.loss_mask: 0.3068  decode.d6.loss_dice: 0.3865  decode.d7.loss_cls: 0.5092  decode.d7.loss_mask: 0.2989  decode.d7.loss_dice: 0.3685  decode.d8.loss_cls: 0.5028  decode.d8.loss_mask: 0.2989  decode.d8.loss_dice: 0.3820
08/06 02:56:30 - mmengine - INFO - Iter(train) [  5450/320000]  base_lr: 9.8466e-05 lr: 9.8466e-06  eta: 1 day, 14:13:00  time: 0.4359  data_time: 0.0089  memory: 5260  grad_norm: 132.8558  loss: 12.7669  decode.loss_cls: 0.5028  decode.loss_mask: 0.2810  decode.loss_dice: 0.3479  decode.d0.loss_cls: 1.3854  decode.d0.loss_mask: 0.2870  decode.d0.loss_dice: 0.4050  decode.d1.loss_cls: 0.6283  decode.d1.loss_mask: 0.2830  decode.d1.loss_dice: 0.3615  decode.d2.loss_cls: 0.6019  decode.d2.loss_mask: 0.2753  decode.d2.loss_dice: 0.3382  decode.d3.loss_cls: 0.5302  decode.d3.loss_mask: 0.2757  decode.d3.loss_dice: 0.3298  decode.d4.loss_cls: 0.6027  decode.d4.loss_mask: 0.2794  decode.d4.loss_dice: 0.3268  decode.d5.loss_cls: 0.5821  decode.d5.loss_mask: 0.2948  decode.d5.loss_dice: 0.3573  decode.d6.loss_cls: 0.5518  decode.d6.loss_mask: 0.2787  decode.d6.loss_dice: 0.3352  decode.d7.loss_cls: 0.5690  decode.d7.loss_mask: 0.2868  decode.d7.loss_dice: 0.3336  decode.d8.loss_cls: 0.5351  decode.d8.loss_mask: 0.2736  decode.d8.loss_dice: 0.3271
08/06 02:56:51 - mmengine - INFO - Iter(train) [  5500/320000]  base_lr: 9.8452e-05 lr: 9.8452e-06  eta: 1 day, 14:12:33  time: 0.4350  data_time: 0.0089  memory: 5242  grad_norm: 192.6995  loss: 15.7457  decode.loss_cls: 0.7707  decode.loss_mask: 0.3265  decode.loss_dice: 0.4138  decode.d0.loss_cls: 1.4580  decode.d0.loss_mask: 0.3953  decode.d0.loss_dice: 0.4920  decode.d1.loss_cls: 0.7003  decode.d1.loss_mask: 0.3513  decode.d1.loss_dice: 0.4308  decode.d2.loss_cls: 0.6914  decode.d2.loss_mask: 0.3477  decode.d2.loss_dice: 0.4345  decode.d3.loss_cls: 0.6664  decode.d3.loss_mask: 0.3421  decode.d3.loss_dice: 0.4235  decode.d4.loss_cls: 0.7111  decode.d4.loss_mask: 0.3365  decode.d4.loss_dice: 0.4222  decode.d5.loss_cls: 0.6613  decode.d5.loss_mask: 0.3341  decode.d5.loss_dice: 0.4596  decode.d6.loss_cls: 0.7119  decode.d6.loss_mask: 0.3427  decode.d6.loss_dice: 0.4207  decode.d7.loss_cls: 0.7604  decode.d7.loss_mask: 0.3404  decode.d7.loss_dice: 0.4262  decode.d8.loss_cls: 0.7691  decode.d8.loss_mask: 0.3724  decode.d8.loss_dice: 0.4327
08/06 02:57:13 - mmengine - INFO - Iter(train) [  5550/320000]  base_lr: 9.8438e-05 lr: 9.8438e-06  eta: 1 day, 14:12:07  time: 0.4363  data_time: 0.0089  memory: 5260  grad_norm: 140.1991  loss: 12.5713  decode.loss_cls: 0.4521  decode.loss_mask: 0.3143  decode.loss_dice: 0.3355  decode.d0.loss_cls: 1.4999  decode.d0.loss_mask: 0.3457  decode.d0.loss_dice: 0.3977  decode.d1.loss_cls: 0.5538  decode.d1.loss_mask: 0.3235  decode.d1.loss_dice: 0.3419  decode.d2.loss_cls: 0.5023  decode.d2.loss_mask: 0.3320  decode.d2.loss_dice: 0.3300  decode.d3.loss_cls: 0.5007  decode.d3.loss_mask: 0.3203  decode.d3.loss_dice: 0.3193  decode.d4.loss_cls: 0.4806  decode.d4.loss_mask: 0.3182  decode.d4.loss_dice: 0.3427  decode.d5.loss_cls: 0.4626  decode.d5.loss_mask: 0.3335  decode.d5.loss_dice: 0.3548  decode.d6.loss_cls: 0.4989  decode.d6.loss_mask: 0.3225  decode.d6.loss_dice: 0.3470  decode.d7.loss_cls: 0.4566  decode.d7.loss_mask: 0.3246  decode.d7.loss_dice: 0.3742  decode.d8.loss_cls: 0.4340  decode.d8.loss_mask: 0.3178  decode.d8.loss_dice: 0.3339
08/06 02:57:35 - mmengine - INFO - Iter(train) [  5600/320000]  base_lr: 9.8424e-05 lr: 9.8424e-06  eta: 1 day, 14:11:42  time: 0.4359  data_time: 0.0091  memory: 5240  grad_norm: 107.3748  loss: 12.9380  decode.loss_cls: 0.4642  decode.loss_mask: 0.3159  decode.loss_dice: 0.3508  decode.d0.loss_cls: 1.3371  decode.d0.loss_mask: 0.3519  decode.d0.loss_dice: 0.4280  decode.d1.loss_cls: 0.6403  decode.d1.loss_mask: 0.2856  decode.d1.loss_dice: 0.3590  decode.d2.loss_cls: 0.6197  decode.d2.loss_mask: 0.2764  decode.d2.loss_dice: 0.3406  decode.d3.loss_cls: 0.5613  decode.d3.loss_mask: 0.3030  decode.d3.loss_dice: 0.3456  decode.d4.loss_cls: 0.5328  decode.d4.loss_mask: 0.3004  decode.d4.loss_dice: 0.3465  decode.d5.loss_cls: 0.5034  decode.d5.loss_mask: 0.3044  decode.d5.loss_dice: 0.3554  decode.d6.loss_cls: 0.6018  decode.d6.loss_mask: 0.3104  decode.d6.loss_dice: 0.3566  decode.d7.loss_cls: 0.5319  decode.d7.loss_mask: 0.3126  decode.d7.loss_dice: 0.3542  decode.d8.loss_cls: 0.5008  decode.d8.loss_mask: 0.3173  decode.d8.loss_dice: 0.3302
08/06 02:57:57 - mmengine - INFO - Iter(train) [  5650/320000]  base_lr: 9.8410e-05 lr: 9.8410e-06  eta: 1 day, 14:11:17  time: 0.4359  data_time: 0.0091  memory: 5260  grad_norm: 603.2018  loss: 15.5516  decode.loss_cls: 0.4980  decode.loss_mask: 0.4351  decode.loss_dice: 0.4159  decode.d0.loss_cls: 1.3616  decode.d0.loss_mask: 0.4645  decode.d0.loss_dice: 0.4776  decode.d1.loss_cls: 0.6893  decode.d1.loss_mask: 0.4513  decode.d1.loss_dice: 0.4493  decode.d2.loss_cls: 0.6661  decode.d2.loss_mask: 0.4268  decode.d2.loss_dice: 0.3907  decode.d3.loss_cls: 0.6674  decode.d3.loss_mask: 0.4327  decode.d3.loss_dice: 0.4424  decode.d4.loss_cls: 0.6880  decode.d4.loss_mask: 0.4261  decode.d4.loss_dice: 0.4013  decode.d5.loss_cls: 0.5602  decode.d5.loss_mask: 0.4459  decode.d5.loss_dice: 0.4338  decode.d6.loss_cls: 0.6036  decode.d6.loss_mask: 0.4425  decode.d6.loss_dice: 0.4110  decode.d7.loss_cls: 0.6407  decode.d7.loss_mask: 0.4349  decode.d7.loss_dice: 0.4267  decode.d8.loss_cls: 0.5513  decode.d8.loss_mask: 0.4264  decode.d8.loss_dice: 0.3906
08/06 02:58:19 - mmengine - INFO - Iter(train) [  5700/320000]  base_lr: 9.8396e-05 lr: 9.8396e-06  eta: 1 day, 14:10:50  time: 0.4356  data_time: 0.0089  memory: 5260  grad_norm: 177.3217  loss: 13.8988  decode.loss_cls: 0.6185  decode.loss_mask: 0.2771  decode.loss_dice: 0.3704  decode.d0.loss_cls: 1.4406  decode.d0.loss_mask: 0.2931  decode.d0.loss_dice: 0.4629  decode.d1.loss_cls: 0.7060  decode.d1.loss_mask: 0.2905  decode.d1.loss_dice: 0.4105  decode.d2.loss_cls: 0.6469  decode.d2.loss_mask: 0.2756  decode.d2.loss_dice: 0.3899  decode.d3.loss_cls: 0.6501  decode.d3.loss_mask: 0.2745  decode.d3.loss_dice: 0.3864  decode.d4.loss_cls: 0.6804  decode.d4.loss_mask: 0.2854  decode.d4.loss_dice: 0.3976  decode.d5.loss_cls: 0.6131  decode.d5.loss_mask: 0.2747  decode.d5.loss_dice: 0.3819  decode.d6.loss_cls: 0.5412  decode.d6.loss_mask: 0.2725  decode.d6.loss_dice: 0.3866  decode.d7.loss_cls: 0.5715  decode.d7.loss_mask: 0.2814  decode.d7.loss_dice: 0.3849  decode.d8.loss_cls: 0.6590  decode.d8.loss_mask: 0.2952  decode.d8.loss_dice: 0.3803
08/06 02:58:40 - mmengine - INFO - Iter(train) [  5750/320000]  base_lr: 9.8382e-05 lr: 9.8382e-06  eta: 1 day, 14:10:23  time: 0.4346  data_time: 0.0091  memory: 5242  grad_norm: 113.2445  loss: 11.0693  decode.loss_cls: 0.4137  decode.loss_mask: 0.3171  decode.loss_dice: 0.2939  decode.d0.loss_cls: 1.0603  decode.d0.loss_mask: 0.3181  decode.d0.loss_dice: 0.3072  decode.d1.loss_cls: 0.4953  decode.d1.loss_mask: 0.3159  decode.d1.loss_dice: 0.2978  decode.d2.loss_cls: 0.4926  decode.d2.loss_mask: 0.3092  decode.d2.loss_dice: 0.3035  decode.d3.loss_cls: 0.3912  decode.d3.loss_mask: 0.3280  decode.d3.loss_dice: 0.3088  decode.d4.loss_cls: 0.4370  decode.d4.loss_mask: 0.3098  decode.d4.loss_dice: 0.2894  decode.d5.loss_cls: 0.4080  decode.d5.loss_mask: 0.3120  decode.d5.loss_dice: 0.3004  decode.d6.loss_cls: 0.4171  decode.d6.loss_mask: 0.3148  decode.d6.loss_dice: 0.2885  decode.d7.loss_cls: 0.4261  decode.d7.loss_mask: 0.3158  decode.d7.loss_dice: 0.2953  decode.d8.loss_cls: 0.4081  decode.d8.loss_mask: 0.3106  decode.d8.loss_dice: 0.2837
08/06 02:59:02 - mmengine - INFO - Iter(train) [  5800/320000]  base_lr: 9.8368e-05 lr: 9.8368e-06  eta: 1 day, 14:09:58  time: 0.4360  data_time: 0.0089  memory: 5260  grad_norm: 142.9324  loss: 14.7775  decode.loss_cls: 0.4818  decode.loss_mask: 0.4311  decode.loss_dice: 0.4802  decode.d0.loss_cls: 1.1377  decode.d0.loss_mask: 0.4067  decode.d0.loss_dice: 0.5358  decode.d1.loss_cls: 0.6285  decode.d1.loss_mask: 0.4176  decode.d1.loss_dice: 0.5097  decode.d2.loss_cls: 0.5608  decode.d2.loss_mask: 0.4044  decode.d2.loss_dice: 0.4679  decode.d3.loss_cls: 0.5327  decode.d3.loss_mask: 0.4233  decode.d3.loss_dice: 0.4801  decode.d4.loss_cls: 0.5095  decode.d4.loss_mask: 0.4175  decode.d4.loss_dice: 0.4616  decode.d5.loss_cls: 0.5011  decode.d5.loss_mask: 0.4254  decode.d5.loss_dice: 0.4763  decode.d6.loss_cls: 0.4897  decode.d6.loss_mask: 0.4182  decode.d6.loss_dice: 0.4583  decode.d7.loss_cls: 0.4736  decode.d7.loss_mask: 0.4173  decode.d7.loss_dice: 0.4792  decode.d8.loss_cls: 0.4478  decode.d8.loss_mask: 0.4252  decode.d8.loss_dice: 0.4783
08/06 02:59:24 - mmengine - INFO - Iter(train) [  5850/320000]  base_lr: 9.8353e-05 lr: 9.8353e-06  eta: 1 day, 14:09:31  time: 0.4356  data_time: 0.0088  memory: 5242  grad_norm: 141.3706  loss: 16.3485  decode.loss_cls: 0.7967  decode.loss_mask: 0.3296  decode.loss_dice: 0.4715  decode.d0.loss_cls: 1.4542  decode.d0.loss_mask: 0.3054  decode.d0.loss_dice: 0.4971  decode.d1.loss_cls: 0.8901  decode.d1.loss_mask: 0.3099  decode.d1.loss_dice: 0.4456  decode.d2.loss_cls: 0.8125  decode.d2.loss_mask: 0.3108  decode.d2.loss_dice: 0.4785  decode.d3.loss_cls: 0.7780  decode.d3.loss_mask: 0.2980  decode.d3.loss_dice: 0.4309  decode.d4.loss_cls: 0.7723  decode.d4.loss_mask: 0.3006  decode.d4.loss_dice: 0.4221  decode.d5.loss_cls: 0.8277  decode.d5.loss_mask: 0.2983  decode.d5.loss_dice: 0.4310  decode.d6.loss_cls: 0.8268  decode.d6.loss_mask: 0.2963  decode.d6.loss_dice: 0.4429  decode.d7.loss_cls: 0.8095  decode.d7.loss_mask: 0.3194  decode.d7.loss_dice: 0.4314  decode.d8.loss_cls: 0.7879  decode.d8.loss_mask: 0.3325  decode.d8.loss_dice: 0.4411
08/06 02:59:46 - mmengine - INFO - Iter(train) [  5900/320000]  base_lr: 9.8339e-05 lr: 9.8339e-06  eta: 1 day, 14:09:05  time: 0.4358  data_time: 0.0090  memory: 5260  grad_norm: 118.2699  loss: 12.0476  decode.loss_cls: 0.3689  decode.loss_mask: 0.3060  decode.loss_dice: 0.4012  decode.d0.loss_cls: 1.1758  decode.d0.loss_mask: 0.3417  decode.d0.loss_dice: 0.4848  decode.d1.loss_cls: 0.6118  decode.d1.loss_mask: 0.3097  decode.d1.loss_dice: 0.3975  decode.d2.loss_cls: 0.4674  decode.d2.loss_mask: 0.3183  decode.d2.loss_dice: 0.4048  decode.d3.loss_cls: 0.3832  decode.d3.loss_mask: 0.3070  decode.d3.loss_dice: 0.3751  decode.d4.loss_cls: 0.3840  decode.d4.loss_mask: 0.3063  decode.d4.loss_dice: 0.3975  decode.d5.loss_cls: 0.4192  decode.d5.loss_mask: 0.3082  decode.d5.loss_dice: 0.3693  decode.d6.loss_cls: 0.3840  decode.d6.loss_mask: 0.3034  decode.d6.loss_dice: 0.3713  decode.d7.loss_cls: 0.3358  decode.d7.loss_mask: 0.3066  decode.d7.loss_dice: 0.3989  decode.d8.loss_cls: 0.3951  decode.d8.loss_mask: 0.3088  decode.d8.loss_dice: 0.4060
08/06 03:00:08 - mmengine - INFO - Iter(train) [  5950/320000]  base_lr: 9.8325e-05 lr: 9.8325e-06  eta: 1 day, 14:08:40  time: 0.4367  data_time: 0.0092  memory: 5240  grad_norm: 174.2394  loss: 11.8306  decode.loss_cls: 0.4590  decode.loss_mask: 0.3050  decode.loss_dice: 0.3015  decode.d0.loss_cls: 1.3535  decode.d0.loss_mask: 0.3788  decode.d0.loss_dice: 0.3497  decode.d1.loss_cls: 0.6384  decode.d1.loss_mask: 0.2593  decode.d1.loss_dice: 0.2902  decode.d2.loss_cls: 0.4874  decode.d2.loss_mask: 0.3139  decode.d2.loss_dice: 0.3145  decode.d3.loss_cls: 0.4592  decode.d3.loss_mask: 0.3201  decode.d3.loss_dice: 0.2921  decode.d4.loss_cls: 0.4717  decode.d4.loss_mask: 0.3403  decode.d4.loss_dice: 0.3151  decode.d5.loss_cls: 0.4440  decode.d5.loss_mask: 0.3222  decode.d5.loss_dice: 0.3094  decode.d6.loss_cls: 0.4387  decode.d6.loss_mask: 0.3081  decode.d6.loss_dice: 0.3057  decode.d7.loss_cls: 0.3712  decode.d7.loss_mask: 0.3376  decode.d7.loss_dice: 0.3080  decode.d8.loss_cls: 0.4579  decode.d8.loss_mask: 0.2941  decode.d8.loss_dice: 0.2840
08/06 03:00:29 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 03:00:29 - mmengine - INFO - Iter(train) [  6000/320000]  base_lr: 9.8311e-05 lr: 9.8311e-06  eta: 1 day, 14:08:14  time: 0.4357  data_time: 0.0088  memory: 5260  grad_norm: 115.8187  loss: 11.4527  decode.loss_cls: 0.4486  decode.loss_mask: 0.2652  decode.loss_dice: 0.3340  decode.d0.loss_cls: 1.3039  decode.d0.loss_mask: 0.2531  decode.d0.loss_dice: 0.3330  decode.d1.loss_cls: 0.5867  decode.d1.loss_mask: 0.2538  decode.d1.loss_dice: 0.3229  decode.d2.loss_cls: 0.4764  decode.d2.loss_mask: 0.2556  decode.d2.loss_dice: 0.3143  decode.d3.loss_cls: 0.4896  decode.d3.loss_mask: 0.2505  decode.d3.loss_dice: 0.3124  decode.d4.loss_cls: 0.4175  decode.d4.loss_mask: 0.2618  decode.d4.loss_dice: 0.3275  decode.d5.loss_cls: 0.4246  decode.d5.loss_mask: 0.2595  decode.d5.loss_dice: 0.3287  decode.d6.loss_cls: 0.5012  decode.d6.loss_mask: 0.2672  decode.d6.loss_dice: 0.3156  decode.d7.loss_cls: 0.4726  decode.d7.loss_mask: 0.2710  decode.d7.loss_dice: 0.3260  decode.d8.loss_cls: 0.4984  decode.d8.loss_mask: 0.2585  decode.d8.loss_dice: 0.3224
08/06 03:00:51 - mmengine - INFO - Iter(train) [  6050/320000]  base_lr: 9.8297e-05 lr: 9.8297e-06  eta: 1 day, 14:07:49  time: 0.4357  data_time: 0.0091  memory: 5240  grad_norm: 181.2164  loss: 12.5484  decode.loss_cls: 0.4202  decode.loss_mask: 0.3591  decode.loss_dice: 0.3598  decode.d0.loss_cls: 1.2343  decode.d0.loss_mask: 0.3249  decode.d0.loss_dice: 0.3577  decode.d1.loss_cls: 0.7114  decode.d1.loss_mask: 0.2734  decode.d1.loss_dice: 0.3349  decode.d2.loss_cls: 0.5346  decode.d2.loss_mask: 0.3186  decode.d2.loss_dice: 0.3142  decode.d3.loss_cls: 0.4949  decode.d3.loss_mask: 0.2982  decode.d3.loss_dice: 0.3167  decode.d4.loss_cls: 0.4975  decode.d4.loss_mask: 0.3216  decode.d4.loss_dice: 0.3261  decode.d5.loss_cls: 0.5043  decode.d5.loss_mask: 0.3345  decode.d5.loss_dice: 0.3393  decode.d6.loss_cls: 0.4923  decode.d6.loss_mask: 0.3605  decode.d6.loss_dice: 0.3659  decode.d7.loss_cls: 0.4162  decode.d7.loss_mask: 0.3511  decode.d7.loss_dice: 0.3663  decode.d8.loss_cls: 0.4703  decode.d8.loss_mask: 0.3790  decode.d8.loss_dice: 0.3706
08/06 03:01:13 - mmengine - INFO - Iter(train) [  6100/320000]  base_lr: 9.8283e-05 lr: 9.8283e-06  eta: 1 day, 14:07:23  time: 0.4354  data_time: 0.0089  memory: 5224  grad_norm: 78.3209  loss: 13.3582  decode.loss_cls: 0.5244  decode.loss_mask: 0.3121  decode.loss_dice: 0.3993  decode.d0.loss_cls: 1.2590  decode.d0.loss_mask: 0.3371  decode.d0.loss_dice: 0.4629  decode.d1.loss_cls: 0.6387  decode.d1.loss_mask: 0.3222  decode.d1.loss_dice: 0.4194  decode.d2.loss_cls: 0.4814  decode.d2.loss_mask: 0.3120  decode.d2.loss_dice: 0.4093  decode.d3.loss_cls: 0.4587  decode.d3.loss_mask: 0.3170  decode.d3.loss_dice: 0.4092  decode.d4.loss_cls: 0.4783  decode.d4.loss_mask: 0.3183  decode.d4.loss_dice: 0.4156  decode.d5.loss_cls: 0.5305  decode.d5.loss_mask: 0.3117  decode.d5.loss_dice: 0.4239  decode.d6.loss_cls: 0.5713  decode.d6.loss_mask: 0.3132  decode.d6.loss_dice: 0.4170  decode.d7.loss_cls: 0.5373  decode.d7.loss_mask: 0.3151  decode.d7.loss_dice: 0.4288  decode.d8.loss_cls: 0.4931  decode.d8.loss_mask: 0.3179  decode.d8.loss_dice: 0.4234
08/06 03:01:35 - mmengine - INFO - Iter(train) [  6150/320000]  base_lr: 9.8269e-05 lr: 9.8269e-06  eta: 1 day, 14:07:06  time: 0.4362  data_time: 0.0091  memory: 5224  grad_norm: 187.3270  loss: 14.4855  decode.loss_cls: 0.6747  decode.loss_mask: 0.3505  decode.loss_dice: 0.3562  decode.d0.loss_cls: 1.5344  decode.d0.loss_mask: 0.3631  decode.d0.loss_dice: 0.4171  decode.d1.loss_cls: 0.6383  decode.d1.loss_mask: 0.3550  decode.d1.loss_dice: 0.3636  decode.d2.loss_cls: 0.6750  decode.d2.loss_mask: 0.3486  decode.d2.loss_dice: 0.3562  decode.d3.loss_cls: 0.6608  decode.d3.loss_mask: 0.3507  decode.d3.loss_dice: 0.3412  decode.d4.loss_cls: 0.6292  decode.d4.loss_mask: 0.3589  decode.d4.loss_dice: 0.3619  decode.d5.loss_cls: 0.6416  decode.d5.loss_mask: 0.3568  decode.d5.loss_dice: 0.3669  decode.d6.loss_cls: 0.5960  decode.d6.loss_mask: 0.3400  decode.d6.loss_dice: 0.3514  decode.d7.loss_cls: 0.6152  decode.d7.loss_mask: 0.3486  decode.d7.loss_dice: 0.3678  decode.d8.loss_cls: 0.6705  decode.d8.loss_mask: 0.3448  decode.d8.loss_dice: 0.3506
08/06 03:01:57 - mmengine - INFO - Iter(train) [  6200/320000]  base_lr: 9.8255e-05 lr: 9.8255e-06  eta: 1 day, 14:06:40  time: 0.4354  data_time: 0.0089  memory: 5240  grad_norm: 159.4119  loss: 13.3374  decode.loss_cls: 0.5095  decode.loss_mask: 0.3736  decode.loss_dice: 0.3698  decode.d0.loss_cls: 1.3848  decode.d0.loss_mask: 0.3885  decode.d0.loss_dice: 0.4109  decode.d1.loss_cls: 0.5067  decode.d1.loss_mask: 0.4025  decode.d1.loss_dice: 0.4062  decode.d2.loss_cls: 0.4964  decode.d2.loss_mask: 0.3835  decode.d2.loss_dice: 0.3616  decode.d3.loss_cls: 0.4684  decode.d3.loss_mask: 0.3726  decode.d3.loss_dice: 0.3629  decode.d4.loss_cls: 0.4642  decode.d4.loss_mask: 0.3793  decode.d4.loss_dice: 0.3729  decode.d5.loss_cls: 0.4827  decode.d5.loss_mask: 0.3798  decode.d5.loss_dice: 0.3582  decode.d6.loss_cls: 0.4658  decode.d6.loss_mask: 0.3748  decode.d6.loss_dice: 0.3559  decode.d7.loss_cls: 0.5272  decode.d7.loss_mask: 0.3799  decode.d7.loss_dice: 0.3525  decode.d8.loss_cls: 0.5107  decode.d8.loss_mask: 0.3717  decode.d8.loss_dice: 0.3640
08/06 03:02:18 - mmengine - INFO - Iter(train) [  6250/320000]  base_lr: 9.8241e-05 lr: 9.8241e-06  eta: 1 day, 14:06:15  time: 0.4360  data_time: 0.0090  memory: 5260  grad_norm: 102.9503  loss: 11.1310  decode.loss_cls: 0.4160  decode.loss_mask: 0.2625  decode.loss_dice: 0.2892  decode.d0.loss_cls: 1.3356  decode.d0.loss_mask: 0.2784  decode.d0.loss_dice: 0.3293  decode.d1.loss_cls: 0.5817  decode.d1.loss_mask: 0.2602  decode.d1.loss_dice: 0.3136  decode.d2.loss_cls: 0.4732  decode.d2.loss_mask: 0.2567  decode.d2.loss_dice: 0.3139  decode.d3.loss_cls: 0.4267  decode.d3.loss_mask: 0.2649  decode.d3.loss_dice: 0.3032  decode.d4.loss_cls: 0.4325  decode.d4.loss_mask: 0.2616  decode.d4.loss_dice: 0.3074  decode.d5.loss_cls: 0.3742  decode.d5.loss_mask: 0.2578  decode.d5.loss_dice: 0.3125  decode.d6.loss_cls: 0.4282  decode.d6.loss_mask: 0.2581  decode.d6.loss_dice: 0.2933  decode.d7.loss_cls: 0.4955  decode.d7.loss_mask: 0.2675  decode.d7.loss_dice: 0.3005  decode.d8.loss_cls: 0.4714  decode.d8.loss_mask: 0.2624  decode.d8.loss_dice: 0.3028
08/06 03:02:40 - mmengine - INFO - Iter(train) [  6300/320000]  base_lr: 9.8227e-05 lr: 9.8227e-06  eta: 1 day, 14:05:49  time: 0.4355  data_time: 0.0090  memory: 5242  grad_norm: 112.6848  loss: 12.0778  decode.loss_cls: 0.4113  decode.loss_mask: 0.3120  decode.loss_dice: 0.3820  decode.d0.loss_cls: 1.2792  decode.d0.loss_mask: 0.3259  decode.d0.loss_dice: 0.4612  decode.d1.loss_cls: 0.5372  decode.d1.loss_mask: 0.3242  decode.d1.loss_dice: 0.4145  decode.d2.loss_cls: 0.4119  decode.d2.loss_mask: 0.3205  decode.d2.loss_dice: 0.4223  decode.d3.loss_cls: 0.3851  decode.d3.loss_mask: 0.3092  decode.d3.loss_dice: 0.4093  decode.d4.loss_cls: 0.3383  decode.d4.loss_mask: 0.3136  decode.d4.loss_dice: 0.4094  decode.d5.loss_cls: 0.3369  decode.d5.loss_mask: 0.3110  decode.d5.loss_dice: 0.4194  decode.d6.loss_cls: 0.3711  decode.d6.loss_mask: 0.3139  decode.d6.loss_dice: 0.4130  decode.d7.loss_cls: 0.3579  decode.d7.loss_mask: 0.3145  decode.d7.loss_dice: 0.4014  decode.d8.loss_cls: 0.3638  decode.d8.loss_mask: 0.3158  decode.d8.loss_dice: 0.3923
08/06 03:03:02 - mmengine - INFO - Iter(train) [  6350/320000]  base_lr: 9.8213e-05 lr: 9.8213e-06  eta: 1 day, 14:05:23  time: 0.4358  data_time: 0.0091  memory: 5240  grad_norm: 166.9793  loss: 11.9190  decode.loss_cls: 0.3805  decode.loss_mask: 0.3215  decode.loss_dice: 0.3278  decode.d0.loss_cls: 1.2456  decode.d0.loss_mask: 0.3495  decode.d0.loss_dice: 0.4250  decode.d1.loss_cls: 0.5111  decode.d1.loss_mask: 0.3436  decode.d1.loss_dice: 0.3651  decode.d2.loss_cls: 0.3497  decode.d2.loss_mask: 0.3687  decode.d2.loss_dice: 0.3746  decode.d3.loss_cls: 0.3739  decode.d3.loss_mask: 0.3444  decode.d3.loss_dice: 0.3457  decode.d4.loss_cls: 0.4041  decode.d4.loss_mask: 0.3840  decode.d4.loss_dice: 0.3660  decode.d5.loss_cls: 0.3467  decode.d5.loss_mask: 0.3820  decode.d5.loss_dice: 0.3561  decode.d6.loss_cls: 0.3431  decode.d6.loss_mask: 0.3815  decode.d6.loss_dice: 0.3658  decode.d7.loss_cls: 0.3382  decode.d7.loss_mask: 0.3689  decode.d7.loss_dice: 0.3578  decode.d8.loss_cls: 0.3605  decode.d8.loss_mask: 0.3759  decode.d8.loss_dice: 0.3616
08/06 03:03:24 - mmengine - INFO - Iter(train) [  6400/320000]  base_lr: 9.8198e-05 lr: 9.8198e-06  eta: 1 day, 14:04:57  time: 0.4359  data_time: 0.0090  memory: 5260  grad_norm: 116.8183  loss: 11.9209  decode.loss_cls: 0.4782  decode.loss_mask: 0.2544  decode.loss_dice: 0.3852  decode.d0.loss_cls: 1.0686  decode.d0.loss_mask: 0.2725  decode.d0.loss_dice: 0.4273  decode.d1.loss_cls: 0.5392  decode.d1.loss_mask: 0.2572  decode.d1.loss_dice: 0.3761  decode.d2.loss_cls: 0.4659  decode.d2.loss_mask: 0.2554  decode.d2.loss_dice: 0.3769  decode.d3.loss_cls: 0.4440  decode.d3.loss_mask: 0.2564  decode.d3.loss_dice: 0.3608  decode.d4.loss_cls: 0.4952  decode.d4.loss_mask: 0.2518  decode.d4.loss_dice: 0.3661  decode.d5.loss_cls: 0.4810  decode.d5.loss_mask: 0.2563  decode.d5.loss_dice: 0.4089  decode.d6.loss_cls: 0.4873  decode.d6.loss_mask: 0.2550  decode.d6.loss_dice: 0.3947  decode.d7.loss_cls: 0.5166  decode.d7.loss_mask: 0.2586  decode.d7.loss_dice: 0.4014  decode.d8.loss_cls: 0.4864  decode.d8.loss_mask: 0.2496  decode.d8.loss_dice: 0.3937
08/06 03:03:46 - mmengine - INFO - Iter(train) [  6450/320000]  base_lr: 9.8184e-05 lr: 9.8184e-06  eta: 1 day, 14:04:32  time: 0.4354  data_time: 0.0090  memory: 5260  grad_norm: 124.4764  loss: 12.1150  decode.loss_cls: 0.4138  decode.loss_mask: 0.3372  decode.loss_dice: 0.3409  decode.d0.loss_cls: 1.1111  decode.d0.loss_mask: 0.5252  decode.d0.loss_dice: 0.3828  decode.d1.loss_cls: 0.5610  decode.d1.loss_mask: 0.3784  decode.d1.loss_dice: 0.3494  decode.d2.loss_cls: 0.4348  decode.d2.loss_mask: 0.3446  decode.d2.loss_dice: 0.3543  decode.d3.loss_cls: 0.4359  decode.d3.loss_mask: 0.3407  decode.d3.loss_dice: 0.3596  decode.d4.loss_cls: 0.4254  decode.d4.loss_mask: 0.3386  decode.d4.loss_dice: 0.3495  decode.d5.loss_cls: 0.4142  decode.d5.loss_mask: 0.3442  decode.d5.loss_dice: 0.3703  decode.d6.loss_cls: 0.3878  decode.d6.loss_mask: 0.3434  decode.d6.loss_dice: 0.3464  decode.d7.loss_cls: 0.3759  decode.d7.loss_mask: 0.3413  decode.d7.loss_dice: 0.3581  decode.d8.loss_cls: 0.3544  decode.d8.loss_mask: 0.3514  decode.d8.loss_dice: 0.3447
08/06 03:04:07 - mmengine - INFO - Iter(train) [  6500/320000]  base_lr: 9.8170e-05 lr: 9.8170e-06  eta: 1 day, 14:04:05  time: 0.4358  data_time: 0.0089  memory: 5224  grad_norm: 191.8616  loss: 12.7317  decode.loss_cls: 0.5891  decode.loss_mask: 0.2446  decode.loss_dice: 0.3053  decode.d0.loss_cls: 1.4284  decode.d0.loss_mask: 0.2573  decode.d0.loss_dice: 0.3795  decode.d1.loss_cls: 0.7344  decode.d1.loss_mask: 0.2733  decode.d1.loss_dice: 0.3258  decode.d2.loss_cls: 0.6992  decode.d2.loss_mask: 0.2496  decode.d2.loss_dice: 0.3087  decode.d3.loss_cls: 0.5959  decode.d3.loss_mask: 0.2537  decode.d3.loss_dice: 0.3189  decode.d4.loss_cls: 0.5878  decode.d4.loss_mask: 0.2549  decode.d4.loss_dice: 0.3298  decode.d5.loss_cls: 0.6164  decode.d5.loss_mask: 0.2523  decode.d5.loss_dice: 0.3267  decode.d6.loss_cls: 0.5614  decode.d6.loss_mask: 0.2433  decode.d6.loss_dice: 0.3135  decode.d7.loss_cls: 0.5955  decode.d7.loss_mask: 0.2434  decode.d7.loss_dice: 0.3144  decode.d8.loss_cls: 0.5822  decode.d8.loss_mask: 0.2439  decode.d8.loss_dice: 0.3024
08/06 03:04:29 - mmengine - INFO - Iter(train) [  6550/320000]  base_lr: 9.8156e-05 lr: 9.8156e-06  eta: 1 day, 14:03:41  time: 0.4367  data_time: 0.0089  memory: 5275  grad_norm: 170.3207  loss: 9.6740  decode.loss_cls: 0.3258  decode.loss_mask: 0.2861  decode.loss_dice: 0.3014  decode.d0.loss_cls: 1.1032  decode.d0.loss_mask: 0.2428  decode.d0.loss_dice: 0.3526  decode.d1.loss_cls: 0.5202  decode.d1.loss_mask: 0.2322  decode.d1.loss_dice: 0.2710  decode.d2.loss_cls: 0.3531  decode.d2.loss_mask: 0.2372  decode.d2.loss_dice: 0.2955  decode.d3.loss_cls: 0.3448  decode.d3.loss_mask: 0.2272  decode.d3.loss_dice: 0.2749  decode.d4.loss_cls: 0.3634  decode.d4.loss_mask: 0.2281  decode.d4.loss_dice: 0.2832  decode.d5.loss_cls: 0.3245  decode.d5.loss_mask: 0.2249  decode.d5.loss_dice: 0.2529  decode.d6.loss_cls: 0.3360  decode.d6.loss_mask: 0.2302  decode.d6.loss_dice: 0.2650  decode.d7.loss_cls: 0.3166  decode.d7.loss_mask: 0.2688  decode.d7.loss_dice: 0.2890  decode.d8.loss_cls: 0.3309  decode.d8.loss_mask: 0.3121  decode.d8.loss_dice: 0.2803
08/06 03:04:51 - mmengine - INFO - Iter(train) [  6600/320000]  base_lr: 9.8142e-05 lr: 9.8142e-06  eta: 1 day, 14:03:16  time: 0.4357  data_time: 0.0090  memory: 5242  grad_norm: 135.3424  loss: 11.4932  decode.loss_cls: 0.3471  decode.loss_mask: 0.3021  decode.loss_dice: 0.3592  decode.d0.loss_cls: 1.4083  decode.d0.loss_mask: 0.3218  decode.d0.loss_dice: 0.4100  decode.d1.loss_cls: 0.5944  decode.d1.loss_mask: 0.3113  decode.d1.loss_dice: 0.3513  decode.d2.loss_cls: 0.4304  decode.d2.loss_mask: 0.2998  decode.d2.loss_dice: 0.3359  decode.d3.loss_cls: 0.4189  decode.d3.loss_mask: 0.2935  decode.d3.loss_dice: 0.3250  decode.d4.loss_cls: 0.3865  decode.d4.loss_mask: 0.2961  decode.d4.loss_dice: 0.3372  decode.d5.loss_cls: 0.3225  decode.d5.loss_mask: 0.2948  decode.d5.loss_dice: 0.3337  decode.d6.loss_cls: 0.3589  decode.d6.loss_mask: 0.3091  decode.d6.loss_dice: 0.3514  decode.d7.loss_cls: 0.3098  decode.d7.loss_mask: 0.3071  decode.d7.loss_dice: 0.3724  decode.d8.loss_cls: 0.3402  decode.d8.loss_mask: 0.3057  decode.d8.loss_dice: 0.3587
08/06 03:05:13 - mmengine - INFO - Iter(train) [  6650/320000]  base_lr: 9.8128e-05 lr: 9.8128e-06  eta: 1 day, 14:02:51  time: 0.4349  data_time: 0.0089  memory: 5275  grad_norm: 83.4973  loss: 10.5365  decode.loss_cls: 0.3273  decode.loss_mask: 0.2729  decode.loss_dice: 0.3083  decode.d0.loss_cls: 1.1650  decode.d0.loss_mask: 0.2944  decode.d0.loss_dice: 0.3467  decode.d1.loss_cls: 0.4078  decode.d1.loss_mask: 0.2859  decode.d1.loss_dice: 0.3488  decode.d2.loss_cls: 0.4494  decode.d2.loss_mask: 0.2804  decode.d2.loss_dice: 0.3189  decode.d3.loss_cls: 0.4124  decode.d3.loss_mask: 0.2843  decode.d3.loss_dice: 0.3058  decode.d4.loss_cls: 0.3885  decode.d4.loss_mask: 0.2798  decode.d4.loss_dice: 0.3060  decode.d5.loss_cls: 0.3637  decode.d5.loss_mask: 0.2795  decode.d5.loss_dice: 0.3012  decode.d6.loss_cls: 0.3666  decode.d6.loss_mask: 0.2776  decode.d6.loss_dice: 0.3235  decode.d7.loss_cls: 0.3406  decode.d7.loss_mask: 0.2761  decode.d7.loss_dice: 0.3062  decode.d8.loss_cls: 0.3392  decode.d8.loss_mask: 0.2717  decode.d8.loss_dice: 0.3082
08/06 03:05:34 - mmengine - INFO - Iter(train) [  6700/320000]  base_lr: 9.8114e-05 lr: 9.8114e-06  eta: 1 day, 14:02:26  time: 0.4351  data_time: 0.0089  memory: 5275  grad_norm: 97.5462  loss: 11.7989  decode.loss_cls: 0.2201  decode.loss_mask: 0.3703  decode.loss_dice: 0.4326  decode.d0.loss_cls: 1.1994  decode.d0.loss_mask: 0.3742  decode.d0.loss_dice: 0.4236  decode.d1.loss_cls: 0.4216  decode.d1.loss_mask: 0.3756  decode.d1.loss_dice: 0.4330  decode.d2.loss_cls: 0.3255  decode.d2.loss_mask: 0.3668  decode.d2.loss_dice: 0.4479  decode.d3.loss_cls: 0.2668  decode.d3.loss_mask: 0.3729  decode.d3.loss_dice: 0.4073  decode.d4.loss_cls: 0.2978  decode.d4.loss_mask: 0.3768  decode.d4.loss_dice: 0.4113  decode.d5.loss_cls: 0.3308  decode.d5.loss_mask: 0.3754  decode.d5.loss_dice: 0.4273  decode.d6.loss_cls: 0.2970  decode.d6.loss_mask: 0.3753  decode.d6.loss_dice: 0.3980  decode.d7.loss_cls: 0.2762  decode.d7.loss_mask: 0.3723  decode.d7.loss_dice: 0.4045  decode.d8.loss_cls: 0.2319  decode.d8.loss_mask: 0.3729  decode.d8.loss_dice: 0.4141
08/06 03:05:56 - mmengine - INFO - Iter(train) [  6750/320000]  base_lr: 9.8100e-05 lr: 9.8100e-06  eta: 1 day, 14:02:00  time: 0.4356  data_time: 0.0089  memory: 5260  grad_norm: 159.1128  loss: 13.8184  decode.loss_cls: 0.5583  decode.loss_mask: 0.3571  decode.loss_dice: 0.4116  decode.d0.loss_cls: 1.2887  decode.d0.loss_mask: 0.3371  decode.d0.loss_dice: 0.4136  decode.d1.loss_cls: 0.6131  decode.d1.loss_mask: 0.3068  decode.d1.loss_dice: 0.4071  decode.d2.loss_cls: 0.6088  decode.d2.loss_mask: 0.3142  decode.d2.loss_dice: 0.4136  decode.d3.loss_cls: 0.6267  decode.d3.loss_mask: 0.3057  decode.d3.loss_dice: 0.3922  decode.d4.loss_cls: 0.5716  decode.d4.loss_mask: 0.3159  decode.d4.loss_dice: 0.3977  decode.d5.loss_cls: 0.4972  decode.d5.loss_mask: 0.3574  decode.d5.loss_dice: 0.4216  decode.d6.loss_cls: 0.5364  decode.d6.loss_mask: 0.3435  decode.d6.loss_dice: 0.4179  decode.d7.loss_cls: 0.5273  decode.d7.loss_mask: 0.3526  decode.d7.loss_dice: 0.4237  decode.d8.loss_cls: 0.5259  decode.d8.loss_mask: 0.3588  decode.d8.loss_dice: 0.4162
08/06 03:06:18 - mmengine - INFO - Iter(train) [  6800/320000]  base_lr: 9.8086e-05 lr: 9.8086e-06  eta: 1 day, 14:01:35  time: 0.4351  data_time: 0.0090  memory: 5258  grad_norm: 96.2366  loss: 12.4277  decode.loss_cls: 0.4150  decode.loss_mask: 0.4396  decode.loss_dice: 0.3127  decode.d0.loss_cls: 1.1823  decode.d0.loss_mask: 0.4674  decode.d0.loss_dice: 0.3643  decode.d1.loss_cls: 0.4015  decode.d1.loss_mask: 0.4474  decode.d1.loss_dice: 0.3140  decode.d2.loss_cls: 0.3746  decode.d2.loss_mask: 0.4500  decode.d2.loss_dice: 0.3183  decode.d3.loss_cls: 0.4044  decode.d3.loss_mask: 0.4340  decode.d3.loss_dice: 0.3174  decode.d4.loss_cls: 0.3863  decode.d4.loss_mask: 0.4357  decode.d4.loss_dice: 0.3295  decode.d5.loss_cls: 0.3849  decode.d5.loss_mask: 0.4345  decode.d5.loss_dice: 0.3250  decode.d6.loss_cls: 0.3835  decode.d6.loss_mask: 0.4270  decode.d6.loss_dice: 0.3327  decode.d7.loss_cls: 0.4185  decode.d7.loss_mask: 0.4245  decode.d7.loss_dice: 0.3129  decode.d8.loss_cls: 0.4272  decode.d8.loss_mask: 0.4435  decode.d8.loss_dice: 0.3188
08/06 03:06:40 - mmengine - INFO - Iter(train) [  6850/320000]  base_lr: 9.8072e-05 lr: 9.8072e-06  eta: 1 day, 14:01:09  time: 0.4350  data_time: 0.0088  memory: 5240  grad_norm: 175.7244  loss: 11.9399  decode.loss_cls: 0.3722  decode.loss_mask: 0.3480  decode.loss_dice: 0.3666  decode.d0.loss_cls: 1.2809  decode.d0.loss_mask: 0.3360  decode.d0.loss_dice: 0.3921  decode.d1.loss_cls: 0.4781  decode.d1.loss_mask: 0.3305  decode.d1.loss_dice: 0.3568  decode.d2.loss_cls: 0.4033  decode.d2.loss_mask: 0.3518  decode.d2.loss_dice: 0.3737  decode.d3.loss_cls: 0.3851  decode.d3.loss_mask: 0.3604  decode.d3.loss_dice: 0.3655  decode.d4.loss_cls: 0.3466  decode.d4.loss_mask: 0.3618  decode.d4.loss_dice: 0.3550  decode.d5.loss_cls: 0.3767  decode.d5.loss_mask: 0.3507  decode.d5.loss_dice: 0.3698  decode.d6.loss_cls: 0.3523  decode.d6.loss_mask: 0.3518  decode.d6.loss_dice: 0.3599  decode.d7.loss_cls: 0.3689  decode.d7.loss_mask: 0.3503  decode.d7.loss_dice: 0.3612  decode.d8.loss_cls: 0.4153  decode.d8.loss_mask: 0.3418  decode.d8.loss_dice: 0.3771
08/06 03:07:02 - mmengine - INFO - Iter(train) [  6900/320000]  base_lr: 9.8058e-05 lr: 9.8058e-06  eta: 1 day, 14:00:45  time: 0.4355  data_time: 0.0089  memory: 5260  grad_norm: 243.2207  loss: 10.8130  decode.loss_cls: 0.2284  decode.loss_mask: 0.3739  decode.loss_dice: 0.3937  decode.d0.loss_cls: 1.2150  decode.d0.loss_mask: 0.3609  decode.d0.loss_dice: 0.4385  decode.d1.loss_cls: 0.4741  decode.d1.loss_mask: 0.3289  decode.d1.loss_dice: 0.3683  decode.d2.loss_cls: 0.2702  decode.d2.loss_mask: 0.3203  decode.d2.loss_dice: 0.3538  decode.d3.loss_cls: 0.2607  decode.d3.loss_mask: 0.3197  decode.d3.loss_dice: 0.3534  decode.d4.loss_cls: 0.2902  decode.d4.loss_mask: 0.3185  decode.d4.loss_dice: 0.3609  decode.d5.loss_cls: 0.2344  decode.d5.loss_mask: 0.3773  decode.d5.loss_dice: 0.3683  decode.d6.loss_cls: 0.2120  decode.d6.loss_mask: 0.3589  decode.d6.loss_dice: 0.3672  decode.d7.loss_cls: 0.2184  decode.d7.loss_mask: 0.3301  decode.d7.loss_dice: 0.3545  decode.d8.loss_cls: 0.2350  decode.d8.loss_mask: 0.3552  decode.d8.loss_dice: 0.3725
08/06 03:07:23 - mmengine - INFO - Iter(train) [  6950/320000]  base_lr: 9.8043e-05 lr: 9.8043e-06  eta: 1 day, 14:00:20  time: 0.4358  data_time: 0.0091  memory: 5240  grad_norm: 131.0678  loss: 14.3855  decode.loss_cls: 0.5796  decode.loss_mask: 0.3783  decode.loss_dice: 0.4691  decode.d0.loss_cls: 1.2931  decode.d0.loss_mask: 0.3852  decode.d0.loss_dice: 0.4700  decode.d1.loss_cls: 0.5695  decode.d1.loss_mask: 0.3748  decode.d1.loss_dice: 0.4361  decode.d2.loss_cls: 0.6119  decode.d2.loss_mask: 0.3681  decode.d2.loss_dice: 0.4420  decode.d3.loss_cls: 0.4874  decode.d3.loss_mask: 0.3610  decode.d3.loss_dice: 0.4342  decode.d4.loss_cls: 0.5293  decode.d4.loss_mask: 0.3723  decode.d4.loss_dice: 0.4329  decode.d5.loss_cls: 0.5814  decode.d5.loss_mask: 0.3547  decode.d5.loss_dice: 0.4227  decode.d6.loss_cls: 0.5359  decode.d6.loss_mask: 0.3513  decode.d6.loss_dice: 0.4233  decode.d7.loss_cls: 0.5337  decode.d7.loss_mask: 0.3516  decode.d7.loss_dice: 0.4491  decode.d8.loss_cls: 0.5654  decode.d8.loss_mask: 0.3602  decode.d8.loss_dice: 0.4616
08/06 03:07:45 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 03:07:45 - mmengine - INFO - Iter(train) [  7000/320000]  base_lr: 9.8029e-05 lr: 9.8029e-06  eta: 1 day, 13:59:57  time: 0.4373  data_time: 0.0091  memory: 5260  grad_norm: 133.5573  loss: 13.2547  decode.loss_cls: 0.4721  decode.loss_mask: 0.3566  decode.loss_dice: 0.3591  decode.d0.loss_cls: 1.4468  decode.d0.loss_mask: 0.3530  decode.d0.loss_dice: 0.3769  decode.d1.loss_cls: 0.7034  decode.d1.loss_mask: 0.3414  decode.d1.loss_dice: 0.3418  decode.d2.loss_cls: 0.5789  decode.d2.loss_mask: 0.3296  decode.d2.loss_dice: 0.3361  decode.d3.loss_cls: 0.5833  decode.d3.loss_mask: 0.3340  decode.d3.loss_dice: 0.3437  decode.d4.loss_cls: 0.5807  decode.d4.loss_mask: 0.3425  decode.d4.loss_dice: 0.3527  decode.d5.loss_cls: 0.5245  decode.d5.loss_mask: 0.3383  decode.d5.loss_dice: 0.3122  decode.d6.loss_cls: 0.5249  decode.d6.loss_mask: 0.3377  decode.d6.loss_dice: 0.3350  decode.d7.loss_cls: 0.4761  decode.d7.loss_mask: 0.3381  decode.d7.loss_dice: 0.3051  decode.d8.loss_cls: 0.5487  decode.d8.loss_mask: 0.3423  decode.d8.loss_dice: 0.3390
08/06 03:08:07 - mmengine - INFO - Iter(train) [  7050/320000]  base_lr: 9.8015e-05 lr: 9.8015e-06  eta: 1 day, 13:59:34  time: 0.4356  data_time: 0.0090  memory: 5260  grad_norm: 67.4923  loss: 9.2000  decode.loss_cls: 0.2614  decode.loss_mask: 0.3003  decode.loss_dice: 0.2900  decode.d0.loss_cls: 1.0360  decode.d0.loss_mask: 0.2682  decode.d0.loss_dice: 0.2971  decode.d1.loss_cls: 0.3981  decode.d1.loss_mask: 0.2716  decode.d1.loss_dice: 0.2737  decode.d2.loss_cls: 0.2698  decode.d2.loss_mask: 0.2710  decode.d2.loss_dice: 0.2768  decode.d3.loss_cls: 0.2853  decode.d3.loss_mask: 0.2754  decode.d3.loss_dice: 0.2739  decode.d4.loss_cls: 0.2778  decode.d4.loss_mask: 0.2721  decode.d4.loss_dice: 0.2686  decode.d5.loss_cls: 0.2687  decode.d5.loss_mask: 0.2690  decode.d5.loss_dice: 0.2610  decode.d6.loss_cls: 0.3044  decode.d6.loss_mask: 0.2717  decode.d6.loss_dice: 0.2864  decode.d7.loss_cls: 0.2909  decode.d7.loss_mask: 0.2690  decode.d7.loss_dice: 0.2638  decode.d8.loss_cls: 0.3133  decode.d8.loss_mask: 0.2758  decode.d8.loss_dice: 0.2588
08/06 03:08:29 - mmengine - INFO - Iter(train) [  7100/320000]  base_lr: 9.8001e-05 lr: 9.8001e-06  eta: 1 day, 13:59:10  time: 0.4373  data_time: 0.0090  memory: 5260  grad_norm: 133.7888  loss: 12.5356  decode.loss_cls: 0.4535  decode.loss_mask: 0.3380  decode.loss_dice: 0.3463  decode.d0.loss_cls: 1.4145  decode.d0.loss_mask: 0.3640  decode.d0.loss_dice: 0.4100  decode.d1.loss_cls: 0.6590  decode.d1.loss_mask: 0.3343  decode.d1.loss_dice: 0.3426  decode.d2.loss_cls: 0.4337  decode.d2.loss_mask: 0.3284  decode.d2.loss_dice: 0.3254  decode.d3.loss_cls: 0.4315  decode.d3.loss_mask: 0.3261  decode.d3.loss_dice: 0.3237  decode.d4.loss_cls: 0.5029  decode.d4.loss_mask: 0.3351  decode.d4.loss_dice: 0.3309  decode.d5.loss_cls: 0.4986  decode.d5.loss_mask: 0.3302  decode.d5.loss_dice: 0.3207  decode.d6.loss_cls: 0.4431  decode.d6.loss_mask: 0.3515  decode.d6.loss_dice: 0.3304  decode.d7.loss_cls: 0.4931  decode.d7.loss_mask: 0.3446  decode.d7.loss_dice: 0.3305  decode.d8.loss_cls: 0.4235  decode.d8.loss_mask: 0.3353  decode.d8.loss_dice: 0.3342
08/06 03:08:51 - mmengine - INFO - Iter(train) [  7150/320000]  base_lr: 9.7987e-05 lr: 9.7987e-06  eta: 1 day, 13:58:46  time: 0.4350  data_time: 0.0088  memory: 5260  grad_norm: 95.6659  loss: 12.4999  decode.loss_cls: 0.5109  decode.loss_mask: 0.2761  decode.loss_dice: 0.3612  decode.d0.loss_cls: 1.1721  decode.d0.loss_mask: 0.3148  decode.d0.loss_dice: 0.4236  decode.d1.loss_cls: 0.5699  decode.d1.loss_mask: 0.2818  decode.d1.loss_dice: 0.4084  decode.d2.loss_cls: 0.5035  decode.d2.loss_mask: 0.2789  decode.d2.loss_dice: 0.3849  decode.d3.loss_cls: 0.4691  decode.d3.loss_mask: 0.2751  decode.d3.loss_dice: 0.3879  decode.d4.loss_cls: 0.4453  decode.d4.loss_mask: 0.2760  decode.d4.loss_dice: 0.3878  decode.d5.loss_cls: 0.4942  decode.d5.loss_mask: 0.2759  decode.d5.loss_dice: 0.3664  decode.d6.loss_cls: 0.5233  decode.d6.loss_mask: 0.2785  decode.d6.loss_dice: 0.3735  decode.d7.loss_cls: 0.5830  decode.d7.loss_mask: 0.2794  decode.d7.loss_dice: 0.3643  decode.d8.loss_cls: 0.5583  decode.d8.loss_mask: 0.2779  decode.d8.loss_dice: 0.3979
08/06 03:09:12 - mmengine - INFO - Iter(train) [  7200/320000]  base_lr: 9.7973e-05 lr: 9.7973e-06  eta: 1 day, 13:58:21  time: 0.4360  data_time: 0.0088  memory: 5236  grad_norm: 171.8438  loss: 11.7189  decode.loss_cls: 0.4110  decode.loss_mask: 0.2733  decode.loss_dice: 0.3389  decode.d0.loss_cls: 1.3484  decode.d0.loss_mask: 0.2817  decode.d0.loss_dice: 0.4143  decode.d1.loss_cls: 0.5464  decode.d1.loss_mask: 0.2486  decode.d1.loss_dice: 0.3224  decode.d2.loss_cls: 0.5452  decode.d2.loss_mask: 0.2392  decode.d2.loss_dice: 0.3059  decode.d3.loss_cls: 0.4686  decode.d3.loss_mask: 0.2893  decode.d3.loss_dice: 0.3266  decode.d4.loss_cls: 0.4248  decode.d4.loss_mask: 0.2904  decode.d4.loss_dice: 0.3466  decode.d5.loss_cls: 0.4053  decode.d5.loss_mask: 0.3030  decode.d5.loss_dice: 0.3409  decode.d6.loss_cls: 0.4120  decode.d6.loss_mask: 0.2755  decode.d6.loss_dice: 0.3307  decode.d7.loss_cls: 0.4626  decode.d7.loss_mask: 0.3153  decode.d7.loss_dice: 0.3569  decode.d8.loss_cls: 0.4082  decode.d8.loss_mask: 0.3078  decode.d8.loss_dice: 0.3787
08/06 03:09:34 - mmengine - INFO - Iter(train) [  7250/320000]  base_lr: 9.7959e-05 lr: 9.7959e-06  eta: 1 day, 13:57:58  time: 0.4363  data_time: 0.0091  memory: 5242  grad_norm: 140.0820  loss: 11.8162  decode.loss_cls: 0.3308  decode.loss_mask: 0.4206  decode.loss_dice: 0.3026  decode.d0.loss_cls: 1.1658  decode.d0.loss_mask: 0.4378  decode.d0.loss_dice: 0.3341  decode.d1.loss_cls: 0.4777  decode.d1.loss_mask: 0.4284  decode.d1.loss_dice: 0.3281  decode.d2.loss_cls: 0.3548  decode.d2.loss_mask: 0.4209  decode.d2.loss_dice: 0.3212  decode.d3.loss_cls: 0.3429  decode.d3.loss_mask: 0.4257  decode.d3.loss_dice: 0.3267  decode.d4.loss_cls: 0.2895  decode.d4.loss_mask: 0.4259  decode.d4.loss_dice: 0.3149  decode.d5.loss_cls: 0.3766  decode.d5.loss_mask: 0.4190  decode.d5.loss_dice: 0.3167  decode.d6.loss_cls: 0.2983  decode.d6.loss_mask: 0.4358  decode.d6.loss_dice: 0.3147  decode.d7.loss_cls: 0.3730  decode.d7.loss_mask: 0.4187  decode.d7.loss_dice: 0.3061  decode.d8.loss_cls: 0.3565  decode.d8.loss_mask: 0.4330  decode.d8.loss_dice: 0.3190
08/06 03:09:56 - mmengine - INFO - Iter(train) [  7300/320000]  base_lr: 9.7945e-05 lr: 9.7945e-06  eta: 1 day, 13:57:35  time: 0.4367  data_time: 0.0087  memory: 5260  grad_norm: 96.7891  loss: 8.9458  decode.loss_cls: 0.2085  decode.loss_mask: 0.2326  decode.loss_dice: 0.3384  decode.d0.loss_cls: 0.9795  decode.d0.loss_mask: 0.2565  decode.d0.loss_dice: 0.3702  decode.d1.loss_cls: 0.3136  decode.d1.loss_mask: 0.2418  decode.d1.loss_dice: 0.3379  decode.d2.loss_cls: 0.2668  decode.d2.loss_mask: 0.2367  decode.d2.loss_dice: 0.3344  decode.d3.loss_cls: 0.2596  decode.d3.loss_mask: 0.2446  decode.d3.loss_dice: 0.3311  decode.d4.loss_cls: 0.2316  decode.d4.loss_mask: 0.2354  decode.d4.loss_dice: 0.3446  decode.d5.loss_cls: 0.2416  decode.d5.loss_mask: 0.2299  decode.d5.loss_dice: 0.3273  decode.d6.loss_cls: 0.2409  decode.d6.loss_mask: 0.2315  decode.d6.loss_dice: 0.3452  decode.d7.loss_cls: 0.2247  decode.d7.loss_mask: 0.2289  decode.d7.loss_dice: 0.3213  decode.d8.loss_cls: 0.2150  decode.d8.loss_mask: 0.2339  decode.d8.loss_dice: 0.3419
08/06 03:10:18 - mmengine - INFO - Iter(train) [  7350/320000]  base_lr: 9.7931e-05 lr: 9.7931e-06  eta: 1 day, 13:57:11  time: 0.4356  data_time: 0.0088  memory: 5260  grad_norm: 119.7933  loss: 11.2108  decode.loss_cls: 0.3755  decode.loss_mask: 0.2711  decode.loss_dice: 0.3541  decode.d0.loss_cls: 1.1938  decode.d0.loss_mask: 0.2844  decode.d0.loss_dice: 0.4285  decode.d1.loss_cls: 0.5316  decode.d1.loss_mask: 0.2813  decode.d1.loss_dice: 0.3737  decode.d2.loss_cls: 0.4197  decode.d2.loss_mask: 0.2724  decode.d2.loss_dice: 0.3764  decode.d3.loss_cls: 0.3708  decode.d3.loss_mask: 0.2771  decode.d3.loss_dice: 0.3582  decode.d4.loss_cls: 0.3772  decode.d4.loss_mask: 0.2747  decode.d4.loss_dice: 0.3373  decode.d5.loss_cls: 0.4134  decode.d5.loss_mask: 0.2789  decode.d5.loss_dice: 0.3506  decode.d6.loss_cls: 0.3609  decode.d6.loss_mask: 0.2768  decode.d6.loss_dice: 0.3565  decode.d7.loss_cls: 0.4068  decode.d7.loss_mask: 0.2716  decode.d7.loss_dice: 0.3561  decode.d8.loss_cls: 0.3630  decode.d8.loss_mask: 0.2694  decode.d8.loss_dice: 0.3488
08/06 03:10:40 - mmengine - INFO - Iter(train) [  7400/320000]  base_lr: 9.7917e-05 lr: 9.7917e-06  eta: 1 day, 13:56:47  time: 0.4359  data_time: 0.0091  memory: 5242  grad_norm: 216.0607  loss: 13.5465  decode.loss_cls: 0.4642  decode.loss_mask: 0.3568  decode.loss_dice: 0.4009  decode.d0.loss_cls: 1.2904  decode.d0.loss_mask: 0.3588  decode.d0.loss_dice: 0.4761  decode.d1.loss_cls: 0.5256  decode.d1.loss_mask: 0.3729  decode.d1.loss_dice: 0.4542  decode.d2.loss_cls: 0.4925  decode.d2.loss_mask: 0.3688  decode.d2.loss_dice: 0.4262  decode.d3.loss_cls: 0.5145  decode.d3.loss_mask: 0.3766  decode.d3.loss_dice: 0.4179  decode.d4.loss_cls: 0.4581  decode.d4.loss_mask: 0.3775  decode.d4.loss_dice: 0.4405  decode.d5.loss_cls: 0.4655  decode.d5.loss_mask: 0.3647  decode.d5.loss_dice: 0.4131  decode.d6.loss_cls: 0.4398  decode.d6.loss_mask: 0.3629  decode.d6.loss_dice: 0.4149  decode.d7.loss_cls: 0.4769  decode.d7.loss_mask: 0.3713  decode.d7.loss_dice: 0.4263  decode.d8.loss_cls: 0.4380  decode.d8.loss_mask: 0.3688  decode.d8.loss_dice: 0.4318
08/06 03:11:02 - mmengine - INFO - Iter(train) [  7450/320000]  base_lr: 9.7903e-05 lr: 9.7903e-06  eta: 1 day, 13:56:25  time: 0.4367  data_time: 0.0090  memory: 5242  grad_norm: 128.5169  loss: 14.4513  decode.loss_cls: 0.6137  decode.loss_mask: 0.3462  decode.loss_dice: 0.3681  decode.d0.loss_cls: 1.4388  decode.d0.loss_mask: 0.3745  decode.d0.loss_dice: 0.4410  decode.d1.loss_cls: 0.6853  decode.d1.loss_mask: 0.3776  decode.d1.loss_dice: 0.3848  decode.d2.loss_cls: 0.6160  decode.d2.loss_mask: 0.3652  decode.d2.loss_dice: 0.3676  decode.d3.loss_cls: 0.6023  decode.d3.loss_mask: 0.3435  decode.d3.loss_dice: 0.3891  decode.d4.loss_cls: 0.6065  decode.d4.loss_mask: 0.3438  decode.d4.loss_dice: 0.3782  decode.d5.loss_cls: 0.6254  decode.d5.loss_mask: 0.3373  decode.d5.loss_dice: 0.3712  decode.d6.loss_cls: 0.5974  decode.d6.loss_mask: 0.3338  decode.d6.loss_dice: 0.3594  decode.d7.loss_cls: 0.6835  decode.d7.loss_mask: 0.3539  decode.d7.loss_dice: 0.3847  decode.d8.loss_cls: 0.6250  decode.d8.loss_mask: 0.3553  decode.d8.loss_dice: 0.3819
08/06 03:11:23 - mmengine - INFO - Iter(train) [  7500/320000]  base_lr: 9.7888e-05 lr: 9.7888e-06  eta: 1 day, 13:56:00  time: 0.4357  data_time: 0.0088  memory: 5240  grad_norm: 116.2994  loss: 13.3966  decode.loss_cls: 0.5089  decode.loss_mask: 0.3001  decode.loss_dice: 0.3628  decode.d0.loss_cls: 1.4666  decode.d0.loss_mask: 0.3151  decode.d0.loss_dice: 0.4166  decode.d1.loss_cls: 0.8461  decode.d1.loss_mask: 0.3179  decode.d1.loss_dice: 0.3551  decode.d2.loss_cls: 0.6192  decode.d2.loss_mask: 0.3081  decode.d2.loss_dice: 0.3660  decode.d3.loss_cls: 0.5748  decode.d3.loss_mask: 0.3019  decode.d3.loss_dice: 0.3759  decode.d4.loss_cls: 0.4760  decode.d4.loss_mask: 0.3031  decode.d4.loss_dice: 0.3487  decode.d5.loss_cls: 0.4894  decode.d5.loss_mask: 0.3217  decode.d5.loss_dice: 0.3675  decode.d6.loss_cls: 0.4861  decode.d6.loss_mask: 0.3029  decode.d6.loss_dice: 0.3696  decode.d7.loss_cls: 0.5867  decode.d7.loss_mask: 0.3030  decode.d7.loss_dice: 0.3325  decode.d8.loss_cls: 0.6125  decode.d8.loss_mask: 0.2995  decode.d8.loss_dice: 0.3625
08/06 03:11:45 - mmengine - INFO - Iter(train) [  7550/320000]  base_lr: 9.7874e-05 lr: 9.7874e-06  eta: 1 day, 13:55:37  time: 0.4366  data_time: 0.0091  memory: 5242  grad_norm: 141.3246  loss: 15.1647  decode.loss_cls: 0.6658  decode.loss_mask: 0.3078  decode.loss_dice: 0.4005  decode.d0.loss_cls: 1.2844  decode.d0.loss_mask: 0.3595  decode.d0.loss_dice: 0.4726  decode.d1.loss_cls: 0.7132  decode.d1.loss_mask: 0.3274  decode.d1.loss_dice: 0.4473  decode.d2.loss_cls: 0.7822  decode.d2.loss_mask: 0.3110  decode.d2.loss_dice: 0.4119  decode.d3.loss_cls: 0.7564  decode.d3.loss_mask: 0.3180  decode.d3.loss_dice: 0.4138  decode.d4.loss_cls: 0.7144  decode.d4.loss_mask: 0.3231  decode.d4.loss_dice: 0.4026  decode.d5.loss_cls: 0.7961  decode.d5.loss_mask: 0.3133  decode.d5.loss_dice: 0.4032  decode.d6.loss_cls: 0.7057  decode.d6.loss_mask: 0.3086  decode.d6.loss_dice: 0.4054  decode.d7.loss_cls: 0.6684  decode.d7.loss_mask: 0.3162  decode.d7.loss_dice: 0.4150  decode.d8.loss_cls: 0.6852  decode.d8.loss_mask: 0.3163  decode.d8.loss_dice: 0.4194
08/06 03:12:07 - mmengine - INFO - Iter(train) [  7600/320000]  base_lr: 9.7860e-05 lr: 9.7860e-06  eta: 1 day, 13:55:13  time: 0.4350  data_time: 0.0089  memory: 5275  grad_norm: 178.1937  loss: 12.4713  decode.loss_cls: 0.3922  decode.loss_mask: 0.3134  decode.loss_dice: 0.3899  decode.d0.loss_cls: 1.2746  decode.d0.loss_mask: 0.3621  decode.d0.loss_dice: 0.4563  decode.d1.loss_cls: 0.5935  decode.d1.loss_mask: 0.3167  decode.d1.loss_dice: 0.4025  decode.d2.loss_cls: 0.4594  decode.d2.loss_mask: 0.3193  decode.d2.loss_dice: 0.3989  decode.d3.loss_cls: 0.3963  decode.d3.loss_mask: 0.3290  decode.d3.loss_dice: 0.4016  decode.d4.loss_cls: 0.4085  decode.d4.loss_mask: 0.3351  decode.d4.loss_dice: 0.4099  decode.d5.loss_cls: 0.4141  decode.d5.loss_mask: 0.3208  decode.d5.loss_dice: 0.3893  decode.d6.loss_cls: 0.4112  decode.d6.loss_mask: 0.3191  decode.d6.loss_dice: 0.3871  decode.d7.loss_cls: 0.4152  decode.d7.loss_mask: 0.3228  decode.d7.loss_dice: 0.4012  decode.d8.loss_cls: 0.3930  decode.d8.loss_mask: 0.3278  decode.d8.loss_dice: 0.4106
08/06 03:12:29 - mmengine - INFO - Iter(train) [  7650/320000]  base_lr: 9.7846e-05 lr: 9.7846e-06  eta: 1 day, 13:54:49  time: 0.4361  data_time: 0.0090  memory: 5260  grad_norm: 88.0053  loss: 10.3849  decode.loss_cls: 0.3564  decode.loss_mask: 0.2523  decode.loss_dice: 0.3200  decode.d0.loss_cls: 1.0809  decode.d0.loss_mask: 0.2674  decode.d0.loss_dice: 0.3524  decode.d1.loss_cls: 0.3834  decode.d1.loss_mask: 0.2585  decode.d1.loss_dice: 0.3022  decode.d2.loss_cls: 0.4334  decode.d2.loss_mask: 0.2498  decode.d2.loss_dice: 0.3254  decode.d3.loss_cls: 0.3906  decode.d3.loss_mask: 0.2510  decode.d3.loss_dice: 0.3164  decode.d4.loss_cls: 0.4208  decode.d4.loss_mask: 0.2558  decode.d4.loss_dice: 0.3154  decode.d5.loss_cls: 0.4029  decode.d5.loss_mask: 0.2525  decode.d5.loss_dice: 0.3282  decode.d6.loss_cls: 0.4099  decode.d6.loss_mask: 0.2490  decode.d6.loss_dice: 0.3055  decode.d7.loss_cls: 0.3956  decode.d7.loss_mask: 0.2516  decode.d7.loss_dice: 0.3191  decode.d8.loss_cls: 0.3727  decode.d8.loss_mask: 0.2522  decode.d8.loss_dice: 0.3134
08/06 03:12:51 - mmengine - INFO - Iter(train) [  7700/320000]  base_lr: 9.7832e-05 lr: 9.7832e-06  eta: 1 day, 13:54:26  time: 0.4356  data_time: 0.0089  memory: 5240  grad_norm: 161.0952  loss: 10.9279  decode.loss_cls: 0.3779  decode.loss_mask: 0.3220  decode.loss_dice: 0.3195  decode.d0.loss_cls: 1.0426  decode.d0.loss_mask: 0.3085  decode.d0.loss_dice: 0.3016  decode.d1.loss_cls: 0.3215  decode.d1.loss_mask: 0.2982  decode.d1.loss_dice: 0.3110  decode.d2.loss_cls: 0.3681  decode.d2.loss_mask: 0.3218  decode.d2.loss_dice: 0.3248  decode.d3.loss_cls: 0.3664  decode.d3.loss_mask: 0.2989  decode.d3.loss_dice: 0.3445  decode.d4.loss_cls: 0.3373  decode.d4.loss_mask: 0.3421  decode.d4.loss_dice: 0.3864  decode.d5.loss_cls: 0.3872  decode.d5.loss_mask: 0.3580  decode.d5.loss_dice: 0.3806  decode.d6.loss_cls: 0.3802  decode.d6.loss_mask: 0.3545  decode.d6.loss_dice: 0.3287  decode.d7.loss_cls: 0.3843  decode.d7.loss_mask: 0.3242  decode.d7.loss_dice: 0.3239  decode.d8.loss_cls: 0.3465  decode.d8.loss_mask: 0.3328  decode.d8.loss_dice: 0.3338
08/06 03:13:12 - mmengine - INFO - Iter(train) [  7750/320000]  base_lr: 9.7818e-05 lr: 9.7818e-06  eta: 1 day, 13:54:01  time: 0.4346  data_time: 0.0090  memory: 5242  grad_norm: 136.9545  loss: 13.3917  decode.loss_cls: 0.5317  decode.loss_mask: 0.3342  decode.loss_dice: 0.4466  decode.d0.loss_cls: 1.1173  decode.d0.loss_mask: 0.3544  decode.d0.loss_dice: 0.4910  decode.d1.loss_cls: 0.5009  decode.d1.loss_mask: 0.3283  decode.d1.loss_dice: 0.4414  decode.d2.loss_cls: 0.3706  decode.d2.loss_mask: 0.3169  decode.d2.loss_dice: 0.4521  decode.d3.loss_cls: 0.4210  decode.d3.loss_mask: 0.3121  decode.d3.loss_dice: 0.4529  decode.d4.loss_cls: 0.4349  decode.d4.loss_mask: 0.3266  decode.d4.loss_dice: 0.4513  decode.d5.loss_cls: 0.4778  decode.d5.loss_mask: 0.3540  decode.d5.loss_dice: 0.4524  decode.d6.loss_cls: 0.5581  decode.d6.loss_mask: 0.3468  decode.d6.loss_dice: 0.4585  decode.d7.loss_cls: 0.5445  decode.d7.loss_mask: 0.3472  decode.d7.loss_dice: 0.4586  decode.d8.loss_cls: 0.5071  decode.d8.loss_mask: 0.3503  decode.d8.loss_dice: 0.4523
08/06 03:13:34 - mmengine - INFO - Iter(train) [  7800/320000]  base_lr: 9.7804e-05 lr: 9.7804e-06  eta: 1 day, 13:53:45  time: 0.4356  data_time: 0.0089  memory: 5240  grad_norm: 129.0363  loss: 12.9461  decode.loss_cls: 0.5940  decode.loss_mask: 0.2641  decode.loss_dice: 0.3570  decode.d0.loss_cls: 1.3475  decode.d0.loss_mask: 0.2915  decode.d0.loss_dice: 0.4173  decode.d1.loss_cls: 0.6420  decode.d1.loss_mask: 0.2601  decode.d1.loss_dice: 0.3807  decode.d2.loss_cls: 0.5859  decode.d2.loss_mask: 0.2637  decode.d2.loss_dice: 0.3610  decode.d3.loss_cls: 0.5654  decode.d3.loss_mask: 0.2612  decode.d3.loss_dice: 0.3662  decode.d4.loss_cls: 0.5545  decode.d4.loss_mask: 0.2638  decode.d4.loss_dice: 0.3563  decode.d5.loss_cls: 0.5685  decode.d5.loss_mask: 0.2697  decode.d5.loss_dice: 0.3674  decode.d6.loss_cls: 0.5556  decode.d6.loss_mask: 0.2651  decode.d6.loss_dice: 0.3626  decode.d7.loss_cls: 0.5639  decode.d7.loss_mask: 0.2694  decode.d7.loss_dice: 0.3871  decode.d8.loss_cls: 0.5400  decode.d8.loss_mask: 0.2750  decode.d8.loss_dice: 0.3895
08/06 03:13:56 - mmengine - INFO - Iter(train) [  7850/320000]  base_lr: 9.7790e-05 lr: 9.7790e-06  eta: 1 day, 13:53:21  time: 0.4359  data_time: 0.0089  memory: 5260  grad_norm: 145.1635  loss: 13.5611  decode.loss_cls: 0.6535  decode.loss_mask: 0.2835  decode.loss_dice: 0.3911  decode.d0.loss_cls: 1.1355  decode.d0.loss_mask: 0.2663  decode.d0.loss_dice: 0.4392  decode.d1.loss_cls: 0.7731  decode.d1.loss_mask: 0.2698  decode.d1.loss_dice: 0.4052  decode.d2.loss_cls: 0.5601  decode.d2.loss_mask: 0.2941  decode.d2.loss_dice: 0.4139  decode.d3.loss_cls: 0.5745  decode.d3.loss_mask: 0.2932  decode.d3.loss_dice: 0.4250  decode.d4.loss_cls: 0.6031  decode.d4.loss_mask: 0.2865  decode.d4.loss_dice: 0.3911  decode.d5.loss_cls: 0.6000  decode.d5.loss_mask: 0.2791  decode.d5.loss_dice: 0.4226  decode.d6.loss_cls: 0.5167  decode.d6.loss_mask: 0.2935  decode.d6.loss_dice: 0.4467  decode.d7.loss_cls: 0.5538  decode.d7.loss_mask: 0.2918  decode.d7.loss_dice: 0.4319  decode.d8.loss_cls: 0.5587  decode.d8.loss_mask: 0.2767  decode.d8.loss_dice: 0.4310
08/06 03:14:18 - mmengine - INFO - Iter(train) [  7900/320000]  base_lr: 9.7776e-05 lr: 9.7776e-06  eta: 1 day, 13:52:56  time: 0.4355  data_time: 0.0091  memory: 5242  grad_norm: 130.4203  loss: 11.5032  decode.loss_cls: 0.3659  decode.loss_mask: 0.3558  decode.loss_dice: 0.3719  decode.d0.loss_cls: 1.1582  decode.d0.loss_mask: 0.3499  decode.d0.loss_dice: 0.3746  decode.d1.loss_cls: 0.4891  decode.d1.loss_mask: 0.3417  decode.d1.loss_dice: 0.3446  decode.d2.loss_cls: 0.3432  decode.d2.loss_mask: 0.3352  decode.d2.loss_dice: 0.3700  decode.d3.loss_cls: 0.3320  decode.d3.loss_mask: 0.3412  decode.d3.loss_dice: 0.3555  decode.d4.loss_cls: 0.3398  decode.d4.loss_mask: 0.3465  decode.d4.loss_dice: 0.3457  decode.d5.loss_cls: 0.3786  decode.d5.loss_mask: 0.3465  decode.d5.loss_dice: 0.3535  decode.d6.loss_cls: 0.3521  decode.d6.loss_mask: 0.3476  decode.d6.loss_dice: 0.3473  decode.d7.loss_cls: 0.3271  decode.d7.loss_mask: 0.3606  decode.d7.loss_dice: 0.3487  decode.d8.loss_cls: 0.3569  decode.d8.loss_mask: 0.3537  decode.d8.loss_dice: 0.3696
08/06 03:14:40 - mmengine - INFO - Iter(train) [  7950/320000]  base_lr: 9.7762e-05 lr: 9.7762e-06  eta: 1 day, 13:52:32  time: 0.4365  data_time: 0.0090  memory: 5224  grad_norm: 144.6861  loss: 13.5914  decode.loss_cls: 0.5644  decode.loss_mask: 0.3185  decode.loss_dice: 0.4070  decode.d0.loss_cls: 1.3277  decode.d0.loss_mask: 0.3188  decode.d0.loss_dice: 0.4456  decode.d1.loss_cls: 0.6108  decode.d1.loss_mask: 0.3212  decode.d1.loss_dice: 0.4245  decode.d2.loss_cls: 0.5837  decode.d2.loss_mask: 0.3284  decode.d2.loss_dice: 0.4368  decode.d3.loss_cls: 0.5364  decode.d3.loss_mask: 0.3191  decode.d3.loss_dice: 0.4189  decode.d4.loss_cls: 0.6191  decode.d4.loss_mask: 0.3111  decode.d4.loss_dice: 0.3905  decode.d5.loss_cls: 0.5760  decode.d5.loss_mask: 0.3104  decode.d5.loss_dice: 0.4278  decode.d6.loss_cls: 0.4395  decode.d6.loss_mask: 0.3132  decode.d6.loss_dice: 0.4165  decode.d7.loss_cls: 0.5048  decode.d7.loss_mask: 0.3110  decode.d7.loss_dice: 0.4032  decode.d8.loss_cls: 0.4687  decode.d8.loss_mask: 0.3169  decode.d8.loss_dice: 0.4212
08/06 03:15:01 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 03:15:01 - mmengine - INFO - Iter(train) [  8000/320000]  base_lr: 9.7747e-05 lr: 9.7747e-06  eta: 1 day, 13:52:08  time: 0.4361  data_time: 0.0091  memory: 5224  grad_norm: 169.7440  loss: 13.6831  decode.loss_cls: 0.6515  decode.loss_mask: 0.3061  decode.loss_dice: 0.3021  decode.d0.loss_cls: 1.2580  decode.d0.loss_mask: 0.2982  decode.d0.loss_dice: 0.3463  decode.d1.loss_cls: 0.7188  decode.d1.loss_mask: 0.2852  decode.d1.loss_dice: 0.3471  decode.d2.loss_cls: 0.6451  decode.d2.loss_mask: 0.2912  decode.d2.loss_dice: 0.3373  decode.d3.loss_cls: 0.6376  decode.d3.loss_mask: 0.3003  decode.d3.loss_dice: 0.3994  decode.d4.loss_cls: 0.6773  decode.d4.loss_mask: 0.2953  decode.d4.loss_dice: 0.3322  decode.d5.loss_cls: 0.7523  decode.d5.loss_mask: 0.2945  decode.d5.loss_dice: 0.3272  decode.d6.loss_cls: 0.6711  decode.d6.loss_mask: 0.3080  decode.d6.loss_dice: 0.3519  decode.d7.loss_cls: 0.6829  decode.d7.loss_mask: 0.2924  decode.d7.loss_dice: 0.3186  decode.d8.loss_cls: 0.6609  decode.d8.loss_mask: 0.2984  decode.d8.loss_dice: 0.2958
08/06 03:15:23 - mmengine - INFO - Iter(train) [  8050/320000]  base_lr: 9.7733e-05 lr: 9.7733e-06  eta: 1 day, 13:51:45  time: 0.4365  data_time: 0.0092  memory: 5260  grad_norm: 191.1310  loss: 10.1379  decode.loss_cls: 0.2961  decode.loss_mask: 0.3062  decode.loss_dice: 0.3639  decode.d0.loss_cls: 1.0137  decode.d0.loss_mask: 0.3115  decode.d0.loss_dice: 0.4337  decode.d1.loss_cls: 0.3540  decode.d1.loss_mask: 0.3092  decode.d1.loss_dice: 0.3702  decode.d2.loss_cls: 0.2516  decode.d2.loss_mask: 0.3103  decode.d2.loss_dice: 0.3572  decode.d3.loss_cls: 0.2454  decode.d3.loss_mask: 0.3050  decode.d3.loss_dice: 0.3473  decode.d4.loss_cls: 0.2551  decode.d4.loss_mask: 0.3076  decode.d4.loss_dice: 0.3593  decode.d5.loss_cls: 0.2414  decode.d5.loss_mask: 0.3054  decode.d5.loss_dice: 0.3496  decode.d6.loss_cls: 0.2417  decode.d6.loss_mask: 0.3106  decode.d6.loss_dice: 0.3544  decode.d7.loss_cls: 0.2369  decode.d7.loss_mask: 0.3067  decode.d7.loss_dice: 0.3575  decode.d8.loss_cls: 0.2714  decode.d8.loss_mask: 0.3007  decode.d8.loss_dice: 0.3643
08/06 03:15:45 - mmengine - INFO - Iter(train) [  8100/320000]  base_lr: 9.7719e-05 lr: 9.7719e-06  eta: 1 day, 13:51:21  time: 0.4357  data_time: 0.0089  memory: 5240  grad_norm: 117.8172  loss: 8.9557  decode.loss_cls: 0.2878  decode.loss_mask: 0.2297  decode.loss_dice: 0.2859  decode.d0.loss_cls: 1.1233  decode.d0.loss_mask: 0.2502  decode.d0.loss_dice: 0.3105  decode.d1.loss_cls: 0.3505  decode.d1.loss_mask: 0.2448  decode.d1.loss_dice: 0.2987  decode.d2.loss_cls: 0.3006  decode.d2.loss_mask: 0.2342  decode.d2.loss_dice: 0.2778  decode.d3.loss_cls: 0.2761  decode.d3.loss_mask: 0.2380  decode.d3.loss_dice: 0.2955  decode.d4.loss_cls: 0.2796  decode.d4.loss_mask: 0.2376  decode.d4.loss_dice: 0.2861  decode.d5.loss_cls: 0.2563  decode.d5.loss_mask: 0.2377  decode.d5.loss_dice: 0.2868  decode.d6.loss_cls: 0.2961  decode.d6.loss_mask: 0.2325  decode.d6.loss_dice: 0.2706  decode.d7.loss_cls: 0.2715  decode.d7.loss_mask: 0.2312  decode.d7.loss_dice: 0.2749  decode.d8.loss_cls: 0.2846  decode.d8.loss_mask: 0.2298  decode.d8.loss_dice: 0.2770
08/06 03:16:07 - mmengine - INFO - Iter(train) [  8150/320000]  base_lr: 9.7705e-05 lr: 9.7705e-06  eta: 1 day, 13:50:57  time: 0.4355  data_time: 0.0090  memory: 5242  grad_norm: 108.5944  loss: 10.1463  decode.loss_cls: 0.3067  decode.loss_mask: 0.3070  decode.loss_dice: 0.3013  decode.d0.loss_cls: 1.1600  decode.d0.loss_mask: 0.3188  decode.d0.loss_dice: 0.3202  decode.d1.loss_cls: 0.3779  decode.d1.loss_mask: 0.3383  decode.d1.loss_dice: 0.3246  decode.d2.loss_cls: 0.3193  decode.d2.loss_mask: 0.3163  decode.d2.loss_dice: 0.2846  decode.d3.loss_cls: 0.3262  decode.d3.loss_mask: 0.3098  decode.d3.loss_dice: 0.2825  decode.d4.loss_cls: 0.2943  decode.d4.loss_mask: 0.3116  decode.d4.loss_dice: 0.2887  decode.d5.loss_cls: 0.3027  decode.d5.loss_mask: 0.3093  decode.d5.loss_dice: 0.3028  decode.d6.loss_cls: 0.2857  decode.d6.loss_mask: 0.3100  decode.d6.loss_dice: 0.3048  decode.d7.loss_cls: 0.2945  decode.d7.loss_mask: 0.3125  decode.d7.loss_dice: 0.3044  decode.d8.loss_cls: 0.2829  decode.d8.loss_mask: 0.3158  decode.d8.loss_dice: 0.3328
08/06 03:16:29 - mmengine - INFO - Iter(train) [  8200/320000]  base_lr: 9.7691e-05 lr: 9.7691e-06  eta: 1 day, 13:50:33  time: 0.4356  data_time: 0.0088  memory: 5240  grad_norm: 108.7461  loss: 9.9749  decode.loss_cls: 0.3651  decode.loss_mask: 0.2122  decode.loss_dice: 0.3289  decode.d0.loss_cls: 1.1179  decode.d0.loss_mask: 0.2179  decode.d0.loss_dice: 0.3727  decode.d1.loss_cls: 0.3768  decode.d1.loss_mask: 0.2205  decode.d1.loss_dice: 0.3871  decode.d2.loss_cls: 0.3495  decode.d2.loss_mask: 0.2186  decode.d2.loss_dice: 0.3337  decode.d3.loss_cls: 0.3018  decode.d3.loss_mask: 0.2129  decode.d3.loss_dice: 0.3424  decode.d4.loss_cls: 0.3520  decode.d4.loss_mask: 0.2173  decode.d4.loss_dice: 0.3611  decode.d5.loss_cls: 0.3605  decode.d5.loss_mask: 0.2116  decode.d5.loss_dice: 0.3215  decode.d6.loss_cls: 0.3714  decode.d6.loss_mask: 0.2138  decode.d6.loss_dice: 0.3280  decode.d7.loss_cls: 0.3792  decode.d7.loss_mask: 0.2139  decode.d7.loss_dice: 0.3257  decode.d8.loss_cls: 0.4045  decode.d8.loss_mask: 0.2136  decode.d8.loss_dice: 0.3428
08/06 03:16:50 - mmengine - INFO - Iter(train) [  8250/320000]  base_lr: 9.7677e-05 lr: 9.7677e-06  eta: 1 day, 13:50:09  time: 0.4361  data_time: 0.0089  memory: 5258  grad_norm: 76.7433  loss: 10.5412  decode.loss_cls: 0.3554  decode.loss_mask: 0.2734  decode.loss_dice: 0.3090  decode.d0.loss_cls: 1.1645  decode.d0.loss_mask: 0.3034  decode.d0.loss_dice: 0.3781  decode.d1.loss_cls: 0.3914  decode.d1.loss_mask: 0.2840  decode.d1.loss_dice: 0.3174  decode.d2.loss_cls: 0.3747  decode.d2.loss_mask: 0.2793  decode.d2.loss_dice: 0.3064  decode.d3.loss_cls: 0.4307  decode.d3.loss_mask: 0.2779  decode.d3.loss_dice: 0.3187  decode.d4.loss_cls: 0.4311  decode.d4.loss_mask: 0.2774  decode.d4.loss_dice: 0.3080  decode.d5.loss_cls: 0.3856  decode.d5.loss_mask: 0.2764  decode.d5.loss_dice: 0.3331  decode.d6.loss_cls: 0.3308  decode.d6.loss_mask: 0.2741  decode.d6.loss_dice: 0.3048  decode.d7.loss_cls: 0.3356  decode.d7.loss_mask: 0.2713  decode.d7.loss_dice: 0.2792  decode.d8.loss_cls: 0.3919  decode.d8.loss_mask: 0.2718  decode.d8.loss_dice: 0.3059
08/06 03:17:12 - mmengine - INFO - Iter(train) [  8300/320000]  base_lr: 9.7663e-05 lr: 9.7663e-06  eta: 1 day, 13:49:46  time: 0.4368  data_time: 0.0089  memory: 5260  grad_norm: 154.7235  loss: 13.3355  decode.loss_cls: 0.3962  decode.loss_mask: 0.3473  decode.loss_dice: 0.4163  decode.d0.loss_cls: 1.2894  decode.d0.loss_mask: 0.3594  decode.d0.loss_dice: 0.4444  decode.d1.loss_cls: 0.6371  decode.d1.loss_mask: 0.3588  decode.d1.loss_dice: 0.4054  decode.d2.loss_cls: 0.5564  decode.d2.loss_mask: 0.3628  decode.d2.loss_dice: 0.4079  decode.d3.loss_cls: 0.4603  decode.d3.loss_mask: 0.3542  decode.d3.loss_dice: 0.4138  decode.d4.loss_cls: 0.4940  decode.d4.loss_mask: 0.3465  decode.d4.loss_dice: 0.3808  decode.d5.loss_cls: 0.5126  decode.d5.loss_mask: 0.3549  decode.d5.loss_dice: 0.4085  decode.d6.loss_cls: 0.4970  decode.d6.loss_mask: 0.3516  decode.d6.loss_dice: 0.4075  decode.d7.loss_cls: 0.4691  decode.d7.loss_mask: 0.3544  decode.d7.loss_dice: 0.4115  decode.d8.loss_cls: 0.3893  decode.d8.loss_mask: 0.3549  decode.d8.loss_dice: 0.3932
08/06 03:17:34 - mmengine - INFO - Iter(train) [  8350/320000]  base_lr: 9.7649e-05 lr: 9.7649e-06  eta: 1 day, 13:49:22  time: 0.4355  data_time: 0.0090  memory: 5260  grad_norm: 122.6258  loss: 10.6046  decode.loss_cls: 0.2569  decode.loss_mask: 0.3711  decode.loss_dice: 0.3192  decode.d0.loss_cls: 1.0962  decode.d0.loss_mask: 0.3598  decode.d0.loss_dice: 0.3709  decode.d1.loss_cls: 0.3345  decode.d1.loss_mask: 0.3405  decode.d1.loss_dice: 0.3274  decode.d2.loss_cls: 0.2855  decode.d2.loss_mask: 0.3426  decode.d2.loss_dice: 0.3304  decode.d3.loss_cls: 0.2871  decode.d3.loss_mask: 0.3395  decode.d3.loss_dice: 0.3094  decode.d4.loss_cls: 0.2482  decode.d4.loss_mask: 0.3532  decode.d4.loss_dice: 0.3410  decode.d5.loss_cls: 0.3587  decode.d5.loss_mask: 0.3623  decode.d5.loss_dice: 0.3351  decode.d6.loss_cls: 0.2651  decode.d6.loss_mask: 0.3772  decode.d6.loss_dice: 0.3178  decode.d7.loss_cls: 0.2758  decode.d7.loss_mask: 0.3662  decode.d7.loss_dice: 0.3430  decode.d8.loss_cls: 0.2677  decode.d8.loss_mask: 0.3724  decode.d8.loss_dice: 0.3499
08/06 03:17:56 - mmengine - INFO - Iter(train) [  8400/320000]  base_lr: 9.7635e-05 lr: 9.7635e-06  eta: 1 day, 13:48:58  time: 0.4354  data_time: 0.0090  memory: 5260  grad_norm: 103.2839  loss: 10.9991  decode.loss_cls: 0.3424  decode.loss_mask: 0.2935  decode.loss_dice: 0.3039  decode.d0.loss_cls: 1.1440  decode.d0.loss_mask: 0.2881  decode.d0.loss_dice: 0.3319  decode.d1.loss_cls: 0.5059  decode.d1.loss_mask: 0.2719  decode.d1.loss_dice: 0.3230  decode.d2.loss_cls: 0.4063  decode.d2.loss_mask: 0.2695  decode.d2.loss_dice: 0.2990  decode.d3.loss_cls: 0.4355  decode.d3.loss_mask: 0.2751  decode.d3.loss_dice: 0.2972  decode.d4.loss_cls: 0.4587  decode.d4.loss_mask: 0.2778  decode.d4.loss_dice: 0.3220  decode.d5.loss_cls: 0.4873  decode.d5.loss_mask: 0.2810  decode.d5.loss_dice: 0.3164  decode.d6.loss_cls: 0.5087  decode.d6.loss_mask: 0.2741  decode.d6.loss_dice: 0.2893  decode.d7.loss_cls: 0.4202  decode.d7.loss_mask: 0.2868  decode.d7.loss_dice: 0.2902  decode.d8.loss_cls: 0.3989  decode.d8.loss_mask: 0.2934  decode.d8.loss_dice: 0.3070
08/06 03:18:18 - mmengine - INFO - Iter(train) [  8450/320000]  base_lr: 9.7621e-05 lr: 9.7621e-06  eta: 1 day, 13:48:34  time: 0.4352  data_time: 0.0088  memory: 5224  grad_norm: 115.2447  loss: 10.7842  decode.loss_cls: 0.3394  decode.loss_mask: 0.2803  decode.loss_dice: 0.3125  decode.d0.loss_cls: 1.2147  decode.d0.loss_mask: 0.2537  decode.d0.loss_dice: 0.3446  decode.d1.loss_cls: 0.4939  decode.d1.loss_mask: 0.2661  decode.d1.loss_dice: 0.3219  decode.d2.loss_cls: 0.3852  decode.d2.loss_mask: 0.2416  decode.d2.loss_dice: 0.3211  decode.d3.loss_cls: 0.4220  decode.d3.loss_mask: 0.2579  decode.d3.loss_dice: 0.3246  decode.d4.loss_cls: 0.3942  decode.d4.loss_mask: 0.2521  decode.d4.loss_dice: 0.3010  decode.d5.loss_cls: 0.4408  decode.d5.loss_mask: 0.2618  decode.d5.loss_dice: 0.3118  decode.d6.loss_cls: 0.4927  decode.d6.loss_mask: 0.2613  decode.d6.loss_dice: 0.3013  decode.d7.loss_cls: 0.4357  decode.d7.loss_mask: 0.2616  decode.d7.loss_dice: 0.3069  decode.d8.loss_cls: 0.4214  decode.d8.loss_mask: 0.2551  decode.d8.loss_dice: 0.3069
08/06 03:18:39 - mmengine - INFO - Iter(train) [  8500/320000]  base_lr: 9.7606e-05 lr: 9.7606e-06  eta: 1 day, 13:48:10  time: 0.4355  data_time: 0.0089  memory: 5242  grad_norm: 128.4446  loss: 11.7115  decode.loss_cls: 0.4312  decode.loss_mask: 0.3204  decode.loss_dice: 0.3781  decode.d0.loss_cls: 1.0589  decode.d0.loss_mask: 0.3437  decode.d0.loss_dice: 0.3959  decode.d1.loss_cls: 0.4530  decode.d1.loss_mask: 0.3111  decode.d1.loss_dice: 0.3957  decode.d2.loss_cls: 0.4389  decode.d2.loss_mask: 0.3018  decode.d2.loss_dice: 0.3521  decode.d3.loss_cls: 0.4108  decode.d3.loss_mask: 0.3029  decode.d3.loss_dice: 0.3593  decode.d4.loss_cls: 0.3884  decode.d4.loss_mask: 0.3055  decode.d4.loss_dice: 0.3672  decode.d5.loss_cls: 0.3585  decode.d5.loss_mask: 0.3130  decode.d5.loss_dice: 0.3651  decode.d6.loss_cls: 0.4191  decode.d6.loss_mask: 0.3145  decode.d6.loss_dice: 0.3842  decode.d7.loss_cls: 0.4471  decode.d7.loss_mask: 0.3103  decode.d7.loss_dice: 0.3776  decode.d8.loss_cls: 0.4255  decode.d8.loss_mask: 0.3158  decode.d8.loss_dice: 0.3659
08/06 03:19:01 - mmengine - INFO - Iter(train) [  8550/320000]  base_lr: 9.7592e-05 lr: 9.7592e-06  eta: 1 day, 13:47:47  time: 0.4351  data_time: 0.0089  memory: 5258  grad_norm: 120.6060  loss: 10.6850  decode.loss_cls: 0.3117  decode.loss_mask: 0.3383  decode.loss_dice: 0.3733  decode.d0.loss_cls: 0.8920  decode.d0.loss_mask: 0.3450  decode.d0.loss_dice: 0.4215  decode.d1.loss_cls: 0.3081  decode.d1.loss_mask: 0.3327  decode.d1.loss_dice: 0.3858  decode.d2.loss_cls: 0.2050  decode.d2.loss_mask: 0.3350  decode.d2.loss_dice: 0.3812  decode.d3.loss_cls: 0.1974  decode.d3.loss_mask: 0.3411  decode.d3.loss_dice: 0.3965  decode.d4.loss_cls: 0.2998  decode.d4.loss_mask: 0.3433  decode.d4.loss_dice: 0.3871  decode.d5.loss_cls: 0.2549  decode.d5.loss_mask: 0.3480  decode.d5.loss_dice: 0.4115  decode.d6.loss_cls: 0.3242  decode.d6.loss_mask: 0.3298  decode.d6.loss_dice: 0.3790  decode.d7.loss_cls: 0.3056  decode.d7.loss_mask: 0.3371  decode.d7.loss_dice: 0.3758  decode.d8.loss_cls: 0.2985  decode.d8.loss_mask: 0.3454  decode.d8.loss_dice: 0.3802
08/06 03:19:23 - mmengine - INFO - Iter(train) [  8600/320000]  base_lr: 9.7578e-05 lr: 9.7578e-06  eta: 1 day, 13:47:24  time: 0.4366  data_time: 0.0090  memory: 5242  grad_norm: 127.2727  loss: 10.7745  decode.loss_cls: 0.4260  decode.loss_mask: 0.2459  decode.loss_dice: 0.2908  decode.d0.loss_cls: 1.4138  decode.d0.loss_mask: 0.2501  decode.d0.loss_dice: 0.3202  decode.d1.loss_cls: 0.5809  decode.d1.loss_mask: 0.2490  decode.d1.loss_dice: 0.3015  decode.d2.loss_cls: 0.3820  decode.d2.loss_mask: 0.2470  decode.d2.loss_dice: 0.2892  decode.d3.loss_cls: 0.3892  decode.d3.loss_mask: 0.2506  decode.d3.loss_dice: 0.2918  decode.d4.loss_cls: 0.4891  decode.d4.loss_mask: 0.2463  decode.d4.loss_dice: 0.2675  decode.d5.loss_cls: 0.3953  decode.d5.loss_mask: 0.2442  decode.d5.loss_dice: 0.2747  decode.d6.loss_cls: 0.4317  decode.d6.loss_mask: 0.2530  decode.d6.loss_dice: 0.2760  decode.d7.loss_cls: 0.4729  decode.d7.loss_mask: 0.2489  decode.d7.loss_dice: 0.2801  decode.d8.loss_cls: 0.4239  decode.d8.loss_mask: 0.2487  decode.d8.loss_dice: 0.2943
08/06 03:19:45 - mmengine - INFO - Iter(train) [  8650/320000]  base_lr: 9.7564e-05 lr: 9.7564e-06  eta: 1 day, 13:47:02  time: 0.4381  data_time: 0.0088  memory: 5260  grad_norm: 88.7506  loss: 9.0137  decode.loss_cls: 0.2191  decode.loss_mask: 0.2682  decode.loss_dice: 0.3008  decode.d0.loss_cls: 0.9392  decode.d0.loss_mask: 0.2690  decode.d0.loss_dice: 0.2977  decode.d1.loss_cls: 0.4046  decode.d1.loss_mask: 0.2622  decode.d1.loss_dice: 0.2796  decode.d2.loss_cls: 0.3373  decode.d2.loss_mask: 0.2639  decode.d2.loss_dice: 0.2896  decode.d3.loss_cls: 0.2582  decode.d3.loss_mask: 0.2636  decode.d3.loss_dice: 0.2819  decode.d4.loss_cls: 0.2813  decode.d4.loss_mask: 0.2634  decode.d4.loss_dice: 0.2769  decode.d5.loss_cls: 0.2748  decode.d5.loss_mask: 0.2652  decode.d5.loss_dice: 0.2846  decode.d6.loss_cls: 0.2619  decode.d6.loss_mask: 0.2691  decode.d6.loss_dice: 0.2894  decode.d7.loss_cls: 0.2631  decode.d7.loss_mask: 0.2624  decode.d7.loss_dice: 0.2843  decode.d8.loss_cls: 0.2318  decode.d8.loss_mask: 0.2722  decode.d8.loss_dice: 0.2984
08/06 03:20:07 - mmengine - INFO - Iter(train) [  8700/320000]  base_lr: 9.7550e-05 lr: 9.7550e-06  eta: 1 day, 13:46:38  time: 0.4361  data_time: 0.0088  memory: 5260  grad_norm: 92.5614  loss: 11.8202  decode.loss_cls: 0.3121  decode.loss_mask: 0.2812  decode.loss_dice: 0.4022  decode.d0.loss_cls: 1.2890  decode.d0.loss_mask: 0.2728  decode.d0.loss_dice: 0.4160  decode.d1.loss_cls: 0.6059  decode.d1.loss_mask: 0.2878  decode.d1.loss_dice: 0.4115  decode.d2.loss_cls: 0.4479  decode.d2.loss_mask: 0.2775  decode.d2.loss_dice: 0.4064  decode.d3.loss_cls: 0.4525  decode.d3.loss_mask: 0.2759  decode.d3.loss_dice: 0.3962  decode.d4.loss_cls: 0.3805  decode.d4.loss_mask: 0.2797  decode.d4.loss_dice: 0.3831  decode.d5.loss_cls: 0.4215  decode.d5.loss_mask: 0.2829  decode.d5.loss_dice: 0.3812  decode.d6.loss_cls: 0.3809  decode.d6.loss_mask: 0.2769  decode.d6.loss_dice: 0.3755  decode.d7.loss_cls: 0.4135  decode.d7.loss_mask: 0.2765  decode.d7.loss_dice: 0.3775  decode.d8.loss_cls: 0.3800  decode.d8.loss_mask: 0.2756  decode.d8.loss_dice: 0.4001
08/06 03:20:28 - mmengine - INFO - Iter(train) [  8750/320000]  base_lr: 9.7536e-05 lr: 9.7536e-06  eta: 1 day, 13:46:14  time: 0.4352  data_time: 0.0089  memory: 5240  grad_norm: 118.4896  loss: 9.8141  decode.loss_cls: 0.3310  decode.loss_mask: 0.2472  decode.loss_dice: 0.3144  decode.d0.loss_cls: 0.9943  decode.d0.loss_mask: 0.2337  decode.d0.loss_dice: 0.2991  decode.d1.loss_cls: 0.4202  decode.d1.loss_mask: 0.2420  decode.d1.loss_dice: 0.3308  decode.d2.loss_cls: 0.4174  decode.d2.loss_mask: 0.2409  decode.d2.loss_dice: 0.3393  decode.d3.loss_cls: 0.3397  decode.d3.loss_mask: 0.2344  decode.d3.loss_dice: 0.2932  decode.d4.loss_cls: 0.3697  decode.d4.loss_mask: 0.2420  decode.d4.loss_dice: 0.3151  decode.d5.loss_cls: 0.3934  decode.d5.loss_mask: 0.2381  decode.d5.loss_dice: 0.3078  decode.d6.loss_cls: 0.3489  decode.d6.loss_mask: 0.2426  decode.d6.loss_dice: 0.2816  decode.d7.loss_cls: 0.3614  decode.d7.loss_mask: 0.2427  decode.d7.loss_dice: 0.3122  decode.d8.loss_cls: 0.3271  decode.d8.loss_mask: 0.2454  decode.d8.loss_dice: 0.3085
08/06 03:20:50 - mmengine - INFO - Iter(train) [  8800/320000]  base_lr: 9.7522e-05 lr: 9.7522e-06  eta: 1 day, 13:45:50  time: 0.4347  data_time: 0.0088  memory: 5260  grad_norm: 169.9355  loss: 10.5459  decode.loss_cls: 0.2507  decode.loss_mask: 0.2982  decode.loss_dice: 0.3216  decode.d0.loss_cls: 1.0232  decode.d0.loss_mask: 0.3034  decode.d0.loss_dice: 0.3655  decode.d1.loss_cls: 0.4903  decode.d1.loss_mask: 0.3083  decode.d1.loss_dice: 0.3367  decode.d2.loss_cls: 0.3938  decode.d2.loss_mask: 0.2979  decode.d2.loss_dice: 0.3188  decode.d3.loss_cls: 0.3888  decode.d3.loss_mask: 0.3038  decode.d3.loss_dice: 0.3233  decode.d4.loss_cls: 0.3880  decode.d4.loss_mask: 0.3008  decode.d4.loss_dice: 0.3246  decode.d5.loss_cls: 0.3427  decode.d5.loss_mask: 0.2975  decode.d5.loss_dice: 0.3238  decode.d6.loss_cls: 0.2995  decode.d6.loss_mask: 0.3080  decode.d6.loss_dice: 0.3401  decode.d7.loss_cls: 0.3285  decode.d7.loss_mask: 0.2945  decode.d7.loss_dice: 0.3337  decode.d8.loss_cls: 0.2758  decode.d8.loss_mask: 0.3041  decode.d8.loss_dice: 0.3601
08/06 03:21:12 - mmengine - INFO - Iter(train) [  8850/320000]  base_lr: 9.7508e-05 lr: 9.7508e-06  eta: 1 day, 13:45:26  time: 0.4356  data_time: 0.0087  memory: 5240  grad_norm: 156.1897  loss: 13.2772  decode.loss_cls: 0.5615  decode.loss_mask: 0.2925  decode.loss_dice: 0.3138  decode.d0.loss_cls: 1.2499  decode.d0.loss_mask: 0.3191  decode.d0.loss_dice: 0.3886  decode.d1.loss_cls: 0.6951  decode.d1.loss_mask: 0.3072  decode.d1.loss_dice: 0.3970  decode.d2.loss_cls: 0.5790  decode.d2.loss_mask: 0.3037  decode.d2.loss_dice: 0.3894  decode.d3.loss_cls: 0.5858  decode.d3.loss_mask: 0.2946  decode.d3.loss_dice: 0.3297  decode.d4.loss_cls: 0.5874  decode.d4.loss_mask: 0.2929  decode.d4.loss_dice: 0.3479  decode.d5.loss_cls: 0.6124  decode.d5.loss_mask: 0.2976  decode.d5.loss_dice: 0.3660  decode.d6.loss_cls: 0.5900  decode.d6.loss_mask: 0.2972  decode.d6.loss_dice: 0.3419  decode.d7.loss_cls: 0.6124  decode.d7.loss_mask: 0.2943  decode.d7.loss_dice: 0.3682  decode.d8.loss_cls: 0.5993  decode.d8.loss_mask: 0.2957  decode.d8.loss_dice: 0.3674
08/06 03:21:34 - mmengine - INFO - Iter(train) [  8900/320000]  base_lr: 9.7494e-05 lr: 9.7494e-06  eta: 1 day, 13:45:02  time: 0.4354  data_time: 0.0088  memory: 5242  grad_norm: 145.5456  loss: 11.1905  decode.loss_cls: 0.3949  decode.loss_mask: 0.2583  decode.loss_dice: 0.3934  decode.d0.loss_cls: 1.1599  decode.d0.loss_mask: 0.2719  decode.d0.loss_dice: 0.4504  decode.d1.loss_cls: 0.5220  decode.d1.loss_mask: 0.2584  decode.d1.loss_dice: 0.3898  decode.d2.loss_cls: 0.3731  decode.d2.loss_mask: 0.2634  decode.d2.loss_dice: 0.3764  decode.d3.loss_cls: 0.3638  decode.d3.loss_mask: 0.2593  decode.d3.loss_dice: 0.3498  decode.d4.loss_cls: 0.3611  decode.d4.loss_mask: 0.2648  decode.d4.loss_dice: 0.3904  decode.d5.loss_cls: 0.3890  decode.d5.loss_mask: 0.2624  decode.d5.loss_dice: 0.3774  decode.d6.loss_cls: 0.3933  decode.d6.loss_mask: 0.2676  decode.d6.loss_dice: 0.3903  decode.d7.loss_cls: 0.3559  decode.d7.loss_mask: 0.2640  decode.d7.loss_dice: 0.4037  decode.d8.loss_cls: 0.3538  decode.d8.loss_mask: 0.2645  decode.d8.loss_dice: 0.3674
08/06 03:21:56 - mmengine - INFO - Iter(train) [  8950/320000]  base_lr: 9.7480e-05 lr: 9.7480e-06  eta: 1 day, 13:44:38  time: 0.4355  data_time: 0.0087  memory: 5258  grad_norm: 191.5479  loss: 12.2424  decode.loss_cls: 0.4297  decode.loss_mask: 0.2969  decode.loss_dice: 0.3166  decode.d0.loss_cls: 1.3126  decode.d0.loss_mask: 0.3203  decode.d0.loss_dice: 0.3402  decode.d1.loss_cls: 0.6594  decode.d1.loss_mask: 0.3048  decode.d1.loss_dice: 0.3376  decode.d2.loss_cls: 0.6462  decode.d2.loss_mask: 0.2983  decode.d2.loss_dice: 0.3058  decode.d3.loss_cls: 0.6141  decode.d3.loss_mask: 0.2968  decode.d3.loss_dice: 0.3061  decode.d4.loss_cls: 0.5540  decode.d4.loss_mask: 0.2967  decode.d4.loss_dice: 0.3198  decode.d5.loss_cls: 0.4425  decode.d5.loss_mask: 0.2956  decode.d5.loss_dice: 0.3130  decode.d6.loss_cls: 0.4681  decode.d6.loss_mask: 0.2906  decode.d6.loss_dice: 0.3034  decode.d7.loss_cls: 0.4720  decode.d7.loss_mask: 0.2888  decode.d7.loss_dice: 0.3094  decode.d8.loss_cls: 0.4675  decode.d8.loss_mask: 0.3174  decode.d8.loss_dice: 0.3181
08/06 03:22:17 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 03:22:17 - mmengine - INFO - Iter(train) [  9000/320000]  base_lr: 9.7465e-05 lr: 9.7465e-06  eta: 1 day, 13:44:14  time: 0.4353  data_time: 0.0088  memory: 5242  grad_norm: 77.6464  loss: 10.5308  decode.loss_cls: 0.3574  decode.loss_mask: 0.2734  decode.loss_dice: 0.3310  decode.d0.loss_cls: 1.0434  decode.d0.loss_mask: 0.2983  decode.d0.loss_dice: 0.3560  decode.d1.loss_cls: 0.4403  decode.d1.loss_mask: 0.2983  decode.d1.loss_dice: 0.3307  decode.d2.loss_cls: 0.4680  decode.d2.loss_mask: 0.2756  decode.d2.loss_dice: 0.3168  decode.d3.loss_cls: 0.3519  decode.d3.loss_mask: 0.2866  decode.d3.loss_dice: 0.3092  decode.d4.loss_cls: 0.3624  decode.d4.loss_mask: 0.2706  decode.d4.loss_dice: 0.3301  decode.d5.loss_cls: 0.3572  decode.d5.loss_mask: 0.2615  decode.d5.loss_dice: 0.3228  decode.d6.loss_cls: 0.3823  decode.d6.loss_mask: 0.2651  decode.d6.loss_dice: 0.3245  decode.d7.loss_cls: 0.3805  decode.d7.loss_mask: 0.2701  decode.d7.loss_dice: 0.3190  decode.d8.loss_cls: 0.3336  decode.d8.loss_mask: 0.2792  decode.d8.loss_dice: 0.3351
08/06 03:22:39 - mmengine - INFO - Iter(train) [  9050/320000]  base_lr: 9.7451e-05 lr: 9.7451e-06  eta: 1 day, 13:43:50  time: 0.4354  data_time: 0.0089  memory: 5260  grad_norm: 135.5322  loss: 12.5171  decode.loss_cls: 0.4651  decode.loss_mask: 0.3208  decode.loss_dice: 0.4118  decode.d0.loss_cls: 1.1212  decode.d0.loss_mask: 0.3129  decode.d0.loss_dice: 0.3970  decode.d1.loss_cls: 0.5213  decode.d1.loss_mask: 0.3100  decode.d1.loss_dice: 0.3927  decode.d2.loss_cls: 0.5261  decode.d2.loss_mask: 0.2949  decode.d2.loss_dice: 0.3756  decode.d3.loss_cls: 0.4286  decode.d3.loss_mask: 0.2898  decode.d3.loss_dice: 0.3821  decode.d4.loss_cls: 0.4646  decode.d4.loss_mask: 0.2913  decode.d4.loss_dice: 0.3786  decode.d5.loss_cls: 0.4656  decode.d5.loss_mask: 0.2911  decode.d5.loss_dice: 0.4062  decode.d6.loss_cls: 0.4927  decode.d6.loss_mask: 0.2979  decode.d6.loss_dice: 0.4206  decode.d7.loss_cls: 0.4777  decode.d7.loss_mask: 0.3047  decode.d7.loss_dice: 0.4114  decode.d8.loss_cls: 0.4910  decode.d8.loss_mask: 0.3194  decode.d8.loss_dice: 0.4546
08/06 03:23:01 - mmengine - INFO - Iter(train) [  9100/320000]  base_lr: 9.7437e-05 lr: 9.7437e-06  eta: 1 day, 13:43:26  time: 0.4352  data_time: 0.0087  memory: 5242  grad_norm: 120.5965  loss: 13.4162  decode.loss_cls: 0.5027  decode.loss_mask: 0.2962  decode.loss_dice: 0.4821  decode.d0.loss_cls: 1.0825  decode.d0.loss_mask: 0.3216  decode.d0.loss_dice: 0.4787  decode.d1.loss_cls: 0.5545  decode.d1.loss_mask: 0.3011  decode.d1.loss_dice: 0.4219  decode.d2.loss_cls: 0.4758  decode.d2.loss_mask: 0.3006  decode.d2.loss_dice: 0.4480  decode.d3.loss_cls: 0.5335  decode.d3.loss_mask: 0.3000  decode.d3.loss_dice: 0.4432  decode.d4.loss_cls: 0.5272  decode.d4.loss_mask: 0.3049  decode.d4.loss_dice: 0.4166  decode.d5.loss_cls: 0.5357  decode.d5.loss_mask: 0.3313  decode.d5.loss_dice: 0.4220  decode.d6.loss_cls: 0.5478  decode.d6.loss_mask: 0.2943  decode.d6.loss_dice: 0.4324  decode.d7.loss_cls: 0.5552  decode.d7.loss_mask: 0.3128  decode.d7.loss_dice: 0.4299  decode.d8.loss_cls: 0.6288  decode.d8.loss_mask: 0.3189  decode.d8.loss_dice: 0.4158
08/06 03:23:23 - mmengine - INFO - Iter(train) [  9150/320000]  base_lr: 9.7423e-05 lr: 9.7423e-06  eta: 1 day, 13:43:02  time: 0.4357  data_time: 0.0088  memory: 5242  grad_norm: 135.5182  loss: 11.4654  decode.loss_cls: 0.4390  decode.loss_mask: 0.2550  decode.loss_dice: 0.3798  decode.d0.loss_cls: 1.1891  decode.d0.loss_mask: 0.2664  decode.d0.loss_dice: 0.3877  decode.d1.loss_cls: 0.4966  decode.d1.loss_mask: 0.2851  decode.d1.loss_dice: 0.3780  decode.d2.loss_cls: 0.4219  decode.d2.loss_mask: 0.2692  decode.d2.loss_dice: 0.4052  decode.d3.loss_cls: 0.4433  decode.d3.loss_mask: 0.2733  decode.d3.loss_dice: 0.3632  decode.d4.loss_cls: 0.4464  decode.d4.loss_mask: 0.2554  decode.d4.loss_dice: 0.3437  decode.d5.loss_cls: 0.4414  decode.d5.loss_mask: 0.2636  decode.d5.loss_dice: 0.3602  decode.d6.loss_cls: 0.4128  decode.d6.loss_mask: 0.2605  decode.d6.loss_dice: 0.3081  decode.d7.loss_cls: 0.4083  decode.d7.loss_mask: 0.2632  decode.d7.loss_dice: 0.3392  decode.d8.loss_cls: 0.4759  decode.d8.loss_mask: 0.2632  decode.d8.loss_dice: 0.3707
08/06 03:23:44 - mmengine - INFO - Iter(train) [  9200/320000]  base_lr: 9.7409e-05 lr: 9.7409e-06  eta: 1 day, 13:42:38  time: 0.4353  data_time: 0.0088  memory: 5224  grad_norm: 87.9414  loss: 10.0601  decode.loss_cls: 0.4019  decode.loss_mask: 0.2238  decode.loss_dice: 0.3292  decode.d0.loss_cls: 1.1361  decode.d0.loss_mask: 0.2222  decode.d0.loss_dice: 0.3633  decode.d1.loss_cls: 0.6489  decode.d1.loss_mask: 0.2120  decode.d1.loss_dice: 0.3249  decode.d2.loss_cls: 0.3338  decode.d2.loss_mask: 0.2200  decode.d2.loss_dice: 0.3183  decode.d3.loss_cls: 0.2991  decode.d3.loss_mask: 0.2306  decode.d3.loss_dice: 0.3223  decode.d4.loss_cls: 0.3099  decode.d4.loss_mask: 0.2206  decode.d4.loss_dice: 0.2990  decode.d5.loss_cls: 0.3873  decode.d5.loss_mask: 0.2252  decode.d5.loss_dice: 0.3058  decode.d6.loss_cls: 0.3845  decode.d6.loss_mask: 0.2178  decode.d6.loss_dice: 0.3275  decode.d7.loss_cls: 0.3973  decode.d7.loss_mask: 0.2228  decode.d7.loss_dice: 0.2919  decode.d8.loss_cls: 0.3465  decode.d8.loss_mask: 0.2209  decode.d8.loss_dice: 0.3171
08/06 03:24:06 - mmengine - INFO - Iter(train) [  9250/320000]  base_lr: 9.7395e-05 lr: 9.7395e-06  eta: 1 day, 13:42:14  time: 0.4352  data_time: 0.0089  memory: 5242  grad_norm: 108.9688  loss: 9.6994  decode.loss_cls: 0.2740  decode.loss_mask: 0.2815  decode.loss_dice: 0.3280  decode.d0.loss_cls: 1.0445  decode.d0.loss_mask: 0.3004  decode.d0.loss_dice: 0.3311  decode.d1.loss_cls: 0.3382  decode.d1.loss_mask: 0.2924  decode.d1.loss_dice: 0.3310  decode.d2.loss_cls: 0.3003  decode.d2.loss_mask: 0.2808  decode.d2.loss_dice: 0.3316  decode.d3.loss_cls: 0.2810  decode.d3.loss_mask: 0.2806  decode.d3.loss_dice: 0.3156  decode.d4.loss_cls: 0.2841  decode.d4.loss_mask: 0.2790  decode.d4.loss_dice: 0.3247  decode.d5.loss_cls: 0.2735  decode.d5.loss_mask: 0.2778  decode.d5.loss_dice: 0.3150  decode.d6.loss_cls: 0.2889  decode.d6.loss_mask: 0.2782  decode.d6.loss_dice: 0.3116  decode.d7.loss_cls: 0.2931  decode.d7.loss_mask: 0.2797  decode.d7.loss_dice: 0.3209  decode.d8.loss_cls: 0.2537  decode.d8.loss_mask: 0.2789  decode.d8.loss_dice: 0.3295
08/06 03:24:28 - mmengine - INFO - Iter(train) [  9300/320000]  base_lr: 9.7381e-05 lr: 9.7381e-06  eta: 1 day, 13:41:51  time: 0.4363  data_time: 0.0089  memory: 5258  grad_norm: 90.8782  loss: 11.8560  decode.loss_cls: 0.3747  decode.loss_mask: 0.2836  decode.loss_dice: 0.4178  decode.d0.loss_cls: 1.0835  decode.d0.loss_mask: 0.3054  decode.d0.loss_dice: 0.4374  decode.d1.loss_cls: 0.5568  decode.d1.loss_mask: 0.2848  decode.d1.loss_dice: 0.4109  decode.d2.loss_cls: 0.4983  decode.d2.loss_mask: 0.2806  decode.d2.loss_dice: 0.3888  decode.d3.loss_cls: 0.4210  decode.d3.loss_mask: 0.2794  decode.d3.loss_dice: 0.3930  decode.d4.loss_cls: 0.3956  decode.d4.loss_mask: 0.3043  decode.d4.loss_dice: 0.4180  decode.d5.loss_cls: 0.3765  decode.d5.loss_mask: 0.3240  decode.d5.loss_dice: 0.3989  decode.d6.loss_cls: 0.3682  decode.d6.loss_mask: 0.3013  decode.d6.loss_dice: 0.4196  decode.d7.loss_cls: 0.3723  decode.d7.loss_mask: 0.2987  decode.d7.loss_dice: 0.4063  decode.d8.loss_cls: 0.3468  decode.d8.loss_mask: 0.2889  decode.d8.loss_dice: 0.4207
08/06 03:24:50 - mmengine - INFO - Iter(train) [  9350/320000]  base_lr: 9.7367e-05 lr: 9.7367e-06  eta: 1 day, 13:41:28  time: 0.4362  data_time: 0.0089  memory: 5224  grad_norm: 169.3720  loss: 11.8823  decode.loss_cls: 0.3513  decode.loss_mask: 0.3865  decode.loss_dice: 0.3552  decode.d0.loss_cls: 1.2811  decode.d0.loss_mask: 0.3793  decode.d0.loss_dice: 0.3735  decode.d1.loss_cls: 0.4440  decode.d1.loss_mask: 0.3684  decode.d1.loss_dice: 0.3414  decode.d2.loss_cls: 0.4325  decode.d2.loss_mask: 0.3413  decode.d2.loss_dice: 0.3310  decode.d3.loss_cls: 0.4238  decode.d3.loss_mask: 0.3470  decode.d3.loss_dice: 0.3345  decode.d4.loss_cls: 0.4239  decode.d4.loss_mask: 0.3554  decode.d4.loss_dice: 0.3375  decode.d5.loss_cls: 0.3521  decode.d5.loss_mask: 0.3759  decode.d5.loss_dice: 0.3605  decode.d6.loss_cls: 0.3232  decode.d6.loss_mask: 0.3788  decode.d6.loss_dice: 0.3435  decode.d7.loss_cls: 0.3623  decode.d7.loss_mask: 0.3525  decode.d7.loss_dice: 0.3475  decode.d8.loss_cls: 0.3351  decode.d8.loss_mask: 0.3854  decode.d8.loss_dice: 0.3575
08/06 03:25:12 - mmengine - INFO - Iter(train) [  9400/320000]  base_lr: 9.7353e-05 lr: 9.7353e-06  eta: 1 day, 13:41:05  time: 0.4368  data_time: 0.0089  memory: 5260  grad_norm: 129.1814  loss: 10.0115  decode.loss_cls: 0.3227  decode.loss_mask: 0.2617  decode.loss_dice: 0.3256  decode.d0.loss_cls: 1.1881  decode.d0.loss_mask: 0.2384  decode.d0.loss_dice: 0.3446  decode.d1.loss_cls: 0.4726  decode.d1.loss_mask: 0.2346  decode.d1.loss_dice: 0.3187  decode.d2.loss_cls: 0.3288  decode.d2.loss_mask: 0.2674  decode.d2.loss_dice: 0.3429  decode.d3.loss_cls: 0.3892  decode.d3.loss_mask: 0.2509  decode.d3.loss_dice: 0.3188  decode.d4.loss_cls: 0.2724  decode.d4.loss_mask: 0.2445  decode.d4.loss_dice: 0.3149  decode.d5.loss_cls: 0.3348  decode.d5.loss_mask: 0.2554  decode.d5.loss_dice: 0.3119  decode.d6.loss_cls: 0.3422  decode.d6.loss_mask: 0.2479  decode.d6.loss_dice: 0.3209  decode.d7.loss_cls: 0.2739  decode.d7.loss_mask: 0.2642  decode.d7.loss_dice: 0.3162  decode.d8.loss_cls: 0.3173  decode.d8.loss_mask: 0.2637  decode.d8.loss_dice: 0.3262
08/06 03:25:34 - mmengine - INFO - Iter(train) [  9450/320000]  base_lr: 9.7338e-05 lr: 9.7338e-06  eta: 1 day, 13:40:47  time: 0.4354  data_time: 0.0089  memory: 5260  grad_norm: 126.6579  loss: 11.1639  decode.loss_cls: 0.3000  decode.loss_mask: 0.2867  decode.loss_dice: 0.3653  decode.d0.loss_cls: 1.0724  decode.d0.loss_mask: 0.2830  decode.d0.loss_dice: 0.3764  decode.d1.loss_cls: 0.4910  decode.d1.loss_mask: 0.2735  decode.d1.loss_dice: 0.3823  decode.d2.loss_cls: 0.4148  decode.d2.loss_mask: 0.3116  decode.d2.loss_dice: 0.3679  decode.d3.loss_cls: 0.3762  decode.d3.loss_mask: 0.2911  decode.d3.loss_dice: 0.3704  decode.d4.loss_cls: 0.3634  decode.d4.loss_mask: 0.2904  decode.d4.loss_dice: 0.3861  decode.d5.loss_cls: 0.3833  decode.d5.loss_mask: 0.2915  decode.d5.loss_dice: 0.3620  decode.d6.loss_cls: 0.3932  decode.d6.loss_mask: 0.2922  decode.d6.loss_dice: 0.3730  decode.d7.loss_cls: 0.3695  decode.d7.loss_mask: 0.3014  decode.d7.loss_dice: 0.3723  decode.d8.loss_cls: 0.3738  decode.d8.loss_mask: 0.2928  decode.d8.loss_dice: 0.3562
08/06 03:25:55 - mmengine - INFO - Iter(train) [  9500/320000]  base_lr: 9.7324e-05 lr: 9.7324e-06  eta: 1 day, 13:40:24  time: 0.4356  data_time: 0.0089  memory: 5240  grad_norm: 143.0751  loss: 11.6443  decode.loss_cls: 0.3650  decode.loss_mask: 0.2880  decode.loss_dice: 0.3590  decode.d0.loss_cls: 1.1714  decode.d0.loss_mask: 0.3036  decode.d0.loss_dice: 0.3771  decode.d1.loss_cls: 0.4998  decode.d1.loss_mask: 0.2924  decode.d1.loss_dice: 0.3672  decode.d2.loss_cls: 0.5353  decode.d2.loss_mask: 0.2981  decode.d2.loss_dice: 0.3736  decode.d3.loss_cls: 0.5236  decode.d3.loss_mask: 0.2883  decode.d3.loss_dice: 0.3264  decode.d4.loss_cls: 0.5002  decode.d4.loss_mask: 0.2888  decode.d4.loss_dice: 0.3655  decode.d5.loss_cls: 0.4165  decode.d5.loss_mask: 0.2935  decode.d5.loss_dice: 0.3700  decode.d6.loss_cls: 0.3753  decode.d6.loss_mask: 0.2870  decode.d6.loss_dice: 0.3441  decode.d7.loss_cls: 0.3875  decode.d7.loss_mask: 0.2894  decode.d7.loss_dice: 0.3595  decode.d8.loss_cls: 0.3825  decode.d8.loss_mask: 0.2889  decode.d8.loss_dice: 0.3270
08/06 03:26:17 - mmengine - INFO - Iter(train) [  9550/320000]  base_lr: 9.7310e-05 lr: 9.7310e-06  eta: 1 day, 13:40:00  time: 0.4354  data_time: 0.0089  memory: 5260  grad_norm: 108.7707  loss: 9.4719  decode.loss_cls: 0.2949  decode.loss_mask: 0.2736  decode.loss_dice: 0.2513  decode.d0.loss_cls: 0.9280  decode.d0.loss_mask: 0.3043  decode.d0.loss_dice: 0.3208  decode.d1.loss_cls: 0.3708  decode.d1.loss_mask: 0.2806  decode.d1.loss_dice: 0.2824  decode.d2.loss_cls: 0.3104  decode.d2.loss_mask: 0.2741  decode.d2.loss_dice: 0.3053  decode.d3.loss_cls: 0.3012  decode.d3.loss_mask: 0.2721  decode.d3.loss_dice: 0.2778  decode.d4.loss_cls: 0.3321  decode.d4.loss_mask: 0.2740  decode.d4.loss_dice: 0.2962  decode.d5.loss_cls: 0.3084  decode.d5.loss_mask: 0.2761  decode.d5.loss_dice: 0.2796  decode.d6.loss_cls: 0.3199  decode.d6.loss_mask: 0.2767  decode.d6.loss_dice: 0.2727  decode.d7.loss_cls: 0.3391  decode.d7.loss_mask: 0.2755  decode.d7.loss_dice: 0.2725  decode.d8.loss_cls: 0.3450  decode.d8.loss_mask: 0.2754  decode.d8.loss_dice: 0.2810
08/06 03:26:39 - mmengine - INFO - Iter(train) [  9600/320000]  base_lr: 9.7296e-05 lr: 9.7296e-06  eta: 1 day, 13:39:38  time: 0.4364  data_time: 0.0090  memory: 5260  grad_norm: 121.0914  loss: 9.4819  decode.loss_cls: 0.2761  decode.loss_mask: 0.2313  decode.loss_dice: 0.3212  decode.d0.loss_cls: 1.1410  decode.d0.loss_mask: 0.2564  decode.d0.loss_dice: 0.3679  decode.d1.loss_cls: 0.4267  decode.d1.loss_mask: 0.2351  decode.d1.loss_dice: 0.3465  decode.d2.loss_cls: 0.3000  decode.d2.loss_mask: 0.2390  decode.d2.loss_dice: 0.3430  decode.d3.loss_cls: 0.1860  decode.d3.loss_mask: 0.2584  decode.d3.loss_dice: 0.3521  decode.d4.loss_cls: 0.2854  decode.d4.loss_mask: 0.2406  decode.d4.loss_dice: 0.3466  decode.d5.loss_cls: 0.2806  decode.d5.loss_mask: 0.2542  decode.d5.loss_dice: 0.3624  decode.d6.loss_cls: 0.2290  decode.d6.loss_mask: 0.2319  decode.d6.loss_dice: 0.3347  decode.d7.loss_cls: 0.2369  decode.d7.loss_mask: 0.2285  decode.d7.loss_dice: 0.3214  decode.d8.loss_cls: 0.3138  decode.d8.loss_mask: 0.2265  decode.d8.loss_dice: 0.3088
08/06 03:27:01 - mmengine - INFO - Iter(train) [  9650/320000]  base_lr: 9.7282e-05 lr: 9.7282e-06  eta: 1 day, 13:39:14  time: 0.4366  data_time: 0.0092  memory: 5275  grad_norm: 199.9337  loss: 12.0093  decode.loss_cls: 0.3838  decode.loss_mask: 0.3388  decode.loss_dice: 0.3517  decode.d0.loss_cls: 1.3622  decode.d0.loss_mask: 0.3605  decode.d0.loss_dice: 0.3824  decode.d1.loss_cls: 0.5353  decode.d1.loss_mask: 0.3538  decode.d1.loss_dice: 0.3456  decode.d2.loss_cls: 0.3999  decode.d2.loss_mask: 0.3482  decode.d2.loss_dice: 0.3627  decode.d3.loss_cls: 0.3757  decode.d3.loss_mask: 0.3428  decode.d3.loss_dice: 0.3543  decode.d4.loss_cls: 0.3485  decode.d4.loss_mask: 0.3409  decode.d4.loss_dice: 0.3521  decode.d5.loss_cls: 0.3924  decode.d5.loss_mask: 0.3393  decode.d5.loss_dice: 0.3435  decode.d6.loss_cls: 0.4208  decode.d6.loss_mask: 0.3346  decode.d6.loss_dice: 0.3449  decode.d7.loss_cls: 0.4378  decode.d7.loss_mask: 0.3339  decode.d7.loss_dice: 0.3458  decode.d8.loss_cls: 0.4005  decode.d8.loss_mask: 0.3337  decode.d8.loss_dice: 0.3428
08/06 03:27:23 - mmengine - INFO - Iter(train) [  9700/320000]  base_lr: 9.7268e-05 lr: 9.7268e-06  eta: 1 day, 13:38:50  time: 0.4348  data_time: 0.0090  memory: 5242  grad_norm: 88.5768  loss: 9.4974  decode.loss_cls: 0.1872  decode.loss_mask: 0.3404  decode.loss_dice: 0.3245  decode.d0.loss_cls: 1.0634  decode.d0.loss_mask: 0.3605  decode.d0.loss_dice: 0.3548  decode.d1.loss_cls: 0.2719  decode.d1.loss_mask: 0.3420  decode.d1.loss_dice: 0.3169  decode.d2.loss_cls: 0.1822  decode.d2.loss_mask: 0.3423  decode.d2.loss_dice: 0.3413  decode.d3.loss_cls: 0.1663  decode.d3.loss_mask: 0.3396  decode.d3.loss_dice: 0.3143  decode.d4.loss_cls: 0.1729  decode.d4.loss_mask: 0.3410  decode.d4.loss_dice: 0.3186  decode.d5.loss_cls: 0.1945  decode.d5.loss_mask: 0.3418  decode.d5.loss_dice: 0.3184  decode.d6.loss_cls: 0.1947  decode.d6.loss_mask: 0.3483  decode.d6.loss_dice: 0.2998  decode.d7.loss_cls: 0.2030  decode.d7.loss_mask: 0.3434  decode.d7.loss_dice: 0.3075  decode.d8.loss_cls: 0.2099  decode.d8.loss_mask: 0.3416  decode.d8.loss_dice: 0.3145
08/06 03:27:44 - mmengine - INFO - Iter(train) [  9750/320000]  base_lr: 9.7254e-05 lr: 9.7254e-06  eta: 1 day, 13:38:26  time: 0.4351  data_time: 0.0090  memory: 5275  grad_norm: 144.4121  loss: 11.2860  decode.loss_cls: 0.4764  decode.loss_mask: 0.3253  decode.loss_dice: 0.3365  decode.d0.loss_cls: 1.1988  decode.d0.loss_mask: 0.3390  decode.d0.loss_dice: 0.3503  decode.d1.loss_cls: 0.4877  decode.d1.loss_mask: 0.3132  decode.d1.loss_dice: 0.3297  decode.d2.loss_cls: 0.3838  decode.d2.loss_mask: 0.3187  decode.d2.loss_dice: 0.3244  decode.d3.loss_cls: 0.3596  decode.d3.loss_mask: 0.3172  decode.d3.loss_dice: 0.3200  decode.d4.loss_cls: 0.3555  decode.d4.loss_mask: 0.3158  decode.d4.loss_dice: 0.3207  decode.d5.loss_cls: 0.3314  decode.d5.loss_mask: 0.3159  decode.d5.loss_dice: 0.3282  decode.d6.loss_cls: 0.3964  decode.d6.loss_mask: 0.3157  decode.d6.loss_dice: 0.3068  decode.d7.loss_cls: 0.3621  decode.d7.loss_mask: 0.3235  decode.d7.loss_dice: 0.3421  decode.d8.loss_cls: 0.4523  decode.d8.loss_mask: 0.3156  decode.d8.loss_dice: 0.3236
08/06 03:28:06 - mmengine - INFO - Iter(train) [  9800/320000]  base_lr: 9.7240e-05 lr: 9.7240e-06  eta: 1 day, 13:38:02  time: 0.4357  data_time: 0.0090  memory: 5260  grad_norm: 107.0648  loss: 8.6092  decode.loss_cls: 0.1768  decode.loss_mask: 0.2763  decode.loss_dice: 0.2704  decode.d0.loss_cls: 1.0137  decode.d0.loss_mask: 0.2906  decode.d0.loss_dice: 0.2872  decode.d1.loss_cls: 0.3486  decode.d1.loss_mask: 0.2794  decode.d1.loss_dice: 0.2574  decode.d2.loss_cls: 0.2473  decode.d2.loss_mask: 0.2745  decode.d2.loss_dice: 0.2663  decode.d3.loss_cls: 0.1927  decode.d3.loss_mask: 0.2761  decode.d3.loss_dice: 0.2757  decode.d4.loss_cls: 0.2553  decode.d4.loss_mask: 0.2824  decode.d4.loss_dice: 0.2818  decode.d5.loss_cls: 0.2270  decode.d5.loss_mask: 0.2775  decode.d5.loss_dice: 0.2806  decode.d6.loss_cls: 0.2107  decode.d6.loss_mask: 0.2840  decode.d6.loss_dice: 0.2794  decode.d7.loss_cls: 0.2046  decode.d7.loss_mask: 0.2779  decode.d7.loss_dice: 0.2727  decode.d8.loss_cls: 0.1824  decode.d8.loss_mask: 0.2811  decode.d8.loss_dice: 0.2786
08/06 03:28:28 - mmengine - INFO - Iter(train) [  9850/320000]  base_lr: 9.7226e-05 lr: 9.7226e-06  eta: 1 day, 13:37:38  time: 0.4348  data_time: 0.0090  memory: 5242  grad_norm: 174.8430  loss: 11.5459  decode.loss_cls: 0.3075  decode.loss_mask: 0.3573  decode.loss_dice: 0.3194  decode.d0.loss_cls: 1.0508  decode.d0.loss_mask: 0.3778  decode.d0.loss_dice: 0.3571  decode.d1.loss_cls: 0.3884  decode.d1.loss_mask: 0.3567  decode.d1.loss_dice: 0.3309  decode.d2.loss_cls: 0.4783  decode.d2.loss_mask: 0.3717  decode.d2.loss_dice: 0.3040  decode.d3.loss_cls: 0.5181  decode.d3.loss_mask: 0.3322  decode.d3.loss_dice: 0.3100  decode.d4.loss_cls: 0.4472  decode.d4.loss_mask: 0.3272  decode.d4.loss_dice: 0.3073  decode.d5.loss_cls: 0.4384  decode.d5.loss_mask: 0.3431  decode.d5.loss_dice: 0.3202  decode.d6.loss_cls: 0.4014  decode.d6.loss_mask: 0.3687  decode.d6.loss_dice: 0.3488  decode.d7.loss_cls: 0.3784  decode.d7.loss_mask: 0.3640  decode.d7.loss_dice: 0.3475  decode.d8.loss_cls: 0.3221  decode.d8.loss_mask: 0.3652  decode.d8.loss_dice: 0.3063
08/06 03:28:50 - mmengine - INFO - Iter(train) [  9900/320000]  base_lr: 9.7212e-05 lr: 9.7212e-06  eta: 1 day, 13:37:15  time: 0.4362  data_time: 0.0091  memory: 5260  grad_norm: 78.4256  loss: 9.1743  decode.loss_cls: 0.2829  decode.loss_mask: 0.2281  decode.loss_dice: 0.2998  decode.d0.loss_cls: 1.1139  decode.d0.loss_mask: 0.2312  decode.d0.loss_dice: 0.3229  decode.d1.loss_cls: 0.3983  decode.d1.loss_mask: 0.2195  decode.d1.loss_dice: 0.2901  decode.d2.loss_cls: 0.3706  decode.d2.loss_mask: 0.2120  decode.d2.loss_dice: 0.3089  decode.d3.loss_cls: 0.3464  decode.d3.loss_mask: 0.2177  decode.d3.loss_dice: 0.3020  decode.d4.loss_cls: 0.2680  decode.d4.loss_mask: 0.2214  decode.d4.loss_dice: 0.2972  decode.d5.loss_cls: 0.3116  decode.d5.loss_mask: 0.2238  decode.d5.loss_dice: 0.3055  decode.d6.loss_cls: 0.2264  decode.d6.loss_mask: 0.2218  decode.d6.loss_dice: 0.3027  decode.d7.loss_cls: 0.3266  decode.d7.loss_mask: 0.2186  decode.d7.loss_dice: 0.3038  decode.d8.loss_cls: 0.2971  decode.d8.loss_mask: 0.2173  decode.d8.loss_dice: 0.2881
08/06 03:29:12 - mmengine - INFO - Iter(train) [  9950/320000]  base_lr: 9.7197e-05 lr: 9.7197e-06  eta: 1 day, 13:36:52  time: 0.4362  data_time: 0.0091  memory: 5275  grad_norm: 72.4443  loss: 9.6542  decode.loss_cls: 0.3601  decode.loss_mask: 0.2514  decode.loss_dice: 0.2888  decode.d0.loss_cls: 1.0875  decode.d0.loss_mask: 0.2555  decode.d0.loss_dice: 0.3439  decode.d1.loss_cls: 0.3944  decode.d1.loss_mask: 0.2479  decode.d1.loss_dice: 0.2935  decode.d2.loss_cls: 0.3114  decode.d2.loss_mask: 0.2430  decode.d2.loss_dice: 0.2816  decode.d3.loss_cls: 0.3329  decode.d3.loss_mask: 0.2403  decode.d3.loss_dice: 0.2993  decode.d4.loss_cls: 0.3438  decode.d4.loss_mask: 0.2442  decode.d4.loss_dice: 0.2819  decode.d5.loss_cls: 0.3575  decode.d5.loss_mask: 0.2501  decode.d5.loss_dice: 0.2974  decode.d6.loss_cls: 0.3615  decode.d6.loss_mask: 0.2432  decode.d6.loss_dice: 0.2871  decode.d7.loss_cls: 0.3654  decode.d7.loss_mask: 0.2469  decode.d7.loss_dice: 0.2961  decode.d8.loss_cls: 0.3066  decode.d8.loss_mask: 0.2489  decode.d8.loss_dice: 0.2920
08/06 03:29:33 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 03:29:33 - mmengine - INFO - Iter(train) [ 10000/320000]  base_lr: 9.7183e-05 lr: 9.7183e-06  eta: 1 day, 13:36:28  time: 0.4362  data_time: 0.0091  memory: 5240  grad_norm: 365.0723  loss: 14.0488  decode.loss_cls: 0.5808  decode.loss_mask: 0.3448  decode.loss_dice: 0.4261  decode.d0.loss_cls: 1.2822  decode.d0.loss_mask: 0.3709  decode.d0.loss_dice: 0.4768  decode.d1.loss_cls: 0.6388  decode.d1.loss_mask: 0.3336  decode.d1.loss_dice: 0.3885  decode.d2.loss_cls: 0.5800  decode.d2.loss_mask: 0.3397  decode.d2.loss_dice: 0.4332  decode.d3.loss_cls: 0.6162  decode.d3.loss_mask: 0.3391  decode.d3.loss_dice: 0.4245  decode.d4.loss_cls: 0.5615  decode.d4.loss_mask: 0.3453  decode.d4.loss_dice: 0.4119  decode.d5.loss_cls: 0.5056  decode.d5.loss_mask: 0.3491  decode.d5.loss_dice: 0.4267  decode.d6.loss_cls: 0.5262  decode.d6.loss_mask: 0.3471  decode.d6.loss_dice: 0.4218  decode.d7.loss_cls: 0.5300  decode.d7.loss_mask: 0.3335  decode.d7.loss_dice: 0.4046  decode.d8.loss_cls: 0.5747  decode.d8.loss_mask: 0.3415  decode.d8.loss_dice: 0.3942
08/06 03:29:55 - mmengine - INFO - Iter(train) [ 10050/320000]  base_lr: 9.7169e-05 lr: 9.7169e-06  eta: 1 day, 13:36:05  time: 0.4356  data_time: 0.0092  memory: 5224  grad_norm: 164.6039  loss: 12.7386  decode.loss_cls: 0.4515  decode.loss_mask: 0.3747  decode.loss_dice: 0.3551  decode.d0.loss_cls: 1.3935  decode.d0.loss_mask: 0.3366  decode.d0.loss_dice: 0.4339  decode.d1.loss_cls: 0.6836  decode.d1.loss_mask: 0.2886  decode.d1.loss_dice: 0.3739  decode.d2.loss_cls: 0.4942  decode.d2.loss_mask: 0.2880  decode.d2.loss_dice: 0.3967  decode.d3.loss_cls: 0.5634  decode.d3.loss_mask: 0.2848  decode.d3.loss_dice: 0.3583  decode.d4.loss_cls: 0.4949  decode.d4.loss_mask: 0.3119  decode.d4.loss_dice: 0.3701  decode.d5.loss_cls: 0.4774  decode.d5.loss_mask: 0.3073  decode.d5.loss_dice: 0.3343  decode.d6.loss_cls: 0.4664  decode.d6.loss_mask: 0.2923  decode.d6.loss_dice: 0.3346  decode.d7.loss_cls: 0.4793  decode.d7.loss_mask: 0.3035  decode.d7.loss_dice: 0.3591  decode.d8.loss_cls: 0.4789  decode.d8.loss_mask: 0.3050  decode.d8.loss_dice: 0.3468
08/06 03:30:17 - mmengine - INFO - Iter(train) [ 10100/320000]  base_lr: 9.7155e-05 lr: 9.7155e-06  eta: 1 day, 13:35:42  time: 0.4350  data_time: 0.0087  memory: 5275  grad_norm: 95.9924  loss: 9.5640  decode.loss_cls: 0.2235  decode.loss_mask: 0.2561  decode.loss_dice: 0.3255  decode.d0.loss_cls: 1.0604  decode.d0.loss_mask: 0.2651  decode.d0.loss_dice: 0.3635  decode.d1.loss_cls: 0.3288  decode.d1.loss_mask: 0.2463  decode.d1.loss_dice: 0.3539  decode.d2.loss_cls: 0.3349  decode.d2.loss_mask: 0.2502  decode.d2.loss_dice: 0.3264  decode.d3.loss_cls: 0.3097  decode.d3.loss_mask: 0.2564  decode.d3.loss_dice: 0.3225  decode.d4.loss_cls: 0.2707  decode.d4.loss_mask: 0.2487  decode.d4.loss_dice: 0.3255  decode.d5.loss_cls: 0.2818  decode.d5.loss_mask: 0.2523  decode.d5.loss_dice: 0.3382  decode.d6.loss_cls: 0.2888  decode.d6.loss_mask: 0.2524  decode.d6.loss_dice: 0.3228  decode.d7.loss_cls: 0.2684  decode.d7.loss_mask: 0.2581  decode.d7.loss_dice: 0.3148  decode.d8.loss_cls: 0.3441  decode.d8.loss_mask: 0.2550  decode.d8.loss_dice: 0.3193
08/06 03:30:39 - mmengine - INFO - Iter(train) [ 10150/320000]  base_lr: 9.7141e-05 lr: 9.7141e-06  eta: 1 day, 13:35:18  time: 0.4357  data_time: 0.0090  memory: 5242  grad_norm: 134.8421  loss: 13.3307  decode.loss_cls: 0.5284  decode.loss_mask: 0.3582  decode.loss_dice: 0.3959  decode.d0.loss_cls: 1.2407  decode.d0.loss_mask: 0.3717  decode.d0.loss_dice: 0.4074  decode.d1.loss_cls: 0.5777  decode.d1.loss_mask: 0.3628  decode.d1.loss_dice: 0.3377  decode.d2.loss_cls: 0.4629  decode.d2.loss_mask: 0.3698  decode.d2.loss_dice: 0.3949  decode.d3.loss_cls: 0.4867  decode.d3.loss_mask: 0.3563  decode.d3.loss_dice: 0.3808  decode.d4.loss_cls: 0.5012  decode.d4.loss_mask: 0.3563  decode.d4.loss_dice: 0.3914  decode.d5.loss_cls: 0.4967  decode.d5.loss_mask: 0.3574  decode.d5.loss_dice: 0.3709  decode.d6.loss_cls: 0.4688  decode.d6.loss_mask: 0.3617  decode.d6.loss_dice: 0.4067  decode.d7.loss_cls: 0.5653  decode.d7.loss_mask: 0.3603  decode.d7.loss_dice: 0.3896  decode.d8.loss_cls: 0.5264  decode.d8.loss_mask: 0.3560  decode.d8.loss_dice: 0.3901
08/06 03:31:01 - mmengine - INFO - Iter(train) [ 10200/320000]  base_lr: 9.7127e-05 lr: 9.7127e-06  eta: 1 day, 13:34:56  time: 0.4361  data_time: 0.0091  memory: 5242  grad_norm: 119.5718  loss: 9.9710  decode.loss_cls: 0.2231  decode.loss_mask: 0.3546  decode.loss_dice: 0.3072  decode.d0.loss_cls: 0.9830  decode.d0.loss_mask: 0.3775  decode.d0.loss_dice: 0.3485  decode.d1.loss_cls: 0.3751  decode.d1.loss_mask: 0.3192  decode.d1.loss_dice: 0.2906  decode.d2.loss_cls: 0.2528  decode.d2.loss_mask: 0.3381  decode.d2.loss_dice: 0.2868  decode.d3.loss_cls: 0.2757  decode.d3.loss_mask: 0.3115  decode.d3.loss_dice: 0.2813  decode.d4.loss_cls: 0.2515  decode.d4.loss_mask: 0.3847  decode.d4.loss_dice: 0.3467  decode.d5.loss_cls: 0.2699  decode.d5.loss_mask: 0.3642  decode.d5.loss_dice: 0.3159  decode.d6.loss_cls: 0.2040  decode.d6.loss_mask: 0.3717  decode.d6.loss_dice: 0.3132  decode.d7.loss_cls: 0.2372  decode.d7.loss_mask: 0.3677  decode.d7.loss_dice: 0.3110  decode.d8.loss_cls: 0.2256  decode.d8.loss_mask: 0.3643  decode.d8.loss_dice: 0.3185
08/06 03:31:22 - mmengine - INFO - Iter(train) [ 10250/320000]  base_lr: 9.7113e-05 lr: 9.7113e-06  eta: 1 day, 13:34:35  time: 0.4385  data_time: 0.0092  memory: 5275  grad_norm: 105.8315  loss: 8.7417  decode.loss_cls: 0.1793  decode.loss_mask: 0.2222  decode.loss_dice: 0.2546  decode.d0.loss_cls: 1.1929  decode.d0.loss_mask: 0.2370  decode.d0.loss_dice: 0.3011  decode.d1.loss_cls: 0.3129  decode.d1.loss_mask: 0.2233  decode.d1.loss_dice: 0.2713  decode.d2.loss_cls: 0.3461  decode.d2.loss_mask: 0.2182  decode.d2.loss_dice: 0.2706  decode.d3.loss_cls: 0.2873  decode.d3.loss_mask: 0.2226  decode.d3.loss_dice: 0.2773  decode.d4.loss_cls: 0.3077  decode.d4.loss_mask: 0.2199  decode.d4.loss_dice: 0.2652  decode.d5.loss_cls: 0.3161  decode.d5.loss_mask: 0.2167  decode.d5.loss_dice: 0.2490  decode.d6.loss_cls: 0.3183  decode.d6.loss_mask: 0.2167  decode.d6.loss_dice: 0.2631  decode.d7.loss_cls: 0.3370  decode.d7.loss_mask: 0.2206  decode.d7.loss_dice: 0.2520  decode.d8.loss_cls: 0.2590  decode.d8.loss_mask: 0.2190  decode.d8.loss_dice: 0.2647
08/06 03:31:44 - mmengine - INFO - Iter(train) [ 10300/320000]  base_lr: 9.7099e-05 lr: 9.7099e-06  eta: 1 day, 13:34:14  time: 0.4364  data_time: 0.0091  memory: 5242  grad_norm: 98.2073  loss: 11.3498  decode.loss_cls: 0.3759  decode.loss_mask: 0.2937  decode.loss_dice: 0.3397  decode.d0.loss_cls: 1.2083  decode.d0.loss_mask: 0.3306  decode.d0.loss_dice: 0.4496  decode.d1.loss_cls: 0.4519  decode.d1.loss_mask: 0.3063  decode.d1.loss_dice: 0.3682  decode.d2.loss_cls: 0.4007  decode.d2.loss_mask: 0.2890  decode.d2.loss_dice: 0.3670  decode.d3.loss_cls: 0.3844  decode.d3.loss_mask: 0.2929  decode.d3.loss_dice: 0.3498  decode.d4.loss_cls: 0.3663  decode.d4.loss_mask: 0.2987  decode.d4.loss_dice: 0.3502  decode.d5.loss_cls: 0.4081  decode.d5.loss_mask: 0.2914  decode.d5.loss_dice: 0.3463  decode.d6.loss_cls: 0.3892  decode.d6.loss_mask: 0.2980  decode.d6.loss_dice: 0.3629  decode.d7.loss_cls: 0.3558  decode.d7.loss_mask: 0.2974  decode.d7.loss_dice: 0.3618  decode.d8.loss_cls: 0.3592  decode.d8.loss_mask: 0.2984  decode.d8.loss_dice: 0.3581
08/06 03:32:06 - mmengine - INFO - Iter(train) [ 10350/320000]  base_lr: 9.7085e-05 lr: 9.7085e-06  eta: 1 day, 13:33:53  time: 0.4369  data_time: 0.0091  memory: 5260  grad_norm: 237.4721  loss: 10.9604  decode.loss_cls: 0.3994  decode.loss_mask: 0.3119  decode.loss_dice: 0.2996  decode.d0.loss_cls: 1.0478  decode.d0.loss_mask: 0.3369  decode.d0.loss_dice: 0.3269  decode.d1.loss_cls: 0.4408  decode.d1.loss_mask: 0.3095  decode.d1.loss_dice: 0.3257  decode.d2.loss_cls: 0.4095  decode.d2.loss_mask: 0.3049  decode.d2.loss_dice: 0.3032  decode.d3.loss_cls: 0.3824  decode.d3.loss_mask: 0.3073  decode.d3.loss_dice: 0.2895  decode.d4.loss_cls: 0.3921  decode.d4.loss_mask: 0.3029  decode.d4.loss_dice: 0.2896  decode.d5.loss_cls: 0.3989  decode.d5.loss_mask: 0.3199  decode.d5.loss_dice: 0.3094  decode.d6.loss_cls: 0.3968  decode.d6.loss_mask: 0.3069  decode.d6.loss_dice: 0.2884  decode.d7.loss_cls: 0.4311  decode.d7.loss_mask: 0.3214  decode.d7.loss_dice: 0.3282  decode.d8.loss_cls: 0.4543  decode.d8.loss_mask: 0.3128  decode.d8.loss_dice: 0.3125
08/06 03:32:28 - mmengine - INFO - Iter(train) [ 10400/320000]  base_lr: 9.7070e-05 lr: 9.7070e-06  eta: 1 day, 13:33:32  time: 0.4377  data_time: 0.0090  memory: 5242  grad_norm: 255.9718  loss: 10.6540  decode.loss_cls: 0.1993  decode.loss_mask: 0.3893  decode.loss_dice: 0.3407  decode.d0.loss_cls: 1.2241  decode.d0.loss_mask: 0.3945  decode.d0.loss_dice: 0.3591  decode.d1.loss_cls: 0.3468  decode.d1.loss_mask: 0.3711  decode.d1.loss_dice: 0.3242  decode.d2.loss_cls: 0.3421  decode.d2.loss_mask: 0.3678  decode.d2.loss_dice: 0.3253  decode.d3.loss_cls: 0.2380  decode.d3.loss_mask: 0.3867  decode.d3.loss_dice: 0.3341  decode.d4.loss_cls: 0.2458  decode.d4.loss_mask: 0.3955  decode.d4.loss_dice: 0.3409  decode.d5.loss_cls: 0.2194  decode.d5.loss_mask: 0.3961  decode.d5.loss_dice: 0.3407  decode.d6.loss_cls: 0.2170  decode.d6.loss_mask: 0.3855  decode.d6.loss_dice: 0.3467  decode.d7.loss_cls: 0.2067  decode.d7.loss_mask: 0.3720  decode.d7.loss_dice: 0.3332  decode.d8.loss_cls: 0.2069  decode.d8.loss_mask: 0.3690  decode.d8.loss_dice: 0.3357
08/06 03:32:50 - mmengine - INFO - Iter(train) [ 10450/320000]  base_lr: 9.7056e-05 lr: 9.7056e-06  eta: 1 day, 13:33:11  time: 0.4384  data_time: 0.0091  memory: 5242  grad_norm: 77.9317  loss: 8.8866  decode.loss_cls: 0.1823  decode.loss_mask: 0.2940  decode.loss_dice: 0.2677  decode.d0.loss_cls: 1.2155  decode.d0.loss_mask: 0.3219  decode.d0.loss_dice: 0.2936  decode.d1.loss_cls: 0.2460  decode.d1.loss_mask: 0.2876  decode.d1.loss_dice: 0.2779  decode.d2.loss_cls: 0.2110  decode.d2.loss_mask: 0.2869  decode.d2.loss_dice: 0.2725  decode.d3.loss_cls: 0.2621  decode.d3.loss_mask: 0.2907  decode.d3.loss_dice: 0.2702  decode.d4.loss_cls: 0.2165  decode.d4.loss_mask: 0.2911  decode.d4.loss_dice: 0.2701  decode.d5.loss_cls: 0.2329  decode.d5.loss_mask: 0.2985  decode.d5.loss_dice: 0.2743  decode.d6.loss_cls: 0.1986  decode.d6.loss_mask: 0.3093  decode.d6.loss_dice: 0.2672  decode.d7.loss_cls: 0.1941  decode.d7.loss_mask: 0.3118  decode.d7.loss_dice: 0.2730  decode.d8.loss_cls: 0.1929  decode.d8.loss_mask: 0.3103  decode.d8.loss_dice: 0.2659
08/06 03:33:12 - mmengine - INFO - Iter(train) [ 10500/320000]  base_lr: 9.7042e-05 lr: 9.7042e-06  eta: 1 day, 13:32:49  time: 0.4357  data_time: 0.0088  memory: 5260  grad_norm: 151.2362  loss: 11.3124  decode.loss_cls: 0.3454  decode.loss_mask: 0.3194  decode.loss_dice: 0.3685  decode.d0.loss_cls: 1.1565  decode.d0.loss_mask: 0.2916  decode.d0.loss_dice: 0.4005  decode.d1.loss_cls: 0.4230  decode.d1.loss_mask: 0.3074  decode.d1.loss_dice: 0.3730  decode.d2.loss_cls: 0.5050  decode.d2.loss_mask: 0.3242  decode.d2.loss_dice: 0.3372  decode.d3.loss_cls: 0.3932  decode.d3.loss_mask: 0.2964  decode.d3.loss_dice: 0.3545  decode.d4.loss_cls: 0.4117  decode.d4.loss_mask: 0.3217  decode.d4.loss_dice: 0.3792  decode.d5.loss_cls: 0.3395  decode.d5.loss_mask: 0.3247  decode.d5.loss_dice: 0.3784  decode.d6.loss_cls: 0.2691  decode.d6.loss_mask: 0.3143  decode.d6.loss_dice: 0.3503  decode.d7.loss_cls: 0.3507  decode.d7.loss_mask: 0.3100  decode.d7.loss_dice: 0.3618  decode.d8.loss_cls: 0.3422  decode.d8.loss_mask: 0.2979  decode.d8.loss_dice: 0.3651
08/06 03:33:34 - mmengine - INFO - Iter(train) [ 10550/320000]  base_lr: 9.7028e-05 lr: 9.7028e-06  eta: 1 day, 13:32:28  time: 0.4373  data_time: 0.0091  memory: 5260  grad_norm: 144.8507  loss: 12.4807  decode.loss_cls: 0.3730  decode.loss_mask: 0.3073  decode.loss_dice: 0.3850  decode.d0.loss_cls: 1.3316  decode.d0.loss_mask: 0.3121  decode.d0.loss_dice: 0.4429  decode.d1.loss_cls: 0.4858  decode.d1.loss_mask: 0.3069  decode.d1.loss_dice: 0.3938  decode.d2.loss_cls: 0.4956  decode.d2.loss_mask: 0.3190  decode.d2.loss_dice: 0.4039  decode.d3.loss_cls: 0.5316  decode.d3.loss_mask: 0.3101  decode.d3.loss_dice: 0.3938  decode.d4.loss_cls: 0.4085  decode.d4.loss_mask: 0.3463  decode.d4.loss_dice: 0.4201  decode.d5.loss_cls: 0.3890  decode.d5.loss_mask: 0.3626  decode.d5.loss_dice: 0.4264  decode.d6.loss_cls: 0.3483  decode.d6.loss_mask: 0.3146  decode.d6.loss_dice: 0.4025  decode.d7.loss_cls: 0.3782  decode.d7.loss_mask: 0.3653  decode.d7.loss_dice: 0.4235  decode.d8.loss_cls: 0.4069  decode.d8.loss_mask: 0.3056  decode.d8.loss_dice: 0.3903
08/06 03:33:55 - mmengine - INFO - Iter(train) [ 10600/320000]  base_lr: 9.7014e-05 lr: 9.7014e-06  eta: 1 day, 13:32:06  time: 0.4368  data_time: 0.0090  memory: 5224  grad_norm: 104.2032  loss: 9.3260  decode.loss_cls: 0.2341  decode.loss_mask: 0.2829  decode.loss_dice: 0.2852  decode.d0.loss_cls: 0.8656  decode.d0.loss_mask: 0.3132  decode.d0.loss_dice: 0.3192  decode.d1.loss_cls: 0.3641  decode.d1.loss_mask: 0.2826  decode.d1.loss_dice: 0.2957  decode.d2.loss_cls: 0.3916  decode.d2.loss_mask: 0.2823  decode.d2.loss_dice: 0.2725  decode.d3.loss_cls: 0.3202  decode.d3.loss_mask: 0.2809  decode.d3.loss_dice: 0.2724  decode.d4.loss_cls: 0.3134  decode.d4.loss_mask: 0.2855  decode.d4.loss_dice: 0.2792  decode.d5.loss_cls: 0.2829  decode.d5.loss_mask: 0.2868  decode.d5.loss_dice: 0.3032  decode.d6.loss_cls: 0.2429  decode.d6.loss_mask: 0.2898  decode.d6.loss_dice: 0.3069  decode.d7.loss_cls: 0.2724  decode.d7.loss_mask: 0.2889  decode.d7.loss_dice: 0.3054  decode.d8.loss_cls: 0.2148  decode.d8.loss_mask: 0.2846  decode.d8.loss_dice: 0.3066
08/06 03:34:17 - mmengine - INFO - Iter(train) [ 10650/320000]  base_lr: 9.7000e-05 lr: 9.7000e-06  eta: 1 day, 13:31:44  time: 0.4363  data_time: 0.0087  memory: 5258  grad_norm: 106.5251  loss: 9.1935  decode.loss_cls: 0.2193  decode.loss_mask: 0.2763  decode.loss_dice: 0.3380  decode.d0.loss_cls: 1.1734  decode.d0.loss_mask: 0.2730  decode.d0.loss_dice: 0.3404  decode.d1.loss_cls: 0.2811  decode.d1.loss_mask: 0.2567  decode.d1.loss_dice: 0.3132  decode.d2.loss_cls: 0.2548  decode.d2.loss_mask: 0.2535  decode.d2.loss_dice: 0.3350  decode.d3.loss_cls: 0.2368  decode.d3.loss_mask: 0.2533  decode.d3.loss_dice: 0.3177  decode.d4.loss_cls: 0.2428  decode.d4.loss_mask: 0.2529  decode.d4.loss_dice: 0.3168  decode.d5.loss_cls: 0.2730  decode.d5.loss_mask: 0.2558  decode.d5.loss_dice: 0.3144  decode.d6.loss_cls: 0.2546  decode.d6.loss_mask: 0.2562  decode.d6.loss_dice: 0.3149  decode.d7.loss_cls: 0.2188  decode.d7.loss_mask: 0.2608  decode.d7.loss_dice: 0.3273  decode.d8.loss_cls: 0.1651  decode.d8.loss_mask: 0.2697  decode.d8.loss_dice: 0.3479
08/06 03:34:39 - mmengine - INFO - Iter(train) [ 10700/320000]  base_lr: 9.6986e-05 lr: 9.6986e-06  eta: 1 day, 13:31:22  time: 0.4357  data_time: 0.0090  memory: 5260  grad_norm: 197.0358  loss: 10.7507  decode.loss_cls: 0.3228  decode.loss_mask: 0.3350  decode.loss_dice: 0.3597  decode.d0.loss_cls: 0.9286  decode.d0.loss_mask: 0.3856  decode.d0.loss_dice: 0.3823  decode.d1.loss_cls: 0.4744  decode.d1.loss_mask: 0.3165  decode.d1.loss_dice: 0.3602  decode.d2.loss_cls: 0.3350  decode.d2.loss_mask: 0.3192  decode.d2.loss_dice: 0.3482  decode.d3.loss_cls: 0.3420  decode.d3.loss_mask: 0.3088  decode.d3.loss_dice: 0.3367  decode.d4.loss_cls: 0.2834  decode.d4.loss_mask: 0.3213  decode.d4.loss_dice: 0.3444  decode.d5.loss_cls: 0.3444  decode.d5.loss_mask: 0.3107  decode.d5.loss_dice: 0.3532  decode.d6.loss_cls: 0.2856  decode.d6.loss_mask: 0.3292  decode.d6.loss_dice: 0.3689  decode.d7.loss_cls: 0.2857  decode.d7.loss_mask: 0.3225  decode.d7.loss_dice: 0.3584  decode.d8.loss_cls: 0.3258  decode.d8.loss_mask: 0.3243  decode.d8.loss_dice: 0.3378
08/06 03:35:01 - mmengine - INFO - Iter(train) [ 10750/320000]  base_lr: 9.6972e-05 lr: 9.6972e-06  eta: 1 day, 13:31:00  time: 0.4367  data_time: 0.0089  memory: 5299  grad_norm: 91.2700  loss: 8.9913  decode.loss_cls: 0.2468  decode.loss_mask: 0.2676  decode.loss_dice: 0.3025  decode.d0.loss_cls: 0.9623  decode.d0.loss_mask: 0.2607  decode.d0.loss_dice: 0.3081  decode.d1.loss_cls: 0.3301  decode.d1.loss_mask: 0.2607  decode.d1.loss_dice: 0.2732  decode.d2.loss_cls: 0.2709  decode.d2.loss_mask: 0.2608  decode.d2.loss_dice: 0.2812  decode.d3.loss_cls: 0.2605  decode.d3.loss_mask: 0.2589  decode.d3.loss_dice: 0.2735  decode.d4.loss_cls: 0.2456  decode.d4.loss_mask: 0.2618  decode.d4.loss_dice: 0.2950  decode.d5.loss_cls: 0.3051  decode.d5.loss_mask: 0.2608  decode.d5.loss_dice: 0.2896  decode.d6.loss_cls: 0.3214  decode.d6.loss_mask: 0.2552  decode.d6.loss_dice: 0.2811  decode.d7.loss_cls: 0.2717  decode.d7.loss_mask: 0.2564  decode.d7.loss_dice: 0.2980  decode.d8.loss_cls: 0.2800  decode.d8.loss_mask: 0.2583  decode.d8.loss_dice: 0.2935
08/06 03:35:23 - mmengine - INFO - Iter(train) [ 10800/320000]  base_lr: 9.6958e-05 lr: 9.6958e-06  eta: 1 day, 13:30:38  time: 0.4371  data_time: 0.0091  memory: 5240  grad_norm: 162.9858  loss: 10.8513  decode.loss_cls: 0.3979  decode.loss_mask: 0.2720  decode.loss_dice: 0.3182  decode.d0.loss_cls: 0.8540  decode.d0.loss_mask: 0.2691  decode.d0.loss_dice: 0.3112  decode.d1.loss_cls: 0.5750  decode.d1.loss_mask: 0.2759  decode.d1.loss_dice: 0.3440  decode.d2.loss_cls: 0.4125  decode.d2.loss_mask: 0.2680  decode.d2.loss_dice: 0.3251  decode.d3.loss_cls: 0.4021  decode.d3.loss_mask: 0.2747  decode.d3.loss_dice: 0.3463  decode.d4.loss_cls: 0.4312  decode.d4.loss_mask: 0.2748  decode.d4.loss_dice: 0.3268  decode.d5.loss_cls: 0.4369  decode.d5.loss_mask: 0.2785  decode.d5.loss_dice: 0.3489  decode.d6.loss_cls: 0.3934  decode.d6.loss_mask: 0.2728  decode.d6.loss_dice: 0.3458  decode.d7.loss_cls: 0.4725  decode.d7.loss_mask: 0.2675  decode.d7.loss_dice: 0.3172  decode.d8.loss_cls: 0.4697  decode.d8.loss_mask: 0.2683  decode.d8.loss_dice: 0.3012
08/06 03:35:45 - mmengine - INFO - Iter(train) [ 10850/320000]  base_lr: 9.6943e-05 lr: 9.6943e-06  eta: 1 day, 13:30:16  time: 0.4367  data_time: 0.0091  memory: 5240  grad_norm: 133.1411  loss: 11.6269  decode.loss_cls: 0.3309  decode.loss_mask: 0.3272  decode.loss_dice: 0.3548  decode.d0.loss_cls: 1.1914  decode.d0.loss_mask: 0.3263  decode.d0.loss_dice: 0.4040  decode.d1.loss_cls: 0.4773  decode.d1.loss_mask: 0.3383  decode.d1.loss_dice: 0.3459  decode.d2.loss_cls: 0.3620  decode.d2.loss_mask: 0.3254  decode.d2.loss_dice: 0.3482  decode.d3.loss_cls: 0.4071  decode.d3.loss_mask: 0.3408  decode.d3.loss_dice: 0.3349  decode.d4.loss_cls: 0.4643  decode.d4.loss_mask: 0.3040  decode.d4.loss_dice: 0.3606  decode.d5.loss_cls: 0.4265  decode.d5.loss_mask: 0.3032  decode.d5.loss_dice: 0.3465  decode.d6.loss_cls: 0.3796  decode.d6.loss_mask: 0.3229  decode.d6.loss_dice: 0.3459  decode.d7.loss_cls: 0.3762  decode.d7.loss_mask: 0.3240  decode.d7.loss_dice: 0.3544  decode.d8.loss_cls: 0.4215  decode.d8.loss_mask: 0.3228  decode.d8.loss_dice: 0.3601
08/06 03:36:06 - mmengine - INFO - Iter(train) [ 10900/320000]  base_lr: 9.6929e-05 lr: 9.6929e-06  eta: 1 day, 13:29:53  time: 0.4352  data_time: 0.0091  memory: 5240  grad_norm: 154.3116  loss: 10.4513  decode.loss_cls: 0.2106  decode.loss_mask: 0.3874  decode.loss_dice: 0.3523  decode.d0.loss_cls: 0.8776  decode.d0.loss_mask: 0.4061  decode.d0.loss_dice: 0.4030  decode.d1.loss_cls: 0.3174  decode.d1.loss_mask: 0.3624  decode.d1.loss_dice: 0.3475  decode.d2.loss_cls: 0.2837  decode.d2.loss_mask: 0.3603  decode.d2.loss_dice: 0.3331  decode.d3.loss_cls: 0.2837  decode.d3.loss_mask: 0.3409  decode.d3.loss_dice: 0.3314  decode.d4.loss_cls: 0.2818  decode.d4.loss_mask: 0.3480  decode.d4.loss_dice: 0.3312  decode.d5.loss_cls: 0.2915  decode.d5.loss_mask: 0.3538  decode.d5.loss_dice: 0.3383  decode.d6.loss_cls: 0.2739  decode.d6.loss_mask: 0.3606  decode.d6.loss_dice: 0.3423  decode.d7.loss_cls: 0.2638  decode.d7.loss_mask: 0.3607  decode.d7.loss_dice: 0.3301  decode.d8.loss_cls: 0.2361  decode.d8.loss_mask: 0.3872  decode.d8.loss_dice: 0.3548
08/06 03:36:28 - mmengine - INFO - Iter(train) [ 10950/320000]  base_lr: 9.6915e-05 lr: 9.6915e-06  eta: 1 day, 13:29:32  time: 0.4362  data_time: 0.0092  memory: 5242  grad_norm: 119.3357  loss: 8.3123  decode.loss_cls: 0.1928  decode.loss_mask: 0.2631  decode.loss_dice: 0.2574  decode.d0.loss_cls: 0.9863  decode.d0.loss_mask: 0.2700  decode.d0.loss_dice: 0.2967  decode.d1.loss_cls: 0.2494  decode.d1.loss_mask: 0.2579  decode.d1.loss_dice: 0.2697  decode.d2.loss_cls: 0.2232  decode.d2.loss_mask: 0.2577  decode.d2.loss_dice: 0.2624  decode.d3.loss_cls: 0.2455  decode.d3.loss_mask: 0.2598  decode.d3.loss_dice: 0.2599  decode.d4.loss_cls: 0.2492  decode.d4.loss_mask: 0.2598  decode.d4.loss_dice: 0.2623  decode.d5.loss_cls: 0.2332  decode.d5.loss_mask: 0.2594  decode.d5.loss_dice: 0.2598  decode.d6.loss_cls: 0.2310  decode.d6.loss_mask: 0.2610  decode.d6.loss_dice: 0.2518  decode.d7.loss_cls: 0.2291  decode.d7.loss_mask: 0.2592  decode.d7.loss_dice: 0.2542  decode.d8.loss_cls: 0.2263  decode.d8.loss_mask: 0.2625  decode.d8.loss_dice: 0.2617
08/06 03:36:50 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 03:36:50 - mmengine - INFO - Iter(train) [ 11000/320000]  base_lr: 9.6901e-05 lr: 9.6901e-06  eta: 1 day, 13:29:10  time: 0.4375  data_time: 0.0091  memory: 5260  grad_norm: 85.6165  loss: 9.6479  decode.loss_cls: 0.2872  decode.loss_mask: 0.2350  decode.loss_dice: 0.3313  decode.d0.loss_cls: 1.0277  decode.d0.loss_mask: 0.2392  decode.d0.loss_dice: 0.4003  decode.d1.loss_cls: 0.3627  decode.d1.loss_mask: 0.2235  decode.d1.loss_dice: 0.3102  decode.d2.loss_cls: 0.3668  decode.d2.loss_mask: 0.2238  decode.d2.loss_dice: 0.3232  decode.d3.loss_cls: 0.3120  decode.d3.loss_mask: 0.2273  decode.d3.loss_dice: 0.3114  decode.d4.loss_cls: 0.3303  decode.d4.loss_mask: 0.2321  decode.d4.loss_dice: 0.3447  decode.d5.loss_cls: 0.3444  decode.d5.loss_mask: 0.2274  decode.d5.loss_dice: 0.3361  decode.d6.loss_cls: 0.3143  decode.d6.loss_mask: 0.2370  decode.d6.loss_dice: 0.3481  decode.d7.loss_cls: 0.3006  decode.d7.loss_mask: 0.2396  decode.d7.loss_dice: 0.3098  decode.d8.loss_cls: 0.3020  decode.d8.loss_mask: 0.2429  decode.d8.loss_dice: 0.3571
08/06 03:37:12 - mmengine - INFO - Iter(train) [ 11050/320000]  base_lr: 9.6887e-05 lr: 9.6887e-06  eta: 1 day, 13:28:48  time: 0.4364  data_time: 0.0092  memory: 5242  grad_norm: 133.8183  loss: 11.6810  decode.loss_cls: 0.3991  decode.loss_mask: 0.3128  decode.loss_dice: 0.3890  decode.d0.loss_cls: 1.1645  decode.d0.loss_mask: 0.3365  decode.d0.loss_dice: 0.4984  decode.d1.loss_cls: 0.4084  decode.d1.loss_mask: 0.3109  decode.d1.loss_dice: 0.3609  decode.d2.loss_cls: 0.3720  decode.d2.loss_mask: 0.3127  decode.d2.loss_dice: 0.3832  decode.d3.loss_cls: 0.3624  decode.d3.loss_mask: 0.3127  decode.d3.loss_dice: 0.3887  decode.d4.loss_cls: 0.3969  decode.d4.loss_mask: 0.3132  decode.d4.loss_dice: 0.3632  decode.d5.loss_cls: 0.3925  decode.d5.loss_mask: 0.3089  decode.d5.loss_dice: 0.3762  decode.d6.loss_cls: 0.4025  decode.d6.loss_mask: 0.3156  decode.d6.loss_dice: 0.3852  decode.d7.loss_cls: 0.3576  decode.d7.loss_mask: 0.3135  decode.d7.loss_dice: 0.3758  decode.d8.loss_cls: 0.3452  decode.d8.loss_mask: 0.3136  decode.d8.loss_dice: 0.4088
08/06 03:37:34 - mmengine - INFO - Iter(train) [ 11100/320000]  base_lr: 9.6873e-05 lr: 9.6873e-06  eta: 1 day, 13:28:30  time: 0.4359  data_time: 0.0091  memory: 5242  grad_norm: 148.3720  loss: 15.0895  decode.loss_cls: 0.5115  decode.loss_mask: 0.4649  decode.loss_dice: 0.4427  decode.d0.loss_cls: 1.2289  decode.d0.loss_mask: 0.4984  decode.d0.loss_dice: 0.4615  decode.d1.loss_cls: 0.5731  decode.d1.loss_mask: 0.4793  decode.d1.loss_dice: 0.4236  decode.d2.loss_cls: 0.5789  decode.d2.loss_mask: 0.4720  decode.d2.loss_dice: 0.4413  decode.d3.loss_cls: 0.5628  decode.d3.loss_mask: 0.4709  decode.d3.loss_dice: 0.4407  decode.d4.loss_cls: 0.4975  decode.d4.loss_mask: 0.4781  decode.d4.loss_dice: 0.4550  decode.d5.loss_cls: 0.5067  decode.d5.loss_mask: 0.4745  decode.d5.loss_dice: 0.4055  decode.d6.loss_cls: 0.4707  decode.d6.loss_mask: 0.4761  decode.d6.loss_dice: 0.4405  decode.d7.loss_cls: 0.4779  decode.d7.loss_mask: 0.4801  decode.d7.loss_dice: 0.4232  decode.d8.loss_cls: 0.5287  decode.d8.loss_mask: 0.4742  decode.d8.loss_dice: 0.4504
08/06 03:37:56 - mmengine - INFO - Iter(train) [ 11150/320000]  base_lr: 9.6859e-05 lr: 9.6859e-06  eta: 1 day, 13:28:07  time: 0.4357  data_time: 0.0091  memory: 5260  grad_norm: 122.4812  loss: 10.1103  decode.loss_cls: 0.3504  decode.loss_mask: 0.2877  decode.loss_dice: 0.3031  decode.d0.loss_cls: 1.1578  decode.d0.loss_mask: 0.3000  decode.d0.loss_dice: 0.3501  decode.d1.loss_cls: 0.4412  decode.d1.loss_mask: 0.2907  decode.d1.loss_dice: 0.2997  decode.d2.loss_cls: 0.3465  decode.d2.loss_mask: 0.2919  decode.d2.loss_dice: 0.3014  decode.d3.loss_cls: 0.2557  decode.d3.loss_mask: 0.2999  decode.d3.loss_dice: 0.3272  decode.d4.loss_cls: 0.2381  decode.d4.loss_mask: 0.2846  decode.d4.loss_dice: 0.3007  decode.d5.loss_cls: 0.2666  decode.d5.loss_mask: 0.2860  decode.d5.loss_dice: 0.3360  decode.d6.loss_cls: 0.2990  decode.d6.loss_mask: 0.2884  decode.d6.loss_dice: 0.3172  decode.d7.loss_cls: 0.3061  decode.d7.loss_mask: 0.2872  decode.d7.loss_dice: 0.3294  decode.d8.loss_cls: 0.3366  decode.d8.loss_mask: 0.2900  decode.d8.loss_dice: 0.3409
08/06 03:38:17 - mmengine - INFO - Iter(train) [ 11200/320000]  base_lr: 9.6845e-05 lr: 9.6845e-06  eta: 1 day, 13:27:43  time: 0.4351  data_time: 0.0091  memory: 5258  grad_norm: 121.8281  loss: 9.8258  decode.loss_cls: 0.2644  decode.loss_mask: 0.2976  decode.loss_dice: 0.2929  decode.d0.loss_cls: 1.2008  decode.d0.loss_mask: 0.3242  decode.d0.loss_dice: 0.3448  decode.d1.loss_cls: 0.3630  decode.d1.loss_mask: 0.3177  decode.d1.loss_dice: 0.3117  decode.d2.loss_cls: 0.2687  decode.d2.loss_mask: 0.3077  decode.d2.loss_dice: 0.2934  decode.d3.loss_cls: 0.3238  decode.d3.loss_mask: 0.3088  decode.d3.loss_dice: 0.2913  decode.d4.loss_cls: 0.2800  decode.d4.loss_mask: 0.3107  decode.d4.loss_dice: 0.2876  decode.d5.loss_cls: 0.2678  decode.d5.loss_mask: 0.3117  decode.d5.loss_dice: 0.3028  decode.d6.loss_cls: 0.2709  decode.d6.loss_mask: 0.3114  decode.d6.loss_dice: 0.2878  decode.d7.loss_cls: 0.2480  decode.d7.loss_mask: 0.3140  decode.d7.loss_dice: 0.2967  decode.d8.loss_cls: 0.2267  decode.d8.loss_mask: 0.3100  decode.d8.loss_dice: 0.2888
08/06 03:38:39 - mmengine - INFO - Iter(train) [ 11250/320000]  base_lr: 9.6831e-05 lr: 9.6831e-06  eta: 1 day, 13:27:20  time: 0.4363  data_time: 0.0091  memory: 5260  grad_norm: 99.8091  loss: 9.4280  decode.loss_cls: 0.2751  decode.loss_mask: 0.2223  decode.loss_dice: 0.2997  decode.d0.loss_cls: 1.1212  decode.d0.loss_mask: 0.2391  decode.d0.loss_dice: 0.3515  decode.d1.loss_cls: 0.5260  decode.d1.loss_mask: 0.2300  decode.d1.loss_dice: 0.3052  decode.d2.loss_cls: 0.3387  decode.d2.loss_mask: 0.2276  decode.d2.loss_dice: 0.2821  decode.d3.loss_cls: 0.2881  decode.d3.loss_mask: 0.2267  decode.d3.loss_dice: 0.3040  decode.d4.loss_cls: 0.3479  decode.d4.loss_mask: 0.2260  decode.d4.loss_dice: 0.3050  decode.d5.loss_cls: 0.2786  decode.d5.loss_mask: 0.2278  decode.d5.loss_dice: 0.3140  decode.d6.loss_cls: 0.2892  decode.d6.loss_mask: 0.2301  decode.d6.loss_dice: 0.3077  decode.d7.loss_cls: 0.3009  decode.d7.loss_mask: 0.2269  decode.d7.loss_dice: 0.3161  decode.d8.loss_cls: 0.2735  decode.d8.loss_mask: 0.2268  decode.d8.loss_dice: 0.3202
08/06 03:39:01 - mmengine - INFO - Iter(train) [ 11300/320000]  base_lr: 9.6816e-05 lr: 9.6816e-06  eta: 1 day, 13:26:57  time: 0.4359  data_time: 0.0092  memory: 5258  grad_norm: 89.0528  loss: 9.1717  decode.loss_cls: 0.3344  decode.loss_mask: 0.2401  decode.loss_dice: 0.2896  decode.d0.loss_cls: 0.9718  decode.d0.loss_mask: 0.2735  decode.d0.loss_dice: 0.3441  decode.d1.loss_cls: 0.3790  decode.d1.loss_mask: 0.2439  decode.d1.loss_dice: 0.2721  decode.d2.loss_cls: 0.3043  decode.d2.loss_mask: 0.2399  decode.d2.loss_dice: 0.2869  decode.d3.loss_cls: 0.2864  decode.d3.loss_mask: 0.2395  decode.d3.loss_dice: 0.2798  decode.d4.loss_cls: 0.3237  decode.d4.loss_mask: 0.2468  decode.d4.loss_dice: 0.2711  decode.d5.loss_cls: 0.3216  decode.d5.loss_mask: 0.2461  decode.d5.loss_dice: 0.2981  decode.d6.loss_cls: 0.2918  decode.d6.loss_mask: 0.2432  decode.d6.loss_dice: 0.2686  decode.d7.loss_cls: 0.3201  decode.d7.loss_mask: 0.2399  decode.d7.loss_dice: 0.2693  decode.d8.loss_cls: 0.3084  decode.d8.loss_mask: 0.2531  decode.d8.loss_dice: 0.2847
08/06 03:39:23 - mmengine - INFO - Iter(train) [ 11350/320000]  base_lr: 9.6802e-05 lr: 9.6802e-06  eta: 1 day, 13:26:34  time: 0.4366  data_time: 0.0091  memory: 5242  grad_norm: 80.8595  loss: 8.9831  decode.loss_cls: 0.1472  decode.loss_mask: 0.2796  decode.loss_dice: 0.3549  decode.d0.loss_cls: 0.9899  decode.d0.loss_mask: 0.2612  decode.d0.loss_dice: 0.3898  decode.d1.loss_cls: 0.2788  decode.d1.loss_mask: 0.2718  decode.d1.loss_dice: 0.3597  decode.d2.loss_cls: 0.1922  decode.d2.loss_mask: 0.2809  decode.d2.loss_dice: 0.3648  decode.d3.loss_cls: 0.1923  decode.d3.loss_mask: 0.2760  decode.d3.loss_dice: 0.3484  decode.d4.loss_cls: 0.1691  decode.d4.loss_mask: 0.2775  decode.d4.loss_dice: 0.3582  decode.d5.loss_cls: 0.2232  decode.d5.loss_mask: 0.2545  decode.d5.loss_dice: 0.3592  decode.d6.loss_cls: 0.1703  decode.d6.loss_mask: 0.2623  decode.d6.loss_dice: 0.3509  decode.d7.loss_cls: 0.1727  decode.d7.loss_mask: 0.2579  decode.d7.loss_dice: 0.3465  decode.d8.loss_cls: 0.1837  decode.d8.loss_mask: 0.2542  decode.d8.loss_dice: 0.3555
08/06 03:39:45 - mmengine - INFO - Iter(train) [ 11400/320000]  base_lr: 9.6788e-05 lr: 9.6788e-06  eta: 1 day, 13:26:10  time: 0.4351  data_time: 0.0089  memory: 5240  grad_norm: 129.7682  loss: 9.8539  decode.loss_cls: 0.2591  decode.loss_mask: 0.3070  decode.loss_dice: 0.2903  decode.d0.loss_cls: 0.8645  decode.d0.loss_mask: 0.3157  decode.d0.loss_dice: 0.3214  decode.d1.loss_cls: 0.4237  decode.d1.loss_mask: 0.3149  decode.d1.loss_dice: 0.2929  decode.d2.loss_cls: 0.3323  decode.d2.loss_mask: 0.3028  decode.d2.loss_dice: 0.2823  decode.d3.loss_cls: 0.2961  decode.d3.loss_mask: 0.3061  decode.d3.loss_dice: 0.3102  decode.d4.loss_cls: 0.3702  decode.d4.loss_mask: 0.3076  decode.d4.loss_dice: 0.3058  decode.d5.loss_cls: 0.3146  decode.d5.loss_mask: 0.3044  decode.d5.loss_dice: 0.3043  decode.d6.loss_cls: 0.3415  decode.d6.loss_mask: 0.2987  decode.d6.loss_dice: 0.2846  decode.d7.loss_cls: 0.3194  decode.d7.loss_mask: 0.3033  decode.d7.loss_dice: 0.3024  decode.d8.loss_cls: 0.2640  decode.d8.loss_mask: 0.3044  decode.d8.loss_dice: 0.3094
08/06 03:40:06 - mmengine - INFO - Iter(train) [ 11450/320000]  base_lr: 9.6774e-05 lr: 9.6774e-06  eta: 1 day, 13:25:46  time: 0.4358  data_time: 0.0091  memory: 5236  grad_norm: 85.3743  loss: 11.3595  decode.loss_cls: 0.2510  decode.loss_mask: 0.3339  decode.loss_dice: 0.3838  decode.d0.loss_cls: 1.1830  decode.d0.loss_mask: 0.3244  decode.d0.loss_dice: 0.3982  decode.d1.loss_cls: 0.3921  decode.d1.loss_mask: 0.3365  decode.d1.loss_dice: 0.3836  decode.d2.loss_cls: 0.4096  decode.d2.loss_mask: 0.3328  decode.d2.loss_dice: 0.3598  decode.d3.loss_cls: 0.3478  decode.d3.loss_mask: 0.3285  decode.d3.loss_dice: 0.3616  decode.d4.loss_cls: 0.3642  decode.d4.loss_mask: 0.3315  decode.d4.loss_dice: 0.3741  decode.d5.loss_cls: 0.3778  decode.d5.loss_mask: 0.3305  decode.d5.loss_dice: 0.3585  decode.d6.loss_cls: 0.3309  decode.d6.loss_mask: 0.3424  decode.d6.loss_dice: 0.3945  decode.d7.loss_cls: 0.3290  decode.d7.loss_mask: 0.3356  decode.d7.loss_dice: 0.3848  decode.d8.loss_cls: 0.2645  decode.d8.loss_mask: 0.3317  decode.d8.loss_dice: 0.3831
08/06 03:40:28 - mmengine - INFO - Iter(train) [ 11500/320000]  base_lr: 9.6760e-05 lr: 9.6760e-06  eta: 1 day, 13:25:23  time: 0.4359  data_time: 0.0090  memory: 5240  grad_norm: 104.7328  loss: 10.7796  decode.loss_cls: 0.3535  decode.loss_mask: 0.2682  decode.loss_dice: 0.3878  decode.d0.loss_cls: 1.1136  decode.d0.loss_mask: 0.2755  decode.d0.loss_dice: 0.3886  decode.d1.loss_cls: 0.4699  decode.d1.loss_mask: 0.2639  decode.d1.loss_dice: 0.3655  decode.d2.loss_cls: 0.3701  decode.d2.loss_mask: 0.2712  decode.d2.loss_dice: 0.3697  decode.d3.loss_cls: 0.4095  decode.d3.loss_mask: 0.2618  decode.d3.loss_dice: 0.3338  decode.d4.loss_cls: 0.3441  decode.d4.loss_mask: 0.2686  decode.d4.loss_dice: 0.3994  decode.d5.loss_cls: 0.3277  decode.d5.loss_mask: 0.2676  decode.d5.loss_dice: 0.3823  decode.d6.loss_cls: 0.3376  decode.d6.loss_mask: 0.2680  decode.d6.loss_dice: 0.3764  decode.d7.loss_cls: 0.3124  decode.d7.loss_mask: 0.2712  decode.d7.loss_dice: 0.3634  decode.d8.loss_cls: 0.3005  decode.d8.loss_mask: 0.2704  decode.d8.loss_dice: 0.3875
08/06 03:40:50 - mmengine - INFO - Iter(train) [ 11550/320000]  base_lr: 9.6746e-05 lr: 9.6746e-06  eta: 1 day, 13:24:59  time: 0.4351  data_time: 0.0089  memory: 5260  grad_norm: 88.3330  loss: 9.1399  decode.loss_cls: 0.2038  decode.loss_mask: 0.2952  decode.loss_dice: 0.3105  decode.d0.loss_cls: 1.0564  decode.d0.loss_mask: 0.2890  decode.d0.loss_dice: 0.3263  decode.d1.loss_cls: 0.2885  decode.d1.loss_mask: 0.2802  decode.d1.loss_dice: 0.2899  decode.d2.loss_cls: 0.2320  decode.d2.loss_mask: 0.2811  decode.d2.loss_dice: 0.3015  decode.d3.loss_cls: 0.2218  decode.d3.loss_mask: 0.2835  decode.d3.loss_dice: 0.3093  decode.d4.loss_cls: 0.2262  decode.d4.loss_mask: 0.2851  decode.d4.loss_dice: 0.3193  decode.d5.loss_cls: 0.2506  decode.d5.loss_mask: 0.2847  decode.d5.loss_dice: 0.2970  decode.d6.loss_cls: 0.2172  decode.d6.loss_mask: 0.2918  decode.d6.loss_dice: 0.3171  decode.d7.loss_cls: 0.2576  decode.d7.loss_mask: 0.2885  decode.d7.loss_dice: 0.2950  decode.d8.loss_cls: 0.2471  decode.d8.loss_mask: 0.2872  decode.d8.loss_dice: 0.3064
08/06 03:41:12 - mmengine - INFO - Iter(train) [ 11600/320000]  base_lr: 9.6732e-05 lr: 9.6732e-06  eta: 1 day, 13:24:36  time: 0.4347  data_time: 0.0090  memory: 5260  grad_norm: 117.7272  loss: 11.1526  decode.loss_cls: 0.2326  decode.loss_mask: 0.3376  decode.loss_dice: 0.4372  decode.d0.loss_cls: 1.0054  decode.d0.loss_mask: 0.3575  decode.d0.loss_dice: 0.4334  decode.d1.loss_cls: 0.3027  decode.d1.loss_mask: 0.3479  decode.d1.loss_dice: 0.4365  decode.d2.loss_cls: 0.2837  decode.d2.loss_mask: 0.3425  decode.d2.loss_dice: 0.4082  decode.d3.loss_cls: 0.2719  decode.d3.loss_mask: 0.3464  decode.d3.loss_dice: 0.4367  decode.d4.loss_cls: 0.2667  decode.d4.loss_mask: 0.3455  decode.d4.loss_dice: 0.4340  decode.d5.loss_cls: 0.2796  decode.d5.loss_mask: 0.3385  decode.d5.loss_dice: 0.4286  decode.d6.loss_cls: 0.2227  decode.d6.loss_mask: 0.3483  decode.d6.loss_dice: 0.4547  decode.d7.loss_cls: 0.2448  decode.d7.loss_mask: 0.3448  decode.d7.loss_dice: 0.4358  decode.d8.loss_cls: 0.2587  decode.d8.loss_mask: 0.3500  decode.d8.loss_dice: 0.4197
08/06 03:41:33 - mmengine - INFO - Iter(train) [ 11650/320000]  base_lr: 9.6718e-05 lr: 9.6718e-06  eta: 1 day, 13:24:12  time: 0.4351  data_time: 0.0090  memory: 5260  grad_norm: 111.4585  loss: 9.3793  decode.loss_cls: 0.2403  decode.loss_mask: 0.2471  decode.loss_dice: 0.3121  decode.d0.loss_cls: 1.1659  decode.d0.loss_mask: 0.2364  decode.d0.loss_dice: 0.2595  decode.d1.loss_cls: 0.3902  decode.d1.loss_mask: 0.2393  decode.d1.loss_dice: 0.2824  decode.d2.loss_cls: 0.4162  decode.d2.loss_mask: 0.3230  decode.d2.loss_dice: 0.3384  decode.d3.loss_cls: 0.3604  decode.d3.loss_mask: 0.2338  decode.d3.loss_dice: 0.2707  decode.d4.loss_cls: 0.3017  decode.d4.loss_mask: 0.2640  decode.d4.loss_dice: 0.2593  decode.d5.loss_cls: 0.2768  decode.d5.loss_mask: 0.3176  decode.d5.loss_dice: 0.2758  decode.d6.loss_cls: 0.2221  decode.d6.loss_mask: 0.3012  decode.d6.loss_dice: 0.3102  decode.d7.loss_cls: 0.2796  decode.d7.loss_mask: 0.2347  decode.d7.loss_dice: 0.2514  decode.d8.loss_cls: 0.2736  decode.d8.loss_mask: 0.2350  decode.d8.loss_dice: 0.2609
08/06 03:41:55 - mmengine - INFO - Iter(train) [ 11700/320000]  base_lr: 9.6704e-05 lr: 9.6704e-06  eta: 1 day, 13:23:50  time: 0.4374  data_time: 0.0090  memory: 5224  grad_norm: 76.3370  loss: 10.0310  decode.loss_cls: 0.3207  decode.loss_mask: 0.2554  decode.loss_dice: 0.3535  decode.d0.loss_cls: 1.0142  decode.d0.loss_mask: 0.2778  decode.d0.loss_dice: 0.4136  decode.d1.loss_cls: 0.3158  decode.d1.loss_mask: 0.2533  decode.d1.loss_dice: 0.3548  decode.d2.loss_cls: 0.2786  decode.d2.loss_mask: 0.2499  decode.d2.loss_dice: 0.3570  decode.d3.loss_cls: 0.3046  decode.d3.loss_mask: 0.2499  decode.d3.loss_dice: 0.3354  decode.d4.loss_cls: 0.2871  decode.d4.loss_mask: 0.2532  decode.d4.loss_dice: 0.3672  decode.d5.loss_cls: 0.3296  decode.d5.loss_mask: 0.2517  decode.d5.loss_dice: 0.3504  decode.d6.loss_cls: 0.3632  decode.d6.loss_mask: 0.2493  decode.d6.loss_dice: 0.3390  decode.d7.loss_cls: 0.3558  decode.d7.loss_mask: 0.2542  decode.d7.loss_dice: 0.3539  decode.d8.loss_cls: 0.3274  decode.d8.loss_mask: 0.2563  decode.d8.loss_dice: 0.3584
08/06 03:42:17 - mmengine - INFO - Iter(train) [ 11750/320000]  base_lr: 9.6689e-05 lr: 9.6689e-06  eta: 1 day, 13:23:29  time: 0.4361  data_time: 0.0089  memory: 5299  grad_norm: 157.7339  loss: 11.3172  decode.loss_cls: 0.2872  decode.loss_mask: 0.3685  decode.loss_dice: 0.3976  decode.d0.loss_cls: 0.9555  decode.d0.loss_mask: 0.3803  decode.d0.loss_dice: 0.4045  decode.d1.loss_cls: 0.3375  decode.d1.loss_mask: 0.3751  decode.d1.loss_dice: 0.4071  decode.d2.loss_cls: 0.3015  decode.d2.loss_mask: 0.3723  decode.d2.loss_dice: 0.3858  decode.d3.loss_cls: 0.2721  decode.d3.loss_mask: 0.3742  decode.d3.loss_dice: 0.3943  decode.d4.loss_cls: 0.2870  decode.d4.loss_mask: 0.3676  decode.d4.loss_dice: 0.4083  decode.d5.loss_cls: 0.3132  decode.d5.loss_mask: 0.3738  decode.d5.loss_dice: 0.3903  decode.d6.loss_cls: 0.2945  decode.d6.loss_mask: 0.3695  decode.d6.loss_dice: 0.3900  decode.d7.loss_cls: 0.2861  decode.d7.loss_mask: 0.3886  decode.d7.loss_dice: 0.4047  decode.d8.loss_cls: 0.2735  decode.d8.loss_mask: 0.3636  decode.d8.loss_dice: 0.3929
08/06 03:42:39 - mmengine - INFO - Iter(train) [ 11800/320000]  base_lr: 9.6675e-05 lr: 9.6675e-06  eta: 1 day, 13:23:07  time: 0.4368  data_time: 0.0089  memory: 5275  grad_norm: 121.8415  loss: 11.7114  decode.loss_cls: 0.4825  decode.loss_mask: 0.2485  decode.loss_dice: 0.3786  decode.d0.loss_cls: 1.0989  decode.d0.loss_mask: 0.2597  decode.d0.loss_dice: 0.4003  decode.d1.loss_cls: 0.5655  decode.d1.loss_mask: 0.2511  decode.d1.loss_dice: 0.3716  decode.d2.loss_cls: 0.4355  decode.d2.loss_mask: 0.2527  decode.d2.loss_dice: 0.3731  decode.d3.loss_cls: 0.4639  decode.d3.loss_mask: 0.2527  decode.d3.loss_dice: 0.3733  decode.d4.loss_cls: 0.4563  decode.d4.loss_mask: 0.2496  decode.d4.loss_dice: 0.3698  decode.d5.loss_cls: 0.4834  decode.d5.loss_mask: 0.2522  decode.d5.loss_dice: 0.3946  decode.d6.loss_cls: 0.4952  decode.d6.loss_mask: 0.2494  decode.d6.loss_dice: 0.3764  decode.d7.loss_cls: 0.4569  decode.d7.loss_mask: 0.2531  decode.d7.loss_dice: 0.3654  decode.d8.loss_cls: 0.4636  decode.d8.loss_mask: 0.2540  decode.d8.loss_dice: 0.3837
08/06 03:43:01 - mmengine - INFO - Iter(train) [ 11850/320000]  base_lr: 9.6661e-05 lr: 9.6661e-06  eta: 1 day, 13:22:45  time: 0.4369  data_time: 0.0089  memory: 5240  grad_norm: 83.4948  loss: 10.4401  decode.loss_cls: 0.2171  decode.loss_mask: 0.3255  decode.loss_dice: 0.3655  decode.d0.loss_cls: 1.1340  decode.d0.loss_mask: 0.3453  decode.d0.loss_dice: 0.4036  decode.d1.loss_cls: 0.3387  decode.d1.loss_mask: 0.3331  decode.d1.loss_dice: 0.3534  decode.d2.loss_cls: 0.3170  decode.d2.loss_mask: 0.3223  decode.d2.loss_dice: 0.3631  decode.d3.loss_cls: 0.2288  decode.d3.loss_mask: 0.3264  decode.d3.loss_dice: 0.3684  decode.d4.loss_cls: 0.3244  decode.d4.loss_mask: 0.3255  decode.d4.loss_dice: 0.3592  decode.d5.loss_cls: 0.2291  decode.d5.loss_mask: 0.3277  decode.d5.loss_dice: 0.3580  decode.d6.loss_cls: 0.2515  decode.d6.loss_mask: 0.3232  decode.d6.loss_dice: 0.3667  decode.d7.loss_cls: 0.2197  decode.d7.loss_mask: 0.3249  decode.d7.loss_dice: 0.3626  decode.d8.loss_cls: 0.2279  decode.d8.loss_mask: 0.3228  decode.d8.loss_dice: 0.3746
08/06 03:43:23 - mmengine - INFO - Iter(train) [ 11900/320000]  base_lr: 9.6647e-05 lr: 9.6647e-06  eta: 1 day, 13:22:23  time: 0.4371  data_time: 0.0089  memory: 5242  grad_norm: 100.4704  loss: 9.7707  decode.loss_cls: 0.2986  decode.loss_mask: 0.2816  decode.loss_dice: 0.2944  decode.d0.loss_cls: 0.9206  decode.d0.loss_mask: 0.2560  decode.d0.loss_dice: 0.3246  decode.d1.loss_cls: 0.4998  decode.d1.loss_mask: 0.2356  decode.d1.loss_dice: 0.2956  decode.d2.loss_cls: 0.4233  decode.d2.loss_mask: 0.2400  decode.d2.loss_dice: 0.3019  decode.d3.loss_cls: 0.3589  decode.d3.loss_mask: 0.2458  decode.d3.loss_dice: 0.2899  decode.d4.loss_cls: 0.3836  decode.d4.loss_mask: 0.2348  decode.d4.loss_dice: 0.2906  decode.d5.loss_cls: 0.2910  decode.d5.loss_mask: 0.2448  decode.d5.loss_dice: 0.2789  decode.d6.loss_cls: 0.3618  decode.d6.loss_mask: 0.2495  decode.d6.loss_dice: 0.2923  decode.d7.loss_cls: 0.3883  decode.d7.loss_mask: 0.2684  decode.d7.loss_dice: 0.3114  decode.d8.loss_cls: 0.3242  decode.d8.loss_mask: 0.2642  decode.d8.loss_dice: 0.3204
08/06 03:43:44 - mmengine - INFO - Iter(train) [ 11950/320000]  base_lr: 9.6633e-05 lr: 9.6633e-06  eta: 1 day, 13:22:01  time: 0.4356  data_time: 0.0089  memory: 5240  grad_norm: 154.9741  loss: 8.3404  decode.loss_cls: 0.1757  decode.loss_mask: 0.2758  decode.loss_dice: 0.3252  decode.d0.loss_cls: 0.8599  decode.d0.loss_mask: 0.2681  decode.d0.loss_dice: 0.2943  decode.d1.loss_cls: 0.3348  decode.d1.loss_mask: 0.2625  decode.d1.loss_dice: 0.2899  decode.d2.loss_cls: 0.2157  decode.d2.loss_mask: 0.2649  decode.d2.loss_dice: 0.2967  decode.d3.loss_cls: 0.1855  decode.d3.loss_mask: 0.2648  decode.d3.loss_dice: 0.2984  decode.d4.loss_cls: 0.1951  decode.d4.loss_mask: 0.2601  decode.d4.loss_dice: 0.2793  decode.d5.loss_cls: 0.2274  decode.d5.loss_mask: 0.2597  decode.d5.loss_dice: 0.2799  decode.d6.loss_cls: 0.1676  decode.d6.loss_mask: 0.2668  decode.d6.loss_dice: 0.3066  decode.d7.loss_cls: 0.1482  decode.d7.loss_mask: 0.2655  decode.d7.loss_dice: 0.3144  decode.d8.loss_cls: 0.1570  decode.d8.loss_mask: 0.2728  decode.d8.loss_dice: 0.3276
08/06 03:44:06 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 03:44:06 - mmengine - INFO - Iter(train) [ 12000/320000]  base_lr: 9.6619e-05 lr: 9.6619e-06  eta: 1 day, 13:21:39  time: 0.4362  data_time: 0.0091  memory: 5299  grad_norm: 130.5099  loss: 9.6464  decode.loss_cls: 0.2830  decode.loss_mask: 0.2462  decode.loss_dice: 0.3271  decode.d0.loss_cls: 1.0420  decode.d0.loss_mask: 0.2433  decode.d0.loss_dice: 0.3503  decode.d1.loss_cls: 0.4983  decode.d1.loss_mask: 0.2457  decode.d1.loss_dice: 0.3345  decode.d2.loss_cls: 0.2918  decode.d2.loss_mask: 0.2445  decode.d2.loss_dice: 0.3292  decode.d3.loss_cls: 0.2937  decode.d3.loss_mask: 0.2525  decode.d3.loss_dice: 0.3281  decode.d4.loss_cls: 0.2814  decode.d4.loss_mask: 0.2428  decode.d4.loss_dice: 0.3214  decode.d5.loss_cls: 0.2801  decode.d5.loss_mask: 0.2383  decode.d5.loss_dice: 0.3261  decode.d6.loss_cls: 0.3134  decode.d6.loss_mask: 0.2418  decode.d6.loss_dice: 0.3141  decode.d7.loss_cls: 0.3013  decode.d7.loss_mask: 0.2447  decode.d7.loss_dice: 0.3225  decode.d8.loss_cls: 0.3555  decode.d8.loss_mask: 0.2409  decode.d8.loss_dice: 0.3116
08/06 03:44:28 - mmengine - INFO - Iter(train) [ 12050/320000]  base_lr: 9.6605e-05 lr: 9.6605e-06  eta: 1 day, 13:21:17  time: 0.4359  data_time: 0.0091  memory: 5260  grad_norm: 58.2922  loss: 9.0503  decode.loss_cls: 0.1619  decode.loss_mask: 0.3713  decode.loss_dice: 0.3063  decode.d0.loss_cls: 0.8215  decode.d0.loss_mask: 0.3788  decode.d0.loss_dice: 0.3402  decode.d1.loss_cls: 0.1926  decode.d1.loss_mask: 0.3739  decode.d1.loss_dice: 0.3082  decode.d2.loss_cls: 0.1809  decode.d2.loss_mask: 0.3755  decode.d2.loss_dice: 0.2928  decode.d3.loss_cls: 0.1489  decode.d3.loss_mask: 0.3814  decode.d3.loss_dice: 0.3099  decode.d4.loss_cls: 0.1653  decode.d4.loss_mask: 0.3743  decode.d4.loss_dice: 0.2852  decode.d5.loss_cls: 0.1544  decode.d5.loss_mask: 0.3690  decode.d5.loss_dice: 0.3069  decode.d6.loss_cls: 0.1554  decode.d6.loss_mask: 0.3731  decode.d6.loss_dice: 0.2865  decode.d7.loss_cls: 0.1451  decode.d7.loss_mask: 0.3768  decode.d7.loss_dice: 0.2900  decode.d8.loss_cls: 0.1409  decode.d8.loss_mask: 0.3741  decode.d8.loss_dice: 0.3094
08/06 03:44:50 - mmengine - INFO - Iter(train) [ 12100/320000]  base_lr: 9.6591e-05 lr: 9.6591e-06  eta: 1 day, 13:20:56  time: 0.4370  data_time: 0.0091  memory: 5242  grad_norm: 88.9764  loss: 10.5385  decode.loss_cls: 0.2641  decode.loss_mask: 0.2973  decode.loss_dice: 0.3576  decode.d0.loss_cls: 1.2887  decode.d0.loss_mask: 0.2933  decode.d0.loss_dice: 0.3452  decode.d1.loss_cls: 0.3790  decode.d1.loss_mask: 0.2989  decode.d1.loss_dice: 0.3328  decode.d2.loss_cls: 0.3089  decode.d2.loss_mask: 0.2933  decode.d2.loss_dice: 0.3325  decode.d3.loss_cls: 0.2976  decode.d3.loss_mask: 0.2951  decode.d3.loss_dice: 0.3434  decode.d4.loss_cls: 0.2691  decode.d4.loss_mask: 0.2978  decode.d4.loss_dice: 0.3557  decode.d5.loss_cls: 0.2859  decode.d5.loss_mask: 0.2979  decode.d5.loss_dice: 0.3562  decode.d6.loss_cls: 0.3862  decode.d6.loss_mask: 0.2948  decode.d6.loss_dice: 0.3563  decode.d7.loss_cls: 0.3059  decode.d7.loss_mask: 0.2964  decode.d7.loss_dice: 0.3581  decode.d8.loss_cls: 0.3004  decode.d8.loss_mask: 0.2963  decode.d8.loss_dice: 0.3538
08/06 03:45:12 - mmengine - INFO - Iter(train) [ 12150/320000]  base_lr: 9.6577e-05 lr: 9.6577e-06  eta: 1 day, 13:20:33  time: 0.4364  data_time: 0.0091  memory: 5258  grad_norm: 145.0859  loss: 12.4205  decode.loss_cls: 0.5178  decode.loss_mask: 0.2628  decode.loss_dice: 0.3309  decode.d0.loss_cls: 1.2401  decode.d0.loss_mask: 0.2759  decode.d0.loss_dice: 0.4166  decode.d1.loss_cls: 0.6532  decode.d1.loss_mask: 0.2555  decode.d1.loss_dice: 0.3834  decode.d2.loss_cls: 0.6332  decode.d2.loss_mask: 0.2507  decode.d2.loss_dice: 0.3506  decode.d3.loss_cls: 0.5791  decode.d3.loss_mask: 0.2554  decode.d3.loss_dice: 0.3153  decode.d4.loss_cls: 0.5132  decode.d4.loss_mask: 0.2674  decode.d4.loss_dice: 0.3614  decode.d5.loss_cls: 0.5383  decode.d5.loss_mask: 0.2684  decode.d5.loss_dice: 0.3496  decode.d6.loss_cls: 0.4956  decode.d6.loss_mask: 0.2615  decode.d6.loss_dice: 0.3440  decode.d7.loss_cls: 0.5292  decode.d7.loss_mask: 0.2605  decode.d7.loss_dice: 0.3646  decode.d8.loss_cls: 0.5382  decode.d8.loss_mask: 0.2614  decode.d8.loss_dice: 0.3468
08/06 03:45:34 - mmengine - INFO - Iter(train) [ 12200/320000]  base_lr: 9.6562e-05 lr: 9.6562e-06  eta: 1 day, 13:20:11  time: 0.4362  data_time: 0.0091  memory: 5224  grad_norm: 81.4203  loss: 9.1026  decode.loss_cls: 0.2386  decode.loss_mask: 0.2575  decode.loss_dice: 0.2901  decode.d0.loss_cls: 0.9590  decode.d0.loss_mask: 0.2721  decode.d0.loss_dice: 0.3481  decode.d1.loss_cls: 0.3363  decode.d1.loss_mask: 0.2551  decode.d1.loss_dice: 0.2795  decode.d2.loss_cls: 0.3223  decode.d2.loss_mask: 0.2602  decode.d2.loss_dice: 0.3057  decode.d3.loss_cls: 0.2992  decode.d3.loss_mask: 0.2570  decode.d3.loss_dice: 0.2889  decode.d4.loss_cls: 0.2289  decode.d4.loss_mask: 0.2771  decode.d4.loss_dice: 0.3122  decode.d5.loss_cls: 0.2959  decode.d5.loss_mask: 0.2646  decode.d5.loss_dice: 0.3051  decode.d6.loss_cls: 0.2676  decode.d6.loss_mask: 0.2597  decode.d6.loss_dice: 0.2896  decode.d7.loss_cls: 0.2758  decode.d7.loss_mask: 0.2572  decode.d7.loss_dice: 0.2935  decode.d8.loss_cls: 0.2260  decode.d8.loss_mask: 0.2794  decode.d8.loss_dice: 0.3003
08/06 03:45:55 - mmengine - INFO - Iter(train) [ 12250/320000]  base_lr: 9.6548e-05 lr: 9.6548e-06  eta: 1 day, 13:19:49  time: 0.4358  data_time: 0.0091  memory: 5260  grad_norm: 57.5266  loss: 7.7170  decode.loss_cls: 0.1779  decode.loss_mask: 0.2586  decode.loss_dice: 0.2715  decode.d0.loss_cls: 0.9272  decode.d0.loss_mask: 0.2620  decode.d0.loss_dice: 0.2814  decode.d1.loss_cls: 0.2827  decode.d1.loss_mask: 0.2616  decode.d1.loss_dice: 0.2489  decode.d2.loss_cls: 0.1443  decode.d2.loss_mask: 0.2603  decode.d2.loss_dice: 0.2590  decode.d3.loss_cls: 0.1578  decode.d3.loss_mask: 0.2608  decode.d3.loss_dice: 0.2513  decode.d4.loss_cls: 0.1773  decode.d4.loss_mask: 0.2621  decode.d4.loss_dice: 0.2585  decode.d5.loss_cls: 0.1592  decode.d5.loss_mask: 0.2632  decode.d5.loss_dice: 0.2504  decode.d6.loss_cls: 0.1591  decode.d6.loss_mask: 0.2625  decode.d6.loss_dice: 0.2740  decode.d7.loss_cls: 0.1428  decode.d7.loss_mask: 0.2578  decode.d7.loss_dice: 0.2625  decode.d8.loss_cls: 0.1671  decode.d8.loss_mask: 0.2540  decode.d8.loss_dice: 0.2614
08/06 03:46:17 - mmengine - INFO - Iter(train) [ 12300/320000]  base_lr: 9.6534e-05 lr: 9.6534e-06  eta: 1 day, 13:19:27  time: 0.4366  data_time: 0.0090  memory: 5260  grad_norm: 133.8710  loss: 8.6864  decode.loss_cls: 0.2572  decode.loss_mask: 0.2446  decode.loss_dice: 0.2941  decode.d0.loss_cls: 1.0840  decode.d0.loss_mask: 0.2503  decode.d0.loss_dice: 0.3053  decode.d1.loss_cls: 0.2966  decode.d1.loss_mask: 0.2486  decode.d1.loss_dice: 0.3155  decode.d2.loss_cls: 0.2288  decode.d2.loss_mask: 0.2449  decode.d2.loss_dice: 0.2994  decode.d3.loss_cls: 0.2446  decode.d3.loss_mask: 0.2493  decode.d3.loss_dice: 0.2988  decode.d4.loss_cls: 0.2349  decode.d4.loss_mask: 0.2530  decode.d4.loss_dice: 0.3052  decode.d5.loss_cls: 0.2164  decode.d5.loss_mask: 0.2338  decode.d5.loss_dice: 0.3105  decode.d6.loss_cls: 0.1998  decode.d6.loss_mask: 0.2445  decode.d6.loss_dice: 0.2890  decode.d7.loss_cls: 0.2380  decode.d7.loss_mask: 0.2392  decode.d7.loss_dice: 0.3029  decode.d8.loss_cls: 0.2239  decode.d8.loss_mask: 0.2426  decode.d8.loss_dice: 0.2905
08/06 03:46:39 - mmengine - INFO - Iter(train) [ 12350/320000]  base_lr: 9.6520e-05 lr: 9.6520e-06  eta: 1 day, 13:19:05  time: 0.4363  data_time: 0.0090  memory: 5260  grad_norm: 110.3394  loss: 10.0763  decode.loss_cls: 0.3201  decode.loss_mask: 0.2549  decode.loss_dice: 0.3241  decode.d0.loss_cls: 1.0047  decode.d0.loss_mask: 0.2561  decode.d0.loss_dice: 0.3350  decode.d1.loss_cls: 0.3770  decode.d1.loss_mask: 0.2483  decode.d1.loss_dice: 0.2976  decode.d2.loss_cls: 0.3721  decode.d2.loss_mask: 0.2528  decode.d2.loss_dice: 0.3031  decode.d3.loss_cls: 0.3979  decode.d3.loss_mask: 0.2529  decode.d3.loss_dice: 0.3038  decode.d4.loss_cls: 0.3717  decode.d4.loss_mask: 0.2573  decode.d4.loss_dice: 0.3033  decode.d5.loss_cls: 0.4231  decode.d5.loss_mask: 0.2525  decode.d5.loss_dice: 0.3021  decode.d6.loss_cls: 0.4153  decode.d6.loss_mask: 0.2562  decode.d6.loss_dice: 0.2992  decode.d7.loss_cls: 0.4414  decode.d7.loss_mask: 0.2548  decode.d7.loss_dice: 0.2937  decode.d8.loss_cls: 0.3280  decode.d8.loss_mask: 0.2578  decode.d8.loss_dice: 0.3198
08/06 03:47:01 - mmengine - INFO - Iter(train) [ 12400/320000]  base_lr: 9.6506e-05 lr: 9.6506e-06  eta: 1 day, 13:18:43  time: 0.4373  data_time: 0.0090  memory: 5224  grad_norm: 102.7705  loss: 9.3547  decode.loss_cls: 0.2790  decode.loss_mask: 0.2549  decode.loss_dice: 0.3210  decode.d0.loss_cls: 0.9199  decode.d0.loss_mask: 0.2628  decode.d0.loss_dice: 0.3778  decode.d1.loss_cls: 0.3560  decode.d1.loss_mask: 0.2571  decode.d1.loss_dice: 0.3411  decode.d2.loss_cls: 0.2131  decode.d2.loss_mask: 0.2552  decode.d2.loss_dice: 0.3380  decode.d3.loss_cls: 0.2599  decode.d3.loss_mask: 0.2551  decode.d3.loss_dice: 0.3380  decode.d4.loss_cls: 0.2606  decode.d4.loss_mask: 0.2544  decode.d4.loss_dice: 0.3262  decode.d5.loss_cls: 0.3484  decode.d5.loss_mask: 0.2538  decode.d5.loss_dice: 0.3201  decode.d6.loss_cls: 0.2931  decode.d6.loss_mask: 0.2529  decode.d6.loss_dice: 0.3097  decode.d7.loss_cls: 0.3021  decode.d7.loss_mask: 0.2563  decode.d7.loss_dice: 0.3119  decode.d8.loss_cls: 0.2616  decode.d8.loss_mask: 0.2540  decode.d8.loss_dice: 0.3209
08/06 03:47:23 - mmengine - INFO - Iter(train) [ 12450/320000]  base_lr: 9.6492e-05 lr: 9.6492e-06  eta: 1 day, 13:18:21  time: 0.4364  data_time: 0.0088  memory: 5260  grad_norm: 146.0307  loss: 9.5474  decode.loss_cls: 0.2852  decode.loss_mask: 0.2309  decode.loss_dice: 0.2949  decode.d0.loss_cls: 1.0538  decode.d0.loss_mask: 0.2498  decode.d0.loss_dice: 0.3418  decode.d1.loss_cls: 0.3364  decode.d1.loss_mask: 0.2490  decode.d1.loss_dice: 0.3144  decode.d2.loss_cls: 0.3731  decode.d2.loss_mask: 0.2483  decode.d2.loss_dice: 0.3000  decode.d3.loss_cls: 0.3599  decode.d3.loss_mask: 0.2298  decode.d3.loss_dice: 0.2859  decode.d4.loss_cls: 0.3115  decode.d4.loss_mask: 0.2366  decode.d4.loss_dice: 0.3021  decode.d5.loss_cls: 0.3811  decode.d5.loss_mask: 0.2432  decode.d5.loss_dice: 0.3032  decode.d6.loss_cls: 0.3480  decode.d6.loss_mask: 0.2403  decode.d6.loss_dice: 0.2995  decode.d7.loss_cls: 0.3558  decode.d7.loss_mask: 0.2392  decode.d7.loss_dice: 0.3056  decode.d8.loss_cls: 0.2855  decode.d8.loss_mask: 0.2413  decode.d8.loss_dice: 0.3012
08/06 03:47:45 - mmengine - INFO - Iter(train) [ 12500/320000]  base_lr: 9.6478e-05 lr: 9.6478e-06  eta: 1 day, 13:17:59  time: 0.4352  data_time: 0.0089  memory: 5258  grad_norm: 72.9200  loss: 8.6139  decode.loss_cls: 0.2647  decode.loss_mask: 0.2238  decode.loss_dice: 0.2975  decode.d0.loss_cls: 1.0373  decode.d0.loss_mask: 0.2354  decode.d0.loss_dice: 0.3402  decode.d1.loss_cls: 0.2189  decode.d1.loss_mask: 0.2257  decode.d1.loss_dice: 0.3215  decode.d2.loss_cls: 0.2089  decode.d2.loss_mask: 0.2211  decode.d2.loss_dice: 0.3075  decode.d3.loss_cls: 0.2327  decode.d3.loss_mask: 0.2198  decode.d3.loss_dice: 0.2937  decode.d4.loss_cls: 0.2236  decode.d4.loss_mask: 0.2225  decode.d4.loss_dice: 0.2970  decode.d5.loss_cls: 0.2459  decode.d5.loss_mask: 0.2262  decode.d5.loss_dice: 0.3128  decode.d6.loss_cls: 0.2557  decode.d6.loss_mask: 0.2257  decode.d6.loss_dice: 0.3019  decode.d7.loss_cls: 0.2894  decode.d7.loss_mask: 0.2245  decode.d7.loss_dice: 0.3142  decode.d8.loss_cls: 0.3005  decode.d8.loss_mask: 0.2239  decode.d8.loss_dice: 0.3015
08/06 03:48:06 - mmengine - INFO - Iter(train) [ 12550/320000]  base_lr: 9.6464e-05 lr: 9.6464e-06  eta: 1 day, 13:17:36  time: 0.4369  data_time: 0.0090  memory: 5240  grad_norm: 82.7311  loss: 8.0134  decode.loss_cls: 0.1261  decode.loss_mask: 0.2665  decode.loss_dice: 0.2783  decode.d0.loss_cls: 0.9549  decode.d0.loss_mask: 0.2755  decode.d0.loss_dice: 0.3146  decode.d1.loss_cls: 0.2706  decode.d1.loss_mask: 0.2741  decode.d1.loss_dice: 0.3114  decode.d2.loss_cls: 0.1539  decode.d2.loss_mask: 0.2753  decode.d2.loss_dice: 0.2761  decode.d3.loss_cls: 0.1562  decode.d3.loss_mask: 0.2715  decode.d3.loss_dice: 0.3009  decode.d4.loss_cls: 0.1227  decode.d4.loss_mask: 0.2718  decode.d4.loss_dice: 0.2817  decode.d5.loss_cls: 0.1649  decode.d5.loss_mask: 0.2712  decode.d5.loss_dice: 0.2848  decode.d6.loss_cls: 0.1484  decode.d6.loss_mask: 0.2704  decode.d6.loss_dice: 0.2777  decode.d7.loss_cls: 0.1505  decode.d7.loss_mask: 0.2713  decode.d7.loss_dice: 0.2863  decode.d8.loss_cls: 0.1408  decode.d8.loss_mask: 0.2726  decode.d8.loss_dice: 0.2925
08/06 03:48:28 - mmengine - INFO - Iter(train) [ 12600/320000]  base_lr: 9.6449e-05 lr: 9.6449e-06  eta: 1 day, 13:17:14  time: 0.4357  data_time: 0.0088  memory: 5242  grad_norm: 116.7447  loss: 9.3233  decode.loss_cls: 0.3496  decode.loss_mask: 0.2554  decode.loss_dice: 0.2601  decode.d0.loss_cls: 1.0154  decode.d0.loss_mask: 0.2916  decode.d0.loss_dice: 0.3040  decode.d1.loss_cls: 0.4063  decode.d1.loss_mask: 0.2474  decode.d1.loss_dice: 0.2527  decode.d2.loss_cls: 0.3154  decode.d2.loss_mask: 0.2523  decode.d2.loss_dice: 0.2537  decode.d3.loss_cls: 0.3292  decode.d3.loss_mask: 0.2528  decode.d3.loss_dice: 0.2576  decode.d4.loss_cls: 0.3463  decode.d4.loss_mask: 0.2616  decode.d4.loss_dice: 0.2724  decode.d5.loss_cls: 0.3363  decode.d5.loss_mask: 0.2651  decode.d5.loss_dice: 0.2716  decode.d6.loss_cls: 0.3316  decode.d6.loss_mask: 0.2587  decode.d6.loss_dice: 0.2626  decode.d7.loss_cls: 0.3242  decode.d7.loss_mask: 0.2665  decode.d7.loss_dice: 0.2538  decode.d8.loss_cls: 0.3240  decode.d8.loss_mask: 0.2589  decode.d8.loss_dice: 0.2461
08/06 03:48:50 - mmengine - INFO - Iter(train) [ 12650/320000]  base_lr: 9.6435e-05 lr: 9.6435e-06  eta: 1 day, 13:16:51  time: 0.4362  data_time: 0.0090  memory: 5242  grad_norm: 82.6935  loss: 8.4267  decode.loss_cls: 0.1689  decode.loss_mask: 0.2787  decode.loss_dice: 0.3017  decode.d0.loss_cls: 0.8824  decode.d0.loss_mask: 0.3033  decode.d0.loss_dice: 0.3052  decode.d1.loss_cls: 0.2389  decode.d1.loss_mask: 0.2854  decode.d1.loss_dice: 0.3069  decode.d2.loss_cls: 0.2090  decode.d2.loss_mask: 0.2846  decode.d2.loss_dice: 0.2960  decode.d3.loss_cls: 0.2101  decode.d3.loss_mask: 0.2852  decode.d3.loss_dice: 0.2989  decode.d4.loss_cls: 0.1810  decode.d4.loss_mask: 0.2815  decode.d4.loss_dice: 0.2991  decode.d5.loss_cls: 0.1585  decode.d5.loss_mask: 0.2826  decode.d5.loss_dice: 0.3156  decode.d6.loss_cls: 0.1713  decode.d6.loss_mask: 0.2866  decode.d6.loss_dice: 0.3111  decode.d7.loss_cls: 0.1844  decode.d7.loss_mask: 0.2808  decode.d7.loss_dice: 0.3001  decode.d8.loss_cls: 0.1427  decode.d8.loss_mask: 0.2857  decode.d8.loss_dice: 0.2907
08/06 03:49:12 - mmengine - INFO - Iter(train) [ 12700/320000]  base_lr: 9.6421e-05 lr: 9.6421e-06  eta: 1 day, 13:16:33  time: 0.4361  data_time: 0.0089  memory: 5240  grad_norm: 124.4141  loss: 9.4056  decode.loss_cls: 0.3070  decode.loss_mask: 0.2281  decode.loss_dice: 0.2805  decode.d0.loss_cls: 1.2602  decode.d0.loss_mask: 0.2408  decode.d0.loss_dice: 0.3228  decode.d1.loss_cls: 0.3833  decode.d1.loss_mask: 0.2300  decode.d1.loss_dice: 0.2800  decode.d2.loss_cls: 0.3387  decode.d2.loss_mask: 0.2257  decode.d2.loss_dice: 0.2768  decode.d3.loss_cls: 0.3277  decode.d3.loss_mask: 0.2376  decode.d3.loss_dice: 0.2741  decode.d4.loss_cls: 0.3248  decode.d4.loss_mask: 0.2378  decode.d4.loss_dice: 0.2785  decode.d5.loss_cls: 0.4034  decode.d5.loss_mask: 0.2267  decode.d5.loss_dice: 0.2662  decode.d6.loss_cls: 0.2897  decode.d6.loss_mask: 0.2389  decode.d6.loss_dice: 0.2768  decode.d7.loss_cls: 0.3247  decode.d7.loss_mask: 0.2284  decode.d7.loss_dice: 0.2731  decode.d8.loss_cls: 0.3153  decode.d8.loss_mask: 0.2320  decode.d8.loss_dice: 0.2761
08/06 03:49:34 - mmengine - INFO - Iter(train) [ 12750/320000]  base_lr: 9.6407e-05 lr: 9.6407e-06  eta: 1 day, 13:16:10  time: 0.4368  data_time: 0.0091  memory: 5260  grad_norm: 80.6329  loss: 9.0248  decode.loss_cls: 0.2606  decode.loss_mask: 0.2605  decode.loss_dice: 0.3074  decode.d0.loss_cls: 0.9567  decode.d0.loss_mask: 0.2708  decode.d0.loss_dice: 0.3443  decode.d1.loss_cls: 0.2765  decode.d1.loss_mask: 0.2633  decode.d1.loss_dice: 0.2833  decode.d2.loss_cls: 0.2938  decode.d2.loss_mask: 0.2684  decode.d2.loss_dice: 0.2929  decode.d3.loss_cls: 0.2128  decode.d3.loss_mask: 0.2672  decode.d3.loss_dice: 0.3076  decode.d4.loss_cls: 0.2310  decode.d4.loss_mask: 0.2662  decode.d4.loss_dice: 0.3042  decode.d5.loss_cls: 0.2782  decode.d5.loss_mask: 0.2664  decode.d5.loss_dice: 0.2935  decode.d6.loss_cls: 0.2736  decode.d6.loss_mask: 0.2683  decode.d6.loss_dice: 0.3066  decode.d7.loss_cls: 0.2784  decode.d7.loss_mask: 0.2631  decode.d7.loss_dice: 0.2800  decode.d8.loss_cls: 0.2924  decode.d8.loss_mask: 0.2641  decode.d8.loss_dice: 0.2926
08/06 03:49:56 - mmengine - INFO - Iter(train) [ 12800/320000]  base_lr: 9.6393e-05 lr: 9.6393e-06  eta: 1 day, 13:15:48  time: 0.4361  data_time: 0.0091  memory: 5242  grad_norm: 132.1934  loss: 9.9884  decode.loss_cls: 0.3752  decode.loss_mask: 0.2218  decode.loss_dice: 0.2729  decode.d0.loss_cls: 1.0848  decode.d0.loss_mask: 0.2308  decode.d0.loss_dice: 0.3289  decode.d1.loss_cls: 0.5156  decode.d1.loss_mask: 0.2262  decode.d1.loss_dice: 0.2841  decode.d2.loss_cls: 0.4202  decode.d2.loss_mask: 0.2189  decode.d2.loss_dice: 0.2802  decode.d3.loss_cls: 0.4998  decode.d3.loss_mask: 0.2177  decode.d3.loss_dice: 0.2489  decode.d4.loss_cls: 0.4488  decode.d4.loss_mask: 0.2234  decode.d4.loss_dice: 0.2748  decode.d5.loss_cls: 0.4403  decode.d5.loss_mask: 0.2811  decode.d5.loss_dice: 0.2823  decode.d6.loss_cls: 0.3898  decode.d6.loss_mask: 0.2227  decode.d6.loss_dice: 0.2591  decode.d7.loss_cls: 0.3743  decode.d7.loss_mask: 0.2217  decode.d7.loss_dice: 0.2753  decode.d8.loss_cls: 0.3712  decode.d8.loss_mask: 0.2210  decode.d8.loss_dice: 0.2765
08/06 03:50:17 - mmengine - INFO - Iter(train) [ 12850/320000]  base_lr: 9.6379e-05 lr: 9.6379e-06  eta: 1 day, 13:15:25  time: 0.4363  data_time: 0.0089  memory: 5275  grad_norm: 129.8365  loss: 9.0855  decode.loss_cls: 0.2042  decode.loss_mask: 0.2379  decode.loss_dice: 0.3346  decode.d0.loss_cls: 1.1425  decode.d0.loss_mask: 0.2594  decode.d0.loss_dice: 0.3621  decode.d1.loss_cls: 0.3166  decode.d1.loss_mask: 0.2526  decode.d1.loss_dice: 0.3451  decode.d2.loss_cls: 0.2902  decode.d2.loss_mask: 0.2402  decode.d2.loss_dice: 0.3014  decode.d3.loss_cls: 0.2467  decode.d3.loss_mask: 0.2381  decode.d3.loss_dice: 0.3265  decode.d4.loss_cls: 0.3199  decode.d4.loss_mask: 0.2361  decode.d4.loss_dice: 0.2961  decode.d5.loss_cls: 0.2537  decode.d5.loss_mask: 0.2326  decode.d5.loss_dice: 0.2943  decode.d6.loss_cls: 0.2904  decode.d6.loss_mask: 0.2369  decode.d6.loss_dice: 0.3012  decode.d7.loss_cls: 0.2500  decode.d7.loss_mask: 0.2374  decode.d7.loss_dice: 0.2884  decode.d8.loss_cls: 0.2160  decode.d8.loss_mask: 0.2385  decode.d8.loss_dice: 0.2959
08/06 03:50:39 - mmengine - INFO - Iter(train) [ 12900/320000]  base_lr: 9.6365e-05 lr: 9.6365e-06  eta: 1 day, 13:15:01  time: 0.4349  data_time: 0.0089  memory: 5240  grad_norm: 147.1153  loss: 10.0605  decode.loss_cls: 0.2706  decode.loss_mask: 0.3048  decode.loss_dice: 0.3327  decode.d0.loss_cls: 1.0307  decode.d0.loss_mask: 0.3076  decode.d0.loss_dice: 0.3643  decode.d1.loss_cls: 0.3550  decode.d1.loss_mask: 0.3026  decode.d1.loss_dice: 0.3547  decode.d2.loss_cls: 0.2690  decode.d2.loss_mask: 0.3026  decode.d2.loss_dice: 0.3509  decode.d3.loss_cls: 0.2842  decode.d3.loss_mask: 0.2982  decode.d3.loss_dice: 0.3225  decode.d4.loss_cls: 0.2773  decode.d4.loss_mask: 0.2993  decode.d4.loss_dice: 0.3184  decode.d5.loss_cls: 0.2954  decode.d5.loss_mask: 0.3054  decode.d5.loss_dice: 0.3326  decode.d6.loss_cls: 0.2984  decode.d6.loss_mask: 0.3206  decode.d6.loss_dice: 0.3458  decode.d7.loss_cls: 0.2562  decode.d7.loss_mask: 0.3213  decode.d7.loss_dice: 0.3498  decode.d8.loss_cls: 0.2322  decode.d8.loss_mask: 0.3078  decode.d8.loss_dice: 0.3497
08/06 03:51:01 - mmengine - INFO - Iter(train) [ 12950/320000]  base_lr: 9.6351e-05 lr: 9.6351e-06  eta: 1 day, 13:14:38  time: 0.4358  data_time: 0.0092  memory: 5260  grad_norm: 119.0620  loss: 8.6261  decode.loss_cls: 0.2666  decode.loss_mask: 0.2681  decode.loss_dice: 0.2581  decode.d0.loss_cls: 0.8762  decode.d0.loss_mask: 0.2729  decode.d0.loss_dice: 0.2595  decode.d1.loss_cls: 0.2580  decode.d1.loss_mask: 0.2739  decode.d1.loss_dice: 0.2644  decode.d2.loss_cls: 0.2003  decode.d2.loss_mask: 0.2744  decode.d2.loss_dice: 0.2655  decode.d3.loss_cls: 0.2162  decode.d3.loss_mask: 0.2710  decode.d3.loss_dice: 0.2709  decode.d4.loss_cls: 0.3250  decode.d4.loss_mask: 0.2740  decode.d4.loss_dice: 0.2831  decode.d5.loss_cls: 0.2202  decode.d5.loss_mask: 0.2716  decode.d5.loss_dice: 0.2815  decode.d6.loss_cls: 0.2827  decode.d6.loss_mask: 0.2711  decode.d6.loss_dice: 0.2807  decode.d7.loss_cls: 0.3018  decode.d7.loss_mask: 0.2681  decode.d7.loss_dice: 0.2616  decode.d8.loss_cls: 0.2689  decode.d8.loss_mask: 0.2654  decode.d8.loss_dice: 0.2741
08/06 03:51:23 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 03:51:23 - mmengine - INFO - Iter(train) [ 13000/320000]  base_lr: 9.6336e-05 lr: 9.6336e-06  eta: 1 day, 13:14:15  time: 0.4365  data_time: 0.0090  memory: 5260  grad_norm: 68.8259  loss: 8.0985  decode.loss_cls: 0.1180  decode.loss_mask: 0.2802  decode.loss_dice: 0.3160  decode.d0.loss_cls: 0.8729  decode.d0.loss_mask: 0.2910  decode.d0.loss_dice: 0.3328  decode.d1.loss_cls: 0.2081  decode.d1.loss_mask: 0.2692  decode.d1.loss_dice: 0.3188  decode.d2.loss_cls: 0.1504  decode.d2.loss_mask: 0.2711  decode.d2.loss_dice: 0.3261  decode.d3.loss_cls: 0.1362  decode.d3.loss_mask: 0.2695  decode.d3.loss_dice: 0.3250  decode.d4.loss_cls: 0.1341  decode.d4.loss_mask: 0.2615  decode.d4.loss_dice: 0.3241  decode.d5.loss_cls: 0.1316  decode.d5.loss_mask: 0.2695  decode.d5.loss_dice: 0.3225  decode.d6.loss_cls: 0.1288  decode.d6.loss_mask: 0.2737  decode.d6.loss_dice: 0.3257  decode.d7.loss_cls: 0.1251  decode.d7.loss_mask: 0.2803  decode.d7.loss_dice: 0.3218  decode.d8.loss_cls: 0.1272  decode.d8.loss_mask: 0.2737  decode.d8.loss_dice: 0.3138
08/06 03:51:44 - mmengine - INFO - Iter(train) [ 13050/320000]  base_lr: 9.6322e-05 lr: 9.6322e-06  eta: 1 day, 13:13:52  time: 0.4349  data_time: 0.0091  memory: 5260  grad_norm: 192.2685  loss: 13.5267  decode.loss_cls: 0.5069  decode.loss_mask: 0.4414  decode.loss_dice: 0.4473  decode.d0.loss_cls: 1.0913  decode.d0.loss_mask: 0.3686  decode.d0.loss_dice: 0.4205  decode.d1.loss_cls: 0.5328  decode.d1.loss_mask: 0.3686  decode.d1.loss_dice: 0.4153  decode.d2.loss_cls: 0.5651  decode.d2.loss_mask: 0.3604  decode.d2.loss_dice: 0.3762  decode.d3.loss_cls: 0.4591  decode.d3.loss_mask: 0.3661  decode.d3.loss_dice: 0.3988  decode.d4.loss_cls: 0.4534  decode.d4.loss_mask: 0.3753  decode.d4.loss_dice: 0.4171  decode.d5.loss_cls: 0.3911  decode.d5.loss_mask: 0.4036  decode.d5.loss_dice: 0.4584  decode.d6.loss_cls: 0.4791  decode.d6.loss_mask: 0.3624  decode.d6.loss_dice: 0.4062  decode.d7.loss_cls: 0.5062  decode.d7.loss_mask: 0.4154  decode.d7.loss_dice: 0.4391  decode.d8.loss_cls: 0.4325  decode.d8.loss_mask: 0.4339  decode.d8.loss_dice: 0.4347
08/06 03:52:06 - mmengine - INFO - Iter(train) [ 13100/320000]  base_lr: 9.6308e-05 lr: 9.6308e-06  eta: 1 day, 13:13:30  time: 0.4367  data_time: 0.0091  memory: 5260  grad_norm: 174.0428  loss: 10.4864  decode.loss_cls: 0.3510  decode.loss_mask: 0.2493  decode.loss_dice: 0.3157  decode.d0.loss_cls: 1.1377  decode.d0.loss_mask: 0.2664  decode.d0.loss_dice: 0.3384  decode.d1.loss_cls: 0.4636  decode.d1.loss_mask: 0.2674  decode.d1.loss_dice: 0.3254  decode.d2.loss_cls: 0.4088  decode.d2.loss_mask: 0.2573  decode.d2.loss_dice: 0.3297  decode.d3.loss_cls: 0.3814  decode.d3.loss_mask: 0.2621  decode.d3.loss_dice: 0.3139  decode.d4.loss_cls: 0.3746  decode.d4.loss_mask: 0.2684  decode.d4.loss_dice: 0.3113  decode.d5.loss_cls: 0.3632  decode.d5.loss_mask: 0.2725  decode.d5.loss_dice: 0.3095  decode.d6.loss_cls: 0.3827  decode.d6.loss_mask: 0.2651  decode.d6.loss_dice: 0.3301  decode.d7.loss_cls: 0.4323  decode.d7.loss_mask: 0.2522  decode.d7.loss_dice: 0.3053  decode.d8.loss_cls: 0.3897  decode.d8.loss_mask: 0.2501  decode.d8.loss_dice: 0.3113
08/06 03:52:28 - mmengine - INFO - Iter(train) [ 13150/320000]  base_lr: 9.6294e-05 lr: 9.6294e-06  eta: 1 day, 13:13:07  time: 0.4368  data_time: 0.0091  memory: 5242  grad_norm: 65.9209  loss: 7.6233  decode.loss_cls: 0.1252  decode.loss_mask: 0.2911  decode.loss_dice: 0.2638  decode.d0.loss_cls: 0.8210  decode.d0.loss_mask: 0.3187  decode.d0.loss_dice: 0.3052  decode.d1.loss_cls: 0.1422  decode.d1.loss_mask: 0.2972  decode.d1.loss_dice: 0.2692  decode.d2.loss_cls: 0.1226  decode.d2.loss_mask: 0.2978  decode.d2.loss_dice: 0.2669  decode.d3.loss_cls: 0.1223  decode.d3.loss_mask: 0.2930  decode.d3.loss_dice: 0.2693  decode.d4.loss_cls: 0.1352  decode.d4.loss_mask: 0.2943  decode.d4.loss_dice: 0.2593  decode.d5.loss_cls: 0.1302  decode.d5.loss_mask: 0.2935  decode.d5.loss_dice: 0.2661  decode.d6.loss_cls: 0.1200  decode.d6.loss_mask: 0.2952  decode.d6.loss_dice: 0.2657  decode.d7.loss_cls: 0.1332  decode.d7.loss_mask: 0.2929  decode.d7.loss_dice: 0.2592  decode.d8.loss_cls: 0.1172  decode.d8.loss_mask: 0.2915  decode.d8.loss_dice: 0.2642
08/06 03:52:50 - mmengine - INFO - Iter(train) [ 13200/320000]  base_lr: 9.6280e-05 lr: 9.6280e-06  eta: 1 day, 13:12:46  time: 0.4366  data_time: 0.0091  memory: 5224  grad_norm: 63.0467  loss: 9.8839  decode.loss_cls: 0.1737  decode.loss_mask: 0.3333  decode.loss_dice: 0.3811  decode.d0.loss_cls: 1.0092  decode.d0.loss_mask: 0.3314  decode.d0.loss_dice: 0.3583  decode.d1.loss_cls: 0.2861  decode.d1.loss_mask: 0.3201  decode.d1.loss_dice: 0.3459  decode.d2.loss_cls: 0.2605  decode.d2.loss_mask: 0.3186  decode.d2.loss_dice: 0.3633  decode.d3.loss_cls: 0.2111  decode.d3.loss_mask: 0.3222  decode.d3.loss_dice: 0.3764  decode.d4.loss_cls: 0.2597  decode.d4.loss_mask: 0.3197  decode.d4.loss_dice: 0.3622  decode.d5.loss_cls: 0.1883  decode.d5.loss_mask: 0.3216  decode.d5.loss_dice: 0.3653  decode.d6.loss_cls: 0.1760  decode.d6.loss_mask: 0.3276  decode.d6.loss_dice: 0.3677  decode.d7.loss_cls: 0.2432  decode.d7.loss_mask: 0.3202  decode.d7.loss_dice: 0.3662  decode.d8.loss_cls: 0.1771  decode.d8.loss_mask: 0.3227  decode.d8.loss_dice: 0.3750
08/06 03:53:12 - mmengine - INFO - Iter(train) [ 13250/320000]  base_lr: 9.6266e-05 lr: 9.6266e-06  eta: 1 day, 13:12:24  time: 0.4367  data_time: 0.0090  memory: 5260  grad_norm: 141.6575  loss: 12.0576  decode.loss_cls: 0.3386  decode.loss_mask: 0.3327  decode.loss_dice: 0.3541  decode.d0.loss_cls: 1.1537  decode.d0.loss_mask: 0.3449  decode.d0.loss_dice: 0.3891  decode.d1.loss_cls: 0.5438  decode.d1.loss_mask: 0.3446  decode.d1.loss_dice: 0.3970  decode.d2.loss_cls: 0.4223  decode.d2.loss_mask: 0.3486  decode.d2.loss_dice: 0.3654  decode.d3.loss_cls: 0.4585  decode.d3.loss_mask: 0.3335  decode.d3.loss_dice: 0.3547  decode.d4.loss_cls: 0.4155  decode.d4.loss_mask: 0.3218  decode.d4.loss_dice: 0.3451  decode.d5.loss_cls: 0.4673  decode.d5.loss_mask: 0.3168  decode.d5.loss_dice: 0.3508  decode.d6.loss_cls: 0.4842  decode.d6.loss_mask: 0.3240  decode.d6.loss_dice: 0.3356  decode.d7.loss_cls: 0.4424  decode.d7.loss_mask: 0.3316  decode.d7.loss_dice: 0.3439  decode.d8.loss_cls: 0.4012  decode.d8.loss_mask: 0.3325  decode.d8.loss_dice: 0.3636
08/06 03:53:34 - mmengine - INFO - Iter(train) [ 13300/320000]  base_lr: 9.6252e-05 lr: 9.6252e-06  eta: 1 day, 13:12:02  time: 0.4367  data_time: 0.0090  memory: 5224  grad_norm: 76.4795  loss: 8.3296  decode.loss_cls: 0.1991  decode.loss_mask: 0.2324  decode.loss_dice: 0.2720  decode.d0.loss_cls: 1.1298  decode.d0.loss_mask: 0.2378  decode.d0.loss_dice: 0.2913  decode.d1.loss_cls: 0.2962  decode.d1.loss_mask: 0.2321  decode.d1.loss_dice: 0.2593  decode.d2.loss_cls: 0.2678  decode.d2.loss_mask: 0.2315  decode.d2.loss_dice: 0.2569  decode.d3.loss_cls: 0.2298  decode.d3.loss_mask: 0.2365  decode.d3.loss_dice: 0.2722  decode.d4.loss_cls: 0.2445  decode.d4.loss_mask: 0.2390  decode.d4.loss_dice: 0.2726  decode.d5.loss_cls: 0.2087  decode.d5.loss_mask: 0.2367  decode.d5.loss_dice: 0.2727  decode.d6.loss_cls: 0.2484  decode.d6.loss_mask: 0.2363  decode.d6.loss_dice: 0.2706  decode.d7.loss_cls: 0.1861  decode.d7.loss_mask: 0.2359  decode.d7.loss_dice: 0.2868  decode.d8.loss_cls: 0.2407  decode.d8.loss_mask: 0.2319  decode.d8.loss_dice: 0.2740
08/06 03:53:55 - mmengine - INFO - Iter(train) [ 13350/320000]  base_lr: 9.6238e-05 lr: 9.6238e-06  eta: 1 day, 13:11:41  time: 0.4365  data_time: 0.0091  memory: 5242  grad_norm: 85.5438  loss: 9.5461  decode.loss_cls: 0.2281  decode.loss_mask: 0.2853  decode.loss_dice: 0.3614  decode.d0.loss_cls: 0.9830  decode.d0.loss_mask: 0.2997  decode.d0.loss_dice: 0.3893  decode.d1.loss_cls: 0.2487  decode.d1.loss_mask: 0.2878  decode.d1.loss_dice: 0.3440  decode.d2.loss_cls: 0.2319  decode.d2.loss_mask: 0.2822  decode.d2.loss_dice: 0.3275  decode.d3.loss_cls: 0.2834  decode.d3.loss_mask: 0.2822  decode.d3.loss_dice: 0.3508  decode.d4.loss_cls: 0.2316  decode.d4.loss_mask: 0.2772  decode.d4.loss_dice: 0.3264  decode.d5.loss_cls: 0.2386  decode.d5.loss_mask: 0.2830  decode.d5.loss_dice: 0.3336  decode.d6.loss_cls: 0.2431  decode.d6.loss_mask: 0.2889  decode.d6.loss_dice: 0.3591  decode.d7.loss_cls: 0.2591  decode.d7.loss_mask: 0.2845  decode.d7.loss_dice: 0.3566  decode.d8.loss_cls: 0.2778  decode.d8.loss_mask: 0.2840  decode.d8.loss_dice: 0.3175
08/06 03:54:17 - mmengine - INFO - Iter(train) [ 13400/320000]  base_lr: 9.6224e-05 lr: 9.6224e-06  eta: 1 day, 13:11:19  time: 0.4363  data_time: 0.0092  memory: 5242  grad_norm: 87.8544  loss: 8.5665  decode.loss_cls: 0.2673  decode.loss_mask: 0.2634  decode.loss_dice: 0.2735  decode.d0.loss_cls: 0.8763  decode.d0.loss_mask: 0.2590  decode.d0.loss_dice: 0.3074  decode.d1.loss_cls: 0.2788  decode.d1.loss_mask: 0.2599  decode.d1.loss_dice: 0.2723  decode.d2.loss_cls: 0.2414  decode.d2.loss_mask: 0.2623  decode.d2.loss_dice: 0.2943  decode.d3.loss_cls: 0.2564  decode.d3.loss_mask: 0.2604  decode.d3.loss_dice: 0.2731  decode.d4.loss_cls: 0.2459  decode.d4.loss_mask: 0.2631  decode.d4.loss_dice: 0.2664  decode.d5.loss_cls: 0.2571  decode.d5.loss_mask: 0.2570  decode.d5.loss_dice: 0.2797  decode.d6.loss_cls: 0.2404  decode.d6.loss_mask: 0.2628  decode.d6.loss_dice: 0.2665  decode.d7.loss_cls: 0.2600  decode.d7.loss_mask: 0.2588  decode.d7.loss_dice: 0.2768  decode.d8.loss_cls: 0.2376  decode.d8.loss_mask: 0.2642  decode.d8.loss_dice: 0.2845
08/06 03:54:39 - mmengine - INFO - Iter(train) [ 13450/320000]  base_lr: 9.6209e-05 lr: 9.6209e-06  eta: 1 day, 13:10:58  time: 0.4370  data_time: 0.0091  memory: 5260  grad_norm: 98.3581  loss: 10.0801  decode.loss_cls: 0.1908  decode.loss_mask: 0.3630  decode.loss_dice: 0.3363  decode.d0.loss_cls: 1.1035  decode.d0.loss_mask: 0.3494  decode.d0.loss_dice: 0.3557  decode.d1.loss_cls: 0.3647  decode.d1.loss_mask: 0.3211  decode.d1.loss_dice: 0.3051  decode.d2.loss_cls: 0.3883  decode.d2.loss_mask: 0.3066  decode.d2.loss_dice: 0.3081  decode.d3.loss_cls: 0.2897  decode.d3.loss_mask: 0.3159  decode.d3.loss_dice: 0.3146  decode.d4.loss_cls: 0.3077  decode.d4.loss_mask: 0.3136  decode.d4.loss_dice: 0.3083  decode.d5.loss_cls: 0.2813  decode.d5.loss_mask: 0.3070  decode.d5.loss_dice: 0.3115  decode.d6.loss_cls: 0.2611  decode.d6.loss_mask: 0.3227  decode.d6.loss_dice: 0.3084  decode.d7.loss_cls: 0.2011  decode.d7.loss_mask: 0.3405  decode.d7.loss_dice: 0.3159  decode.d8.loss_cls: 0.2323  decode.d8.loss_mask: 0.3415  decode.d8.loss_dice: 0.3142
08/06 03:55:01 - mmengine - INFO - Iter(train) [ 13500/320000]  base_lr: 9.6195e-05 lr: 9.6195e-06  eta: 1 day, 13:10:36  time: 0.4367  data_time: 0.0089  memory: 5242  grad_norm: 130.1778  loss: 8.7455  decode.loss_cls: 0.1871  decode.loss_mask: 0.2733  decode.loss_dice: 0.3010  decode.d0.loss_cls: 0.8938  decode.d0.loss_mask: 0.2711  decode.d0.loss_dice: 0.3578  decode.d1.loss_cls: 0.3441  decode.d1.loss_mask: 0.2673  decode.d1.loss_dice: 0.2940  decode.d2.loss_cls: 0.2548  decode.d2.loss_mask: 0.2806  decode.d2.loss_dice: 0.3046  decode.d3.loss_cls: 0.2238  decode.d3.loss_mask: 0.2750  decode.d3.loss_dice: 0.2845  decode.d4.loss_cls: 0.2418  decode.d4.loss_mask: 0.2698  decode.d4.loss_dice: 0.2859  decode.d5.loss_cls: 0.2246  decode.d5.loss_mask: 0.2742  decode.d5.loss_dice: 0.2773  decode.d6.loss_cls: 0.2204  decode.d6.loss_mask: 0.2709  decode.d6.loss_dice: 0.2968  decode.d7.loss_cls: 0.2174  decode.d7.loss_mask: 0.2740  decode.d7.loss_dice: 0.3137  decode.d8.loss_cls: 0.1959  decode.d8.loss_mask: 0.2676  decode.d8.loss_dice: 0.3023
08/06 03:55:23 - mmengine - INFO - Iter(train) [ 13550/320000]  base_lr: 9.6181e-05 lr: 9.6181e-06  eta: 1 day, 13:10:15  time: 0.4371  data_time: 0.0091  memory: 5240  grad_norm: 104.6589  loss: 8.4249  decode.loss_cls: 0.2899  decode.loss_mask: 0.2339  decode.loss_dice: 0.2931  decode.d0.loss_cls: 0.8610  decode.d0.loss_mask: 0.2319  decode.d0.loss_dice: 0.3192  decode.d1.loss_cls: 0.2468  decode.d1.loss_mask: 0.2348  decode.d1.loss_dice: 0.2786  decode.d2.loss_cls: 0.2719  decode.d2.loss_mask: 0.2306  decode.d2.loss_dice: 0.2978  decode.d3.loss_cls: 0.2883  decode.d3.loss_mask: 0.2330  decode.d3.loss_dice: 0.2672  decode.d4.loss_cls: 0.2778  decode.d4.loss_mask: 0.2380  decode.d4.loss_dice: 0.2792  decode.d5.loss_cls: 0.2323  decode.d5.loss_mask: 0.2336  decode.d5.loss_dice: 0.2745  decode.d6.loss_cls: 0.2503  decode.d6.loss_mask: 0.2298  decode.d6.loss_dice: 0.2731  decode.d7.loss_cls: 0.1967  decode.d7.loss_mask: 0.2353  decode.d7.loss_dice: 0.2821  decode.d8.loss_cls: 0.3384  decode.d8.loss_mask: 0.2323  decode.d8.loss_dice: 0.2734
08/06 03:55:45 - mmengine - INFO - Iter(train) [ 13600/320000]  base_lr: 9.6167e-05 lr: 9.6167e-06  eta: 1 day, 13:09:53  time: 0.4369  data_time: 0.0091  memory: 5260  grad_norm: 100.0036  loss: 10.9074  decode.loss_cls: 0.3969  decode.loss_mask: 0.2227  decode.loss_dice: 0.3449  decode.d0.loss_cls: 1.1122  decode.d0.loss_mask: 0.2335  decode.d0.loss_dice: 0.3905  decode.d1.loss_cls: 0.5884  decode.d1.loss_mask: 0.2329  decode.d1.loss_dice: 0.3440  decode.d2.loss_cls: 0.4044  decode.d2.loss_mask: 0.2373  decode.d2.loss_dice: 0.3694  decode.d3.loss_cls: 0.4307  decode.d3.loss_mask: 0.2227  decode.d3.loss_dice: 0.3217  decode.d4.loss_cls: 0.4958  decode.d4.loss_mask: 0.2265  decode.d4.loss_dice: 0.3201  decode.d5.loss_cls: 0.5028  decode.d5.loss_mask: 0.2225  decode.d5.loss_dice: 0.3704  decode.d6.loss_cls: 0.4071  decode.d6.loss_mask: 0.2266  decode.d6.loss_dice: 0.3703  decode.d7.loss_cls: 0.4317  decode.d7.loss_mask: 0.2219  decode.d7.loss_dice: 0.3397  decode.d8.loss_cls: 0.3648  decode.d8.loss_mask: 0.2269  decode.d8.loss_dice: 0.3281
08/06 03:56:07 - mmengine - INFO - Iter(train) [ 13650/320000]  base_lr: 9.6153e-05 lr: 9.6153e-06  eta: 1 day, 13:09:31  time: 0.4359  data_time: 0.0089  memory: 5260  grad_norm: 76.6069  loss: 8.7788  decode.loss_cls: 0.2491  decode.loss_mask: 0.2401  decode.loss_dice: 0.2727  decode.d0.loss_cls: 1.0735  decode.d0.loss_mask: 0.2495  decode.d0.loss_dice: 0.2819  decode.d1.loss_cls: 0.3568  decode.d1.loss_mask: 0.2420  decode.d1.loss_dice: 0.2820  decode.d2.loss_cls: 0.3053  decode.d2.loss_mask: 0.2408  decode.d2.loss_dice: 0.2806  decode.d3.loss_cls: 0.2956  decode.d3.loss_mask: 0.2461  decode.d3.loss_dice: 0.2717  decode.d4.loss_cls: 0.2564  decode.d4.loss_mask: 0.2423  decode.d4.loss_dice: 0.2679  decode.d5.loss_cls: 0.2531  decode.d5.loss_mask: 0.2427  decode.d5.loss_dice: 0.2672  decode.d6.loss_cls: 0.2948  decode.d6.loss_mask: 0.2500  decode.d6.loss_dice: 0.2770  decode.d7.loss_cls: 0.2514  decode.d7.loss_mask: 0.2401  decode.d7.loss_dice: 0.2728  decode.d8.loss_cls: 0.2549  decode.d8.loss_mask: 0.2423  decode.d8.loss_dice: 0.2779
08/06 03:56:28 - mmengine - INFO - Iter(train) [ 13700/320000]  base_lr: 9.6139e-05 lr: 9.6139e-06  eta: 1 day, 13:09:09  time: 0.4364  data_time: 0.0089  memory: 5242  grad_norm: 121.8184  loss: 9.5145  decode.loss_cls: 0.3385  decode.loss_mask: 0.2133  decode.loss_dice: 0.3179  decode.d0.loss_cls: 0.9178  decode.d0.loss_mask: 0.2314  decode.d0.loss_dice: 0.3726  decode.d1.loss_cls: 0.4139  decode.d1.loss_mask: 0.2277  decode.d1.loss_dice: 0.3435  decode.d2.loss_cls: 0.3928  decode.d2.loss_mask: 0.2112  decode.d2.loss_dice: 0.3311  decode.d3.loss_cls: 0.3019  decode.d3.loss_mask: 0.2360  decode.d3.loss_dice: 0.3429  decode.d4.loss_cls: 0.3334  decode.d4.loss_mask: 0.2572  decode.d4.loss_dice: 0.3537  decode.d5.loss_cls: 0.2921  decode.d5.loss_mask: 0.2163  decode.d5.loss_dice: 0.3209  decode.d6.loss_cls: 0.3493  decode.d6.loss_mask: 0.2119  decode.d6.loss_dice: 0.2886  decode.d7.loss_cls: 0.3324  decode.d7.loss_mask: 0.2080  decode.d7.loss_dice: 0.3070  decode.d8.loss_cls: 0.3023  decode.d8.loss_mask: 0.2234  decode.d8.loss_dice: 0.3254
08/06 03:56:50 - mmengine - INFO - Iter(train) [ 13750/320000]  base_lr: 9.6125e-05 lr: 9.6125e-06  eta: 1 day, 13:08:47  time: 0.4363  data_time: 0.0091  memory: 5208  grad_norm: 148.6613  loss: 11.5382  decode.loss_cls: 0.4117  decode.loss_mask: 0.3148  decode.loss_dice: 0.3142  decode.d0.loss_cls: 1.1130  decode.d0.loss_mask: 0.3497  decode.d0.loss_dice: 0.3824  decode.d1.loss_cls: 0.5165  decode.d1.loss_mask: 0.3097  decode.d1.loss_dice: 0.3311  decode.d2.loss_cls: 0.5109  decode.d2.loss_mask: 0.2933  decode.d2.loss_dice: 0.2933  decode.d3.loss_cls: 0.3967  decode.d3.loss_mask: 0.2994  decode.d3.loss_dice: 0.3056  decode.d4.loss_cls: 0.4576  decode.d4.loss_mask: 0.2899  decode.d4.loss_dice: 0.3289  decode.d5.loss_cls: 0.4057  decode.d5.loss_mask: 0.3070  decode.d5.loss_dice: 0.3233  decode.d6.loss_cls: 0.5051  decode.d6.loss_mask: 0.2968  decode.d6.loss_dice: 0.3114  decode.d7.loss_cls: 0.4870  decode.d7.loss_mask: 0.2986  decode.d7.loss_dice: 0.3036  decode.d8.loss_cls: 0.4645  decode.d8.loss_mask: 0.2994  decode.d8.loss_dice: 0.3172
08/06 03:57:12 - mmengine - INFO - Iter(train) [ 13800/320000]  base_lr: 9.6111e-05 lr: 9.6111e-06  eta: 1 day, 13:08:25  time: 0.4365  data_time: 0.0091  memory: 5242  grad_norm: 118.0661  loss: 7.8206  decode.loss_cls: 0.2677  decode.loss_mask: 0.2075  decode.loss_dice: 0.2760  decode.d0.loss_cls: 0.9299  decode.d0.loss_mask: 0.1895  decode.d0.loss_dice: 0.2361  decode.d1.loss_cls: 0.2397  decode.d1.loss_mask: 0.2007  decode.d1.loss_dice: 0.2395  decode.d2.loss_cls: 0.2887  decode.d2.loss_mask: 0.2047  decode.d2.loss_dice: 0.2513  decode.d3.loss_cls: 0.2456  decode.d3.loss_mask: 0.2023  decode.d3.loss_dice: 0.2715  decode.d4.loss_cls: 0.2998  decode.d4.loss_mask: 0.1894  decode.d4.loss_dice: 0.2418  decode.d5.loss_cls: 0.3150  decode.d5.loss_mask: 0.1907  decode.d5.loss_dice: 0.2300  decode.d6.loss_cls: 0.2690  decode.d6.loss_mask: 0.1939  decode.d6.loss_dice: 0.2359  decode.d7.loss_cls: 0.2877  decode.d7.loss_mask: 0.1928  decode.d7.loss_dice: 0.2390  decode.d8.loss_cls: 0.2345  decode.d8.loss_mask: 0.2079  decode.d8.loss_dice: 0.2427
08/06 03:57:34 - mmengine - INFO - Iter(train) [ 13850/320000]  base_lr: 9.6096e-05 lr: 9.6096e-06  eta: 1 day, 13:08:03  time: 0.4369  data_time: 0.0092  memory: 5258  grad_norm: 112.3769  loss: 10.8356  decode.loss_cls: 0.3113  decode.loss_mask: 0.3168  decode.loss_dice: 0.3516  decode.d0.loss_cls: 0.9613  decode.d0.loss_mask: 0.3380  decode.d0.loss_dice: 0.4112  decode.d1.loss_cls: 0.4109  decode.d1.loss_mask: 0.3184  decode.d1.loss_dice: 0.3524  decode.d2.loss_cls: 0.3300  decode.d2.loss_mask: 0.3227  decode.d2.loss_dice: 0.3412  decode.d3.loss_cls: 0.3354  decode.d3.loss_mask: 0.3210  decode.d3.loss_dice: 0.3366  decode.d4.loss_cls: 0.3610  decode.d4.loss_mask: 0.3195  decode.d4.loss_dice: 0.3561  decode.d5.loss_cls: 0.3436  decode.d5.loss_mask: 0.3148  decode.d5.loss_dice: 0.3580  decode.d6.loss_cls: 0.3352  decode.d6.loss_mask: 0.3120  decode.d6.loss_dice: 0.3529  decode.d7.loss_cls: 0.3495  decode.d7.loss_mask: 0.3177  decode.d7.loss_dice: 0.3528  decode.d8.loss_cls: 0.3416  decode.d8.loss_mask: 0.3126  decode.d8.loss_dice: 0.3494
08/06 03:57:56 - mmengine - INFO - Iter(train) [ 13900/320000]  base_lr: 9.6082e-05 lr: 9.6082e-06  eta: 1 day, 13:07:41  time: 0.4367  data_time: 0.0089  memory: 5242  grad_norm: 52.9073  loss: 6.9249  decode.loss_cls: 0.0867  decode.loss_mask: 0.1943  decode.loss_dice: 0.2900  decode.d0.loss_cls: 0.9200  decode.d0.loss_mask: 0.2049  decode.d0.loss_dice: 0.3006  decode.d1.loss_cls: 0.1817  decode.d1.loss_mask: 0.1970  decode.d1.loss_dice: 0.2918  decode.d2.loss_cls: 0.1473  decode.d2.loss_mask: 0.1923  decode.d2.loss_dice: 0.2997  decode.d3.loss_cls: 0.1563  decode.d3.loss_mask: 0.1936  decode.d3.loss_dice: 0.2876  decode.d4.loss_cls: 0.1468  decode.d4.loss_mask: 0.1938  decode.d4.loss_dice: 0.2870  decode.d5.loss_cls: 0.1351  decode.d5.loss_mask: 0.1938  decode.d5.loss_dice: 0.2830  decode.d6.loss_cls: 0.0838  decode.d6.loss_mask: 0.1950  decode.d6.loss_dice: 0.2948  decode.d7.loss_cls: 0.0751  decode.d7.loss_mask: 0.1915  decode.d7.loss_dice: 0.2862  decode.d8.loss_cls: 0.1261  decode.d8.loss_mask: 0.1935  decode.d8.loss_dice: 0.2959
08/06 03:58:17 - mmengine - INFO - Iter(train) [ 13950/320000]  base_lr: 9.6068e-05 lr: 9.6068e-06  eta: 1 day, 13:07:19  time: 0.4355  data_time: 0.0090  memory: 5224  grad_norm: 159.6105  loss: 11.0187  decode.loss_cls: 0.3833  decode.loss_mask: 0.2948  decode.loss_dice: 0.3477  decode.d0.loss_cls: 1.0863  decode.d0.loss_mask: 0.2799  decode.d0.loss_dice: 0.3670  decode.d1.loss_cls: 0.3667  decode.d1.loss_mask: 0.2977  decode.d1.loss_dice: 0.3441  decode.d2.loss_cls: 0.3250  decode.d2.loss_mask: 0.2972  decode.d2.loss_dice: 0.3699  decode.d3.loss_cls: 0.4163  decode.d3.loss_mask: 0.2828  decode.d3.loss_dice: 0.3503  decode.d4.loss_cls: 0.3417  decode.d4.loss_mask: 0.2801  decode.d4.loss_dice: 0.3699  decode.d5.loss_cls: 0.4526  decode.d5.loss_mask: 0.2922  decode.d5.loss_dice: 0.3511  decode.d6.loss_cls: 0.4106  decode.d6.loss_mask: 0.2937  decode.d6.loss_dice: 0.3360  decode.d7.loss_cls: 0.4174  decode.d7.loss_mask: 0.2895  decode.d7.loss_dice: 0.3375  decode.d8.loss_cls: 0.3846  decode.d8.loss_mask: 0.2900  decode.d8.loss_dice: 0.3624
08/06 03:58:39 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 03:58:39 - mmengine - INFO - Iter(train) [ 14000/320000]  base_lr: 9.6054e-05 lr: 9.6054e-06  eta: 1 day, 13:06:57  time: 0.4363  data_time: 0.0092  memory: 5224  grad_norm: 101.5947  loss: 12.2249  decode.loss_cls: 0.4299  decode.loss_mask: 0.3129  decode.loss_dice: 0.4325  decode.d0.loss_cls: 0.9376  decode.d0.loss_mask: 0.3121  decode.d0.loss_dice: 0.5311  decode.d1.loss_cls: 0.4572  decode.d1.loss_mask: 0.3043  decode.d1.loss_dice: 0.4353  decode.d2.loss_cls: 0.4289  decode.d2.loss_mask: 0.3142  decode.d2.loss_dice: 0.4540  decode.d3.loss_cls: 0.4054  decode.d3.loss_mask: 0.3047  decode.d3.loss_dice: 0.4466  decode.d4.loss_cls: 0.3928  decode.d4.loss_mask: 0.3094  decode.d4.loss_dice: 0.4323  decode.d5.loss_cls: 0.3504  decode.d5.loss_mask: 0.2970  decode.d5.loss_dice: 0.4170  decode.d6.loss_cls: 0.3561  decode.d6.loss_mask: 0.2966  decode.d6.loss_dice: 0.4460  decode.d7.loss_cls: 0.4335  decode.d7.loss_mask: 0.3112  decode.d7.loss_dice: 0.4606  decode.d8.loss_cls: 0.4280  decode.d8.loss_mask: 0.3206  decode.d8.loss_dice: 0.4668
08/06 03:59:01 - mmengine - INFO - Iter(train) [ 14050/320000]  base_lr: 9.6040e-05 lr: 9.6040e-06  eta: 1 day, 13:06:35  time: 0.4371  data_time: 0.0091  memory: 5260  grad_norm: 61.2738  loss: 8.0353  decode.loss_cls: 0.2271  decode.loss_mask: 0.1983  decode.loss_dice: 0.2762  decode.d0.loss_cls: 0.9999  decode.d0.loss_mask: 0.2071  decode.d0.loss_dice: 0.2856  decode.d1.loss_cls: 0.3652  decode.d1.loss_mask: 0.2005  decode.d1.loss_dice: 0.2761  decode.d2.loss_cls: 0.2689  decode.d2.loss_mask: 0.2016  decode.d2.loss_dice: 0.2698  decode.d3.loss_cls: 0.2499  decode.d3.loss_mask: 0.1997  decode.d3.loss_dice: 0.2709  decode.d4.loss_cls: 0.2298  decode.d4.loss_mask: 0.2017  decode.d4.loss_dice: 0.2762  decode.d5.loss_cls: 0.2318  decode.d5.loss_mask: 0.2046  decode.d5.loss_dice: 0.2486  decode.d6.loss_cls: 0.2473  decode.d6.loss_mask: 0.2005  decode.d6.loss_dice: 0.2643  decode.d7.loss_cls: 0.2595  decode.d7.loss_mask: 0.1986  decode.d7.loss_dice: 0.2610  decode.d8.loss_cls: 0.2469  decode.d8.loss_mask: 0.1988  decode.d8.loss_dice: 0.2688
08/06 03:59:23 - mmengine - INFO - Iter(train) [ 14100/320000]  base_lr: 9.6026e-05 lr: 9.6026e-06  eta: 1 day, 13:06:13  time: 0.4361  data_time: 0.0090  memory: 5242  grad_norm: 137.3589  loss: 8.9460  decode.loss_cls: 0.1878  decode.loss_mask: 0.2425  decode.loss_dice: 0.2900  decode.d0.loss_cls: 1.1032  decode.d0.loss_mask: 0.2519  decode.d0.loss_dice: 0.3401  decode.d1.loss_cls: 0.3565  decode.d1.loss_mask: 0.2167  decode.d1.loss_dice: 0.2951  decode.d2.loss_cls: 0.2896  decode.d2.loss_mask: 0.2178  decode.d2.loss_dice: 0.2736  decode.d3.loss_cls: 0.2975  decode.d3.loss_mask: 0.2170  decode.d3.loss_dice: 0.2771  decode.d4.loss_cls: 0.3089  decode.d4.loss_mask: 0.2123  decode.d4.loss_dice: 0.2968  decode.d5.loss_cls: 0.2995  decode.d5.loss_mask: 0.2336  decode.d5.loss_dice: 0.3368  decode.d6.loss_cls: 0.3046  decode.d6.loss_mask: 0.2125  decode.d6.loss_dice: 0.2702  decode.d7.loss_cls: 0.3011  decode.d7.loss_mask: 0.2148  decode.d7.loss_dice: 0.2791  decode.d8.loss_cls: 0.2628  decode.d8.loss_mask: 0.2372  decode.d8.loss_dice: 0.3195
08/06 03:59:45 - mmengine - INFO - Iter(train) [ 14150/320000]  base_lr: 9.6012e-05 lr: 9.6012e-06  eta: 1 day, 13:05:51  time: 0.4360  data_time: 0.0090  memory: 5258  grad_norm: 139.9399  loss: 10.8297  decode.loss_cls: 0.3807  decode.loss_mask: 0.2822  decode.loss_dice: 0.3633  decode.d0.loss_cls: 1.0053  decode.d0.loss_mask: 0.3008  decode.d0.loss_dice: 0.3947  decode.d1.loss_cls: 0.4961  decode.d1.loss_mask: 0.2800  decode.d1.loss_dice: 0.3457  decode.d2.loss_cls: 0.4112  decode.d2.loss_mask: 0.2708  decode.d2.loss_dice: 0.3425  decode.d3.loss_cls: 0.2962  decode.d3.loss_mask: 0.2883  decode.d3.loss_dice: 0.3828  decode.d4.loss_cls: 0.3942  decode.d4.loss_mask: 0.2795  decode.d4.loss_dice: 0.3554  decode.d5.loss_cls: 0.3347  decode.d5.loss_mask: 0.2843  decode.d5.loss_dice: 0.3444  decode.d6.loss_cls: 0.3332  decode.d6.loss_mask: 0.2781  decode.d6.loss_dice: 0.3399  decode.d7.loss_cls: 0.3728  decode.d7.loss_mask: 0.2811  decode.d7.loss_dice: 0.3662  decode.d8.loss_cls: 0.3808  decode.d8.loss_mask: 0.2800  decode.d8.loss_dice: 0.3644
08/06 04:00:07 - mmengine - INFO - Iter(train) [ 14200/320000]  base_lr: 9.5998e-05 lr: 9.5998e-06  eta: 1 day, 13:05:29  time: 0.4356  data_time: 0.0090  memory: 5240  grad_norm: 102.6099  loss: 8.6053  decode.loss_cls: 0.1989  decode.loss_mask: 0.2838  decode.loss_dice: 0.2723  decode.d0.loss_cls: 0.9146  decode.d0.loss_mask: 0.3007  decode.d0.loss_dice: 0.3000  decode.d1.loss_cls: 0.2452  decode.d1.loss_mask: 0.2851  decode.d1.loss_dice: 0.2870  decode.d2.loss_cls: 0.2355  decode.d2.loss_mask: 0.2852  decode.d2.loss_dice: 0.2788  decode.d3.loss_cls: 0.2319  decode.d3.loss_mask: 0.2848  decode.d3.loss_dice: 0.2681  decode.d4.loss_cls: 0.2916  decode.d4.loss_mask: 0.2847  decode.d4.loss_dice: 0.2542  decode.d5.loss_cls: 0.2454  decode.d5.loss_mask: 0.2857  decode.d5.loss_dice: 0.2641  decode.d6.loss_cls: 0.2429  decode.d6.loss_mask: 0.2831  decode.d6.loss_dice: 0.2562  decode.d7.loss_cls: 0.2179  decode.d7.loss_mask: 0.2841  decode.d7.loss_dice: 0.2618  decode.d8.loss_cls: 0.2017  decode.d8.loss_mask: 0.2847  decode.d8.loss_dice: 0.2756
08/06 04:00:28 - mmengine - INFO - Iter(train) [ 14250/320000]  base_lr: 9.5983e-05 lr: 9.5983e-06  eta: 1 day, 13:05:07  time: 0.4358  data_time: 0.0089  memory: 5258  grad_norm: 68.8378  loss: 9.4229  decode.loss_cls: 0.2482  decode.loss_mask: 0.3155  decode.loss_dice: 0.3041  decode.d0.loss_cls: 0.9517  decode.d0.loss_mask: 0.3321  decode.d0.loss_dice: 0.3561  decode.d1.loss_cls: 0.2780  decode.d1.loss_mask: 0.3118  decode.d1.loss_dice: 0.3116  decode.d2.loss_cls: 0.2415  decode.d2.loss_mask: 0.3409  decode.d2.loss_dice: 0.3207  decode.d3.loss_cls: 0.2341  decode.d3.loss_mask: 0.3256  decode.d3.loss_dice: 0.3200  decode.d4.loss_cls: 0.2150  decode.d4.loss_mask: 0.3169  decode.d4.loss_dice: 0.3013  decode.d5.loss_cls: 0.2090  decode.d5.loss_mask: 0.3403  decode.d5.loss_dice: 0.3050  decode.d6.loss_cls: 0.1953  decode.d6.loss_mask: 0.3318  decode.d6.loss_dice: 0.3090  decode.d7.loss_cls: 0.2265  decode.d7.loss_mask: 0.3163  decode.d7.loss_dice: 0.3131  decode.d8.loss_cls: 0.2257  decode.d8.loss_mask: 0.3139  decode.d8.loss_dice: 0.3119
08/06 04:00:50 - mmengine - INFO - Iter(train) [ 14300/320000]  base_lr: 9.5969e-05 lr: 9.5969e-06  eta: 1 day, 13:04:44  time: 0.4355  data_time: 0.0091  memory: 5242  grad_norm: 78.4945  loss: 11.1690  decode.loss_cls: 0.4119  decode.loss_mask: 0.2814  decode.loss_dice: 0.3335  decode.d0.loss_cls: 1.0465  decode.d0.loss_mask: 0.2969  decode.d0.loss_dice: 0.3658  decode.d1.loss_cls: 0.4973  decode.d1.loss_mask: 0.2877  decode.d1.loss_dice: 0.3497  decode.d2.loss_cls: 0.4549  decode.d2.loss_mask: 0.2756  decode.d2.loss_dice: 0.3496  decode.d3.loss_cls: 0.4066  decode.d3.loss_mask: 0.2753  decode.d3.loss_dice: 0.3488  decode.d4.loss_cls: 0.4428  decode.d4.loss_mask: 0.2761  decode.d4.loss_dice: 0.3384  decode.d5.loss_cls: 0.4401  decode.d5.loss_mask: 0.2763  decode.d5.loss_dice: 0.3287  decode.d6.loss_cls: 0.3546  decode.d6.loss_mask: 0.2777  decode.d6.loss_dice: 0.3433  decode.d7.loss_cls: 0.4496  decode.d7.loss_mask: 0.2783  decode.d7.loss_dice: 0.3447  decode.d8.loss_cls: 0.4101  decode.d8.loss_mask: 0.2788  decode.d8.loss_dice: 0.3481
08/06 04:01:12 - mmengine - INFO - Iter(train) [ 14350/320000]  base_lr: 9.5955e-05 lr: 9.5955e-06  eta: 1 day, 13:04:26  time: 0.4359  data_time: 0.0092  memory: 5242  grad_norm: 100.3837  loss: 11.1466  decode.loss_cls: 0.2335  decode.loss_mask: 0.3246  decode.loss_dice: 0.4326  decode.d0.loss_cls: 1.1199  decode.d0.loss_mask: 0.3304  decode.d0.loss_dice: 0.4414  decode.d1.loss_cls: 0.4790  decode.d1.loss_mask: 0.3290  decode.d1.loss_dice: 0.4569  decode.d2.loss_cls: 0.2852  decode.d2.loss_mask: 0.3232  decode.d2.loss_dice: 0.4192  decode.d3.loss_cls: 0.1912  decode.d3.loss_mask: 0.3276  decode.d3.loss_dice: 0.4390  decode.d4.loss_cls: 0.2292  decode.d4.loss_mask: 0.3245  decode.d4.loss_dice: 0.4451  decode.d5.loss_cls: 0.2610  decode.d5.loss_mask: 0.3292  decode.d5.loss_dice: 0.4494  decode.d6.loss_cls: 0.2740  decode.d6.loss_mask: 0.3235  decode.d6.loss_dice: 0.4231  decode.d7.loss_cls: 0.2089  decode.d7.loss_mask: 0.3213  decode.d7.loss_dice: 0.4506  decode.d8.loss_cls: 0.2008  decode.d8.loss_mask: 0.3285  decode.d8.loss_dice: 0.4449
08/06 04:01:34 - mmengine - INFO - Iter(train) [ 14400/320000]  base_lr: 9.5941e-05 lr: 9.5941e-06  eta: 1 day, 13:04:04  time: 0.4378  data_time: 0.0091  memory: 5242  grad_norm: 75.0844  loss: 9.0028  decode.loss_cls: 0.3024  decode.loss_mask: 0.2265  decode.loss_dice: 0.3098  decode.d0.loss_cls: 0.9298  decode.d0.loss_mask: 0.2318  decode.d0.loss_dice: 0.3318  decode.d1.loss_cls: 0.3445  decode.d1.loss_mask: 0.2278  decode.d1.loss_dice: 0.2914  decode.d2.loss_cls: 0.3178  decode.d2.loss_mask: 0.2371  decode.d2.loss_dice: 0.2986  decode.d3.loss_cls: 0.3235  decode.d3.loss_mask: 0.2227  decode.d3.loss_dice: 0.2794  decode.d4.loss_cls: 0.3613  decode.d4.loss_mask: 0.2220  decode.d4.loss_dice: 0.2729  decode.d5.loss_cls: 0.3253  decode.d5.loss_mask: 0.2242  decode.d5.loss_dice: 0.2943  decode.d6.loss_cls: 0.2718  decode.d6.loss_mask: 0.2260  decode.d6.loss_dice: 0.2880  decode.d7.loss_cls: 0.2768  decode.d7.loss_mask: 0.2279  decode.d7.loss_dice: 0.3113  decode.d8.loss_cls: 0.2851  decode.d8.loss_mask: 0.2236  decode.d8.loss_dice: 0.3175
08/06 04:01:56 - mmengine - INFO - Iter(train) [ 14450/320000]  base_lr: 9.5927e-05 lr: 9.5927e-06  eta: 1 day, 13:03:43  time: 0.4366  data_time: 0.0092  memory: 5260  grad_norm: 109.3603  loss: 10.3854  decode.loss_cls: 0.2277  decode.loss_mask: 0.3965  decode.loss_dice: 0.3494  decode.d0.loss_cls: 0.9925  decode.d0.loss_mask: 0.3611  decode.d0.loss_dice: 0.3431  decode.d1.loss_cls: 0.4556  decode.d1.loss_mask: 0.3560  decode.d1.loss_dice: 0.3223  decode.d2.loss_cls: 0.3304  decode.d2.loss_mask: 0.3470  decode.d2.loss_dice: 0.3063  decode.d3.loss_cls: 0.2994  decode.d3.loss_mask: 0.3429  decode.d3.loss_dice: 0.3073  decode.d4.loss_cls: 0.2711  decode.d4.loss_mask: 0.3537  decode.d4.loss_dice: 0.3169  decode.d5.loss_cls: 0.2751  decode.d5.loss_mask: 0.3539  decode.d5.loss_dice: 0.3085  decode.d6.loss_cls: 0.2221  decode.d6.loss_mask: 0.3571  decode.d6.loss_dice: 0.3068  decode.d7.loss_cls: 0.2749  decode.d7.loss_mask: 0.3552  decode.d7.loss_dice: 0.3030  decode.d8.loss_cls: 0.2381  decode.d8.loss_mask: 0.3755  decode.d8.loss_dice: 0.3363
08/06 04:02:18 - mmengine - INFO - Iter(train) [ 14500/320000]  base_lr: 9.5913e-05 lr: 9.5913e-06  eta: 1 day, 13:03:21  time: 0.4362  data_time: 0.0092  memory: 5260  grad_norm: 81.2960  loss: 9.2414  decode.loss_cls: 0.1885  decode.loss_mask: 0.3324  decode.loss_dice: 0.2962  decode.d0.loss_cls: 0.7848  decode.d0.loss_mask: 0.3503  decode.d0.loss_dice: 0.3104  decode.d1.loss_cls: 0.2913  decode.d1.loss_mask: 0.3306  decode.d1.loss_dice: 0.3019  decode.d2.loss_cls: 0.2666  decode.d2.loss_mask: 0.3306  decode.d2.loss_dice: 0.2967  decode.d3.loss_cls: 0.2446  decode.d3.loss_mask: 0.3311  decode.d3.loss_dice: 0.2941  decode.d4.loss_cls: 0.2617  decode.d4.loss_mask: 0.3376  decode.d4.loss_dice: 0.2978  decode.d5.loss_cls: 0.2512  decode.d5.loss_mask: 0.3342  decode.d5.loss_dice: 0.2982  decode.d6.loss_cls: 0.2159  decode.d6.loss_mask: 0.3308  decode.d6.loss_dice: 0.2821  decode.d7.loss_cls: 0.2203  decode.d7.loss_mask: 0.3351  decode.d7.loss_dice: 0.2900  decode.d8.loss_cls: 0.2075  decode.d8.loss_mask: 0.3331  decode.d8.loss_dice: 0.2957
08/06 04:02:40 - mmengine - INFO - Iter(train) [ 14550/320000]  base_lr: 9.5899e-05 lr: 9.5899e-06  eta: 1 day, 13:03:00  time: 0.4366  data_time: 0.0091  memory: 5275  grad_norm: 80.8463  loss: 10.0206  decode.loss_cls: 0.2946  decode.loss_mask: 0.2505  decode.loss_dice: 0.3509  decode.d0.loss_cls: 1.1454  decode.d0.loss_mask: 0.2538  decode.d0.loss_dice: 0.3709  decode.d1.loss_cls: 0.4192  decode.d1.loss_mask: 0.2572  decode.d1.loss_dice: 0.3344  decode.d2.loss_cls: 0.3471  decode.d2.loss_mask: 0.2565  decode.d2.loss_dice: 0.3531  decode.d3.loss_cls: 0.3167  decode.d3.loss_mask: 0.2526  decode.d3.loss_dice: 0.3354  decode.d4.loss_cls: 0.3283  decode.d4.loss_mask: 0.2536  decode.d4.loss_dice: 0.3505  decode.d5.loss_cls: 0.3324  decode.d5.loss_mask: 0.2548  decode.d5.loss_dice: 0.3419  decode.d6.loss_cls: 0.2991  decode.d6.loss_mask: 0.2458  decode.d6.loss_dice: 0.3284  decode.d7.loss_cls: 0.2925  decode.d7.loss_mask: 0.2461  decode.d7.loss_dice: 0.3428  decode.d8.loss_cls: 0.2580  decode.d8.loss_mask: 0.2498  decode.d8.loss_dice: 0.3583
08/06 04:03:01 - mmengine - INFO - Iter(train) [ 14600/320000]  base_lr: 9.5884e-05 lr: 9.5884e-06  eta: 1 day, 13:02:38  time: 0.4370  data_time: 0.0090  memory: 5224  grad_norm: 123.0209  loss: 7.0102  decode.loss_cls: 0.1143  decode.loss_mask: 0.2495  decode.loss_dice: 0.2546  decode.d0.loss_cls: 0.8385  decode.d0.loss_mask: 0.2512  decode.d0.loss_dice: 0.2843  decode.d1.loss_cls: 0.1903  decode.d1.loss_mask: 0.2469  decode.d1.loss_dice: 0.2690  decode.d2.loss_cls: 0.1136  decode.d2.loss_mask: 0.2475  decode.d2.loss_dice: 0.2716  decode.d3.loss_cls: 0.1147  decode.d3.loss_mask: 0.2439  decode.d3.loss_dice: 0.2571  decode.d4.loss_cls: 0.1078  decode.d4.loss_mask: 0.2452  decode.d4.loss_dice: 0.2545  decode.d5.loss_cls: 0.1090  decode.d5.loss_mask: 0.2465  decode.d5.loss_dice: 0.2485  decode.d6.loss_cls: 0.1133  decode.d6.loss_mask: 0.2445  decode.d6.loss_dice: 0.2512  decode.d7.loss_cls: 0.1266  decode.d7.loss_mask: 0.2484  decode.d7.loss_dice: 0.2540  decode.d8.loss_cls: 0.1173  decode.d8.loss_mask: 0.2466  decode.d8.loss_dice: 0.2497
08/06 04:03:23 - mmengine - INFO - Iter(train) [ 14650/320000]  base_lr: 9.5870e-05 lr: 9.5870e-06  eta: 1 day, 13:02:16  time: 0.4364  data_time: 0.0089  memory: 5260  grad_norm: 137.2626  loss: 12.2474  decode.loss_cls: 0.4687  decode.loss_mask: 0.2810  decode.loss_dice: 0.3616  decode.d0.loss_cls: 1.0699  decode.d0.loss_mask: 0.2850  decode.d0.loss_dice: 0.3741  decode.d1.loss_cls: 0.5742  decode.d1.loss_mask: 0.2833  decode.d1.loss_dice: 0.3579  decode.d2.loss_cls: 0.5533  decode.d2.loss_mask: 0.2776  decode.d2.loss_dice: 0.3789  decode.d3.loss_cls: 0.4727  decode.d3.loss_mask: 0.2747  decode.d3.loss_dice: 0.3855  decode.d4.loss_cls: 0.5203  decode.d4.loss_mask: 0.2766  decode.d4.loss_dice: 0.3796  decode.d5.loss_cls: 0.5269  decode.d5.loss_mask: 0.2786  decode.d5.loss_dice: 0.3897  decode.d6.loss_cls: 0.4875  decode.d6.loss_mask: 0.2872  decode.d6.loss_dice: 0.3626  decode.d7.loss_cls: 0.4828  decode.d7.loss_mask: 0.2893  decode.d7.loss_dice: 0.3481  decode.d8.loss_cls: 0.5268  decode.d8.loss_mask: 0.2849  decode.d8.loss_dice: 0.4082
08/06 04:03:45 - mmengine - INFO - Iter(train) [ 14700/320000]  base_lr: 9.5856e-05 lr: 9.5856e-06  eta: 1 day, 13:01:55  time: 0.4364  data_time: 0.0089  memory: 5240  grad_norm: 66.3484  loss: 8.8044  decode.loss_cls: 0.1853  decode.loss_mask: 0.2816  decode.loss_dice: 0.3437  decode.d0.loss_cls: 1.0448  decode.d0.loss_mask: 0.3024  decode.d0.loss_dice: 0.3815  decode.d1.loss_cls: 0.2330  decode.d1.loss_mask: 0.2853  decode.d1.loss_dice: 0.3465  decode.d2.loss_cls: 0.1660  decode.d2.loss_mask: 0.2805  decode.d2.loss_dice: 0.3419  decode.d3.loss_cls: 0.1693  decode.d3.loss_mask: 0.2807  decode.d3.loss_dice: 0.3250  decode.d4.loss_cls: 0.1730  decode.d4.loss_mask: 0.2766  decode.d4.loss_dice: 0.3359  decode.d5.loss_cls: 0.1486  decode.d5.loss_mask: 0.2769  decode.d5.loss_dice: 0.3255  decode.d6.loss_cls: 0.1330  decode.d6.loss_mask: 0.2799  decode.d6.loss_dice: 0.3185  decode.d7.loss_cls: 0.1676  decode.d7.loss_mask: 0.2838  decode.d7.loss_dice: 0.3493  decode.d8.loss_cls: 0.1604  decode.d8.loss_mask: 0.2809  decode.d8.loss_dice: 0.3271
08/06 04:04:07 - mmengine - INFO - Iter(train) [ 14750/320000]  base_lr: 9.5842e-05 lr: 9.5842e-06  eta: 1 day, 13:01:33  time: 0.4358  data_time: 0.0090  memory: 5208  grad_norm: 119.5661  loss: 9.0269  decode.loss_cls: 0.3112  decode.loss_mask: 0.2594  decode.loss_dice: 0.2961  decode.d0.loss_cls: 0.9092  decode.d0.loss_mask: 0.2705  decode.d0.loss_dice: 0.2993  decode.d1.loss_cls: 0.3651  decode.d1.loss_mask: 0.2579  decode.d1.loss_dice: 0.2876  decode.d2.loss_cls: 0.2784  decode.d2.loss_mask: 0.2568  decode.d2.loss_dice: 0.2758  decode.d3.loss_cls: 0.2552  decode.d3.loss_mask: 0.2554  decode.d3.loss_dice: 0.2774  decode.d4.loss_cls: 0.3205  decode.d4.loss_mask: 0.2561  decode.d4.loss_dice: 0.2723  decode.d5.loss_cls: 0.2458  decode.d5.loss_mask: 0.2609  decode.d5.loss_dice: 0.2680  decode.d6.loss_cls: 0.3096  decode.d6.loss_mask: 0.2560  decode.d6.loss_dice: 0.2747  decode.d7.loss_cls: 0.3037  decode.d7.loss_mask: 0.2658  decode.d7.loss_dice: 0.2948  decode.d8.loss_cls: 0.3067  decode.d8.loss_mask: 0.2570  decode.d8.loss_dice: 0.2796
08/06 04:04:29 - mmengine - INFO - Iter(train) [ 14800/320000]  base_lr: 9.5828e-05 lr: 9.5828e-06  eta: 1 day, 13:01:11  time: 0.4376  data_time: 0.0091  memory: 5260  grad_norm: 110.5685  loss: 9.1933  decode.loss_cls: 0.2638  decode.loss_mask: 0.2729  decode.loss_dice: 0.2891  decode.d0.loss_cls: 1.0256  decode.d0.loss_mask: 0.2898  decode.d0.loss_dice: 0.3385  decode.d1.loss_cls: 0.4167  decode.d1.loss_mask: 0.2615  decode.d1.loss_dice: 0.3029  decode.d2.loss_cls: 0.2990  decode.d2.loss_mask: 0.2696  decode.d2.loss_dice: 0.2884  decode.d3.loss_cls: 0.2128  decode.d3.loss_mask: 0.2809  decode.d3.loss_dice: 0.2988  decode.d4.loss_cls: 0.2239  decode.d4.loss_mask: 0.2805  decode.d4.loss_dice: 0.2809  decode.d5.loss_cls: 0.1895  decode.d5.loss_mask: 0.2815  decode.d5.loss_dice: 0.3012  decode.d6.loss_cls: 0.2038  decode.d6.loss_mask: 0.3075  decode.d6.loss_dice: 0.3194  decode.d7.loss_cls: 0.2399  decode.d7.loss_mask: 0.3239  decode.d7.loss_dice: 0.3087  decode.d8.loss_cls: 0.2398  decode.d8.loss_mask: 0.2842  decode.d8.loss_dice: 0.2984
08/06 04:04:51 - mmengine - INFO - Iter(train) [ 14850/320000]  base_lr: 9.5814e-05 lr: 9.5814e-06  eta: 1 day, 13:00:49  time: 0.4371  data_time: 0.0089  memory: 5260  grad_norm: 79.8044  loss: 9.2574  decode.loss_cls: 0.2779  decode.loss_mask: 0.2533  decode.loss_dice: 0.3537  decode.d0.loss_cls: 0.9682  decode.d0.loss_mask: 0.2279  decode.d0.loss_dice: 0.3608  decode.d1.loss_cls: 0.3286  decode.d1.loss_mask: 0.2183  decode.d1.loss_dice: 0.3126  decode.d2.loss_cls: 0.2349  decode.d2.loss_mask: 0.2200  decode.d2.loss_dice: 0.3191  decode.d3.loss_cls: 0.2930  decode.d3.loss_mask: 0.2231  decode.d3.loss_dice: 0.3305  decode.d4.loss_cls: 0.2942  decode.d4.loss_mask: 0.2324  decode.d4.loss_dice: 0.3355  decode.d5.loss_cls: 0.3739  decode.d5.loss_mask: 0.2382  decode.d5.loss_dice: 0.3369  decode.d6.loss_cls: 0.2349  decode.d6.loss_mask: 0.2333  decode.d6.loss_dice: 0.3321  decode.d7.loss_cls: 0.2574  decode.d7.loss_mask: 0.2474  decode.d7.loss_dice: 0.3413  decode.d8.loss_cls: 0.2955  decode.d8.loss_mask: 0.2422  decode.d8.loss_dice: 0.3404
08/06 04:05:13 - mmengine - INFO - Iter(train) [ 14900/320000]  base_lr: 9.5800e-05 lr: 9.5800e-06  eta: 1 day, 13:00:28  time: 0.4357  data_time: 0.0088  memory: 5260  grad_norm: 115.2922  loss: 7.7430  decode.loss_cls: 0.1338  decode.loss_mask: 0.3032  decode.loss_dice: 0.2455  decode.d0.loss_cls: 0.9111  decode.d0.loss_mask: 0.3150  decode.d0.loss_dice: 0.2596  decode.d1.loss_cls: 0.1639  decode.d1.loss_mask: 0.2975  decode.d1.loss_dice: 0.2481  decode.d2.loss_cls: 0.1668  decode.d2.loss_mask: 0.3024  decode.d2.loss_dice: 0.2449  decode.d3.loss_cls: 0.1465  decode.d3.loss_mask: 0.3014  decode.d3.loss_dice: 0.2540  decode.d4.loss_cls: 0.1297  decode.d4.loss_mask: 0.3027  decode.d4.loss_dice: 0.2530  decode.d5.loss_cls: 0.1753  decode.d5.loss_mask: 0.2962  decode.d5.loss_dice: 0.2451  decode.d6.loss_cls: 0.1221  decode.d6.loss_mask: 0.2995  decode.d6.loss_dice: 0.2486  decode.d7.loss_cls: 0.1264  decode.d7.loss_mask: 0.2990  decode.d7.loss_dice: 0.2418  decode.d8.loss_cls: 0.1547  decode.d8.loss_mask: 0.3047  decode.d8.loss_dice: 0.2504
08/06 04:05:34 - mmengine - INFO - Iter(train) [ 14950/320000]  base_lr: 9.5786e-05 lr: 9.5786e-06  eta: 1 day, 13:00:06  time: 0.4361  data_time: 0.0092  memory: 5275  grad_norm: 111.6897  loss: 10.3856  decode.loss_cls: 0.2792  decode.loss_mask: 0.3410  decode.loss_dice: 0.3187  decode.d0.loss_cls: 0.8478  decode.d0.loss_mask: 0.3506  decode.d0.loss_dice: 0.3619  decode.d1.loss_cls: 0.4776  decode.d1.loss_mask: 0.3308  decode.d1.loss_dice: 0.3364  decode.d2.loss_cls: 0.3129  decode.d2.loss_mask: 0.3311  decode.d2.loss_dice: 0.3430  decode.d3.loss_cls: 0.3494  decode.d3.loss_mask: 0.3186  decode.d3.loss_dice: 0.3147  decode.d4.loss_cls: 0.3915  decode.d4.loss_mask: 0.3397  decode.d4.loss_dice: 0.3454  decode.d5.loss_cls: 0.3228  decode.d5.loss_mask: 0.3458  decode.d5.loss_dice: 0.3473  decode.d6.loss_cls: 0.2327  decode.d6.loss_mask: 0.3348  decode.d6.loss_dice: 0.2907  decode.d7.loss_cls: 0.2459  decode.d7.loss_mask: 0.3345  decode.d7.loss_dice: 0.3186  decode.d8.loss_cls: 0.2512  decode.d8.loss_mask: 0.3397  decode.d8.loss_dice: 0.3314
08/06 04:05:56 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 04:05:56 - mmengine - INFO - Iter(train) [ 15000/320000]  base_lr: 9.5771e-05 lr: 9.5771e-06  eta: 1 day, 12:59:44  time: 0.4365  data_time: 0.0089  memory: 5242  grad_norm: 163.5441  loss: 11.2556  decode.loss_cls: 0.3738  decode.loss_mask: 0.2523  decode.loss_dice: 0.3932  decode.d0.loss_cls: 1.1234  decode.d0.loss_mask: 0.2606  decode.d0.loss_dice: 0.4028  decode.d1.loss_cls: 0.5132  decode.d1.loss_mask: 0.2458  decode.d1.loss_dice: 0.4039  decode.d2.loss_cls: 0.4798  decode.d2.loss_mask: 0.2427  decode.d2.loss_dice: 0.3986  decode.d3.loss_cls: 0.4034  decode.d3.loss_mask: 0.2453  decode.d3.loss_dice: 0.3917  decode.d4.loss_cls: 0.4104  decode.d4.loss_mask: 0.2509  decode.d4.loss_dice: 0.4047  decode.d5.loss_cls: 0.4151  decode.d5.loss_mask: 0.2506  decode.d5.loss_dice: 0.3860  decode.d6.loss_cls: 0.3873  decode.d6.loss_mask: 0.2501  decode.d6.loss_dice: 0.4050  decode.d7.loss_cls: 0.3409  decode.d7.loss_mask: 0.2511  decode.d7.loss_dice: 0.3931  decode.d8.loss_cls: 0.3918  decode.d8.loss_mask: 0.2471  decode.d8.loss_dice: 0.3409
08/06 04:06:18 - mmengine - INFO - Iter(train) [ 15050/320000]  base_lr: 9.5757e-05 lr: 9.5757e-06  eta: 1 day, 12:59:22  time: 0.4365  data_time: 0.0092  memory: 5224  grad_norm: 110.9474  loss: 9.8764  decode.loss_cls: 0.2848  decode.loss_mask: 0.2756  decode.loss_dice: 0.3072  decode.d0.loss_cls: 1.0420  decode.d0.loss_mask: 0.2951  decode.d0.loss_dice: 0.2999  decode.d1.loss_cls: 0.3720  decode.d1.loss_mask: 0.2690  decode.d1.loss_dice: 0.2861  decode.d2.loss_cls: 0.3614  decode.d2.loss_mask: 0.2654  decode.d2.loss_dice: 0.2916  decode.d3.loss_cls: 0.3703  decode.d3.loss_mask: 0.2832  decode.d3.loss_dice: 0.2960  decode.d4.loss_cls: 0.3145  decode.d4.loss_mask: 0.3020  decode.d4.loss_dice: 0.3018  decode.d5.loss_cls: 0.3410  decode.d5.loss_mask: 0.3042  decode.d5.loss_dice: 0.3120  decode.d6.loss_cls: 0.3743  decode.d6.loss_mask: 0.2783  decode.d6.loss_dice: 0.2946  decode.d7.loss_cls: 0.3491  decode.d7.loss_mask: 0.2705  decode.d7.loss_dice: 0.2837  decode.d8.loss_cls: 0.2784  decode.d8.loss_mask: 0.2801  decode.d8.loss_dice: 0.2924
08/06 04:06:40 - mmengine - INFO - Iter(train) [ 15100/320000]  base_lr: 9.5743e-05 lr: 9.5743e-06  eta: 1 day, 12:59:01  time: 0.4374  data_time: 0.0092  memory: 5260  grad_norm: 62.7570  loss: 10.2502  decode.loss_cls: 0.2984  decode.loss_mask: 0.2917  decode.loss_dice: 0.3526  decode.d0.loss_cls: 1.0482  decode.d0.loss_mask: 0.2631  decode.d0.loss_dice: 0.3541  decode.d1.loss_cls: 0.3857  decode.d1.loss_mask: 0.2807  decode.d1.loss_dice: 0.3536  decode.d2.loss_cls: 0.3021  decode.d2.loss_mask: 0.2932  decode.d2.loss_dice: 0.3468  decode.d3.loss_cls: 0.3884  decode.d3.loss_mask: 0.2660  decode.d3.loss_dice: 0.3281  decode.d4.loss_cls: 0.2861  decode.d4.loss_mask: 0.2979  decode.d4.loss_dice: 0.3501  decode.d5.loss_cls: 0.3856  decode.d5.loss_mask: 0.2642  decode.d5.loss_dice: 0.3442  decode.d6.loss_cls: 0.3072  decode.d6.loss_mask: 0.2816  decode.d6.loss_dice: 0.3436  decode.d7.loss_cls: 0.2981  decode.d7.loss_mask: 0.2814  decode.d7.loss_dice: 0.3463  decode.d8.loss_cls: 0.2969  decode.d8.loss_mask: 0.2787  decode.d8.loss_dice: 0.3357
08/06 04:07:02 - mmengine - INFO - Iter(train) [ 15150/320000]  base_lr: 9.5729e-05 lr: 9.5729e-06  eta: 1 day, 12:58:39  time: 0.4369  data_time: 0.0092  memory: 5275  grad_norm: 117.8040  loss: 6.9097  decode.loss_cls: 0.0679  decode.loss_mask: 0.2497  decode.loss_dice: 0.2484  decode.d0.loss_cls: 1.0523  decode.d0.loss_mask: 0.2453  decode.d0.loss_dice: 0.2571  decode.d1.loss_cls: 0.1188  decode.d1.loss_mask: 0.2526  decode.d1.loss_dice: 0.2611  decode.d2.loss_cls: 0.0786  decode.d2.loss_mask: 0.2475  decode.d2.loss_dice: 0.2544  decode.d3.loss_cls: 0.1005  decode.d3.loss_mask: 0.2530  decode.d3.loss_dice: 0.2571  decode.d4.loss_cls: 0.0811  decode.d4.loss_mask: 0.2452  decode.d4.loss_dice: 0.2495  decode.d5.loss_cls: 0.0767  decode.d5.loss_mask: 0.2519  decode.d5.loss_dice: 0.2497  decode.d6.loss_cls: 0.0806  decode.d6.loss_mask: 0.2499  decode.d6.loss_dice: 0.2461  decode.d7.loss_cls: 0.1074  decode.d7.loss_mask: 0.2543  decode.d7.loss_dice: 0.2504  decode.d8.loss_cls: 0.1290  decode.d8.loss_mask: 0.2478  decode.d8.loss_dice: 0.2460
08/06 04:07:24 - mmengine - INFO - Iter(train) [ 15200/320000]  base_lr: 9.5715e-05 lr: 9.5715e-06  eta: 1 day, 12:58:18  time: 0.4360  data_time: 0.0090  memory: 5275  grad_norm: 157.3474  loss: 11.9951  decode.loss_cls: 0.3893  decode.loss_mask: 0.3038  decode.loss_dice: 0.3662  decode.d0.loss_cls: 1.0910  decode.d0.loss_mask: 0.3112  decode.d0.loss_dice: 0.3587  decode.d1.loss_cls: 0.6868  decode.d1.loss_mask: 0.2632  decode.d1.loss_dice: 0.3151  decode.d2.loss_cls: 0.4823  decode.d2.loss_mask: 0.2710  decode.d2.loss_dice: 0.3582  decode.d3.loss_cls: 0.3874  decode.d3.loss_mask: 0.2799  decode.d3.loss_dice: 0.3393  decode.d4.loss_cls: 0.5009  decode.d4.loss_mask: 0.2965  decode.d4.loss_dice: 0.3517  decode.d5.loss_cls: 0.5103  decode.d5.loss_mask: 0.3426  decode.d5.loss_dice: 0.3705  decode.d6.loss_cls: 0.4410  decode.d6.loss_mask: 0.3211  decode.d6.loss_dice: 0.4009  decode.d7.loss_cls: 0.4680  decode.d7.loss_mask: 0.2961  decode.d7.loss_dice: 0.3654  decode.d8.loss_cls: 0.4199  decode.d8.loss_mask: 0.3162  decode.d8.loss_dice: 0.3909
08/06 04:07:45 - mmengine - INFO - Iter(train) [ 15250/320000]  base_lr: 9.5701e-05 lr: 9.5701e-06  eta: 1 day, 12:57:56  time: 0.4362  data_time: 0.0089  memory: 5242  grad_norm: 107.7770  loss: 6.9908  decode.loss_cls: 0.1335  decode.loss_mask: 0.2059  decode.loss_dice: 0.2987  decode.d0.loss_cls: 0.8576  decode.d0.loss_mask: 0.2152  decode.d0.loss_dice: 0.2934  decode.d1.loss_cls: 0.2436  decode.d1.loss_mask: 0.2085  decode.d1.loss_dice: 0.2684  decode.d2.loss_cls: 0.1422  decode.d2.loss_mask: 0.2113  decode.d2.loss_dice: 0.2768  decode.d3.loss_cls: 0.0585  decode.d3.loss_mask: 0.2183  decode.d3.loss_dice: 0.2829  decode.d4.loss_cls: 0.1071  decode.d4.loss_mask: 0.2206  decode.d4.loss_dice: 0.2987  decode.d5.loss_cls: 0.0732  decode.d5.loss_mask: 0.2204  decode.d5.loss_dice: 0.3012  decode.d6.loss_cls: 0.0761  decode.d6.loss_mask: 0.2145  decode.d6.loss_dice: 0.3051  decode.d7.loss_cls: 0.1387  decode.d7.loss_mask: 0.1975  decode.d7.loss_dice: 0.2758  decode.d8.loss_cls: 0.1708  decode.d8.loss_mask: 0.2015  decode.d8.loss_dice: 0.2748
08/06 04:08:07 - mmengine - INFO - Iter(train) [ 15300/320000]  base_lr: 9.5687e-05 lr: 9.5687e-06  eta: 1 day, 12:57:35  time: 0.4371  data_time: 0.0091  memory: 5223  grad_norm: 75.6503  loss: 8.1104  decode.loss_cls: 0.1934  decode.loss_mask: 0.2499  decode.loss_dice: 0.2691  decode.d0.loss_cls: 0.8219  decode.d0.loss_mask: 0.2505  decode.d0.loss_dice: 0.3052  decode.d1.loss_cls: 0.1950  decode.d1.loss_mask: 0.2513  decode.d1.loss_dice: 0.2849  decode.d2.loss_cls: 0.2593  decode.d2.loss_mask: 0.2511  decode.d2.loss_dice: 0.3240  decode.d3.loss_cls: 0.1981  decode.d3.loss_mask: 0.2469  decode.d3.loss_dice: 0.2774  decode.d4.loss_cls: 0.1512  decode.d4.loss_mask: 0.2502  decode.d4.loss_dice: 0.3011  decode.d5.loss_cls: 0.1820  decode.d5.loss_mask: 0.2506  decode.d5.loss_dice: 0.3184  decode.d6.loss_cls: 0.1935  decode.d6.loss_mask: 0.2478  decode.d6.loss_dice: 0.2924  decode.d7.loss_cls: 0.2143  decode.d7.loss_mask: 0.2452  decode.d7.loss_dice: 0.3153  decode.d8.loss_cls: 0.2073  decode.d8.loss_mask: 0.2497  decode.d8.loss_dice: 0.3134
08/06 04:08:29 - mmengine - INFO - Iter(train) [ 15350/320000]  base_lr: 9.5673e-05 lr: 9.5673e-06  eta: 1 day, 12:57:13  time: 0.4370  data_time: 0.0092  memory: 5224  grad_norm: 311.5333  loss: 12.2970  decode.loss_cls: 0.3444  decode.loss_mask: 0.3350  decode.loss_dice: 0.4480  decode.d0.loss_cls: 1.2183  decode.d0.loss_mask: 0.3801  decode.d0.loss_dice: 0.5037  decode.d1.loss_cls: 0.4623  decode.d1.loss_mask: 0.3178  decode.d1.loss_dice: 0.4501  decode.d2.loss_cls: 0.3403  decode.d2.loss_mask: 0.3211  decode.d2.loss_dice: 0.4571  decode.d3.loss_cls: 0.3241  decode.d3.loss_mask: 0.3135  decode.d3.loss_dice: 0.4351  decode.d4.loss_cls: 0.3257  decode.d4.loss_mask: 0.3148  decode.d4.loss_dice: 0.4577  decode.d5.loss_cls: 0.3375  decode.d5.loss_mask: 0.3142  decode.d5.loss_dice: 0.4533  decode.d6.loss_cls: 0.3845  decode.d6.loss_mask: 0.3152  decode.d6.loss_dice: 0.4719  decode.d7.loss_cls: 0.4145  decode.d7.loss_mask: 0.3155  decode.d7.loss_dice: 0.4578  decode.d8.loss_cls: 0.3150  decode.d8.loss_mask: 0.3166  decode.d8.loss_dice: 0.4519
08/06 04:08:51 - mmengine - INFO - Iter(train) [ 15400/320000]  base_lr: 9.5658e-05 lr: 9.5658e-06  eta: 1 day, 12:56:52  time: 0.4370  data_time: 0.0091  memory: 5275  grad_norm: 127.1651  loss: 11.4156  decode.loss_cls: 0.4623  decode.loss_mask: 0.2791  decode.loss_dice: 0.3910  decode.d0.loss_cls: 1.0944  decode.d0.loss_mask: 0.2790  decode.d0.loss_dice: 0.3893  decode.d1.loss_cls: 0.5274  decode.d1.loss_mask: 0.2188  decode.d1.loss_dice: 0.3086  decode.d2.loss_cls: 0.4406  decode.d2.loss_mask: 0.2517  decode.d2.loss_dice: 0.3435  decode.d3.loss_cls: 0.3935  decode.d3.loss_mask: 0.2618  decode.d3.loss_dice: 0.3809  decode.d4.loss_cls: 0.4535  decode.d4.loss_mask: 0.2622  decode.d4.loss_dice: 0.3544  decode.d5.loss_cls: 0.4396  decode.d5.loss_mask: 0.2595  decode.d5.loss_dice: 0.3526  decode.d6.loss_cls: 0.4991  decode.d6.loss_mask: 0.2323  decode.d6.loss_dice: 0.3076  decode.d7.loss_cls: 0.4511  decode.d7.loss_mask: 0.2881  decode.d7.loss_dice: 0.3604  decode.d8.loss_cls: 0.5032  decode.d8.loss_mask: 0.2539  decode.d8.loss_dice: 0.3762
08/06 04:09:13 - mmengine - INFO - Iter(train) [ 15450/320000]  base_lr: 9.5644e-05 lr: 9.5644e-06  eta: 1 day, 12:56:30  time: 0.4373  data_time: 0.0093  memory: 5242  grad_norm: 80.7945  loss: 10.1817  decode.loss_cls: 0.3519  decode.loss_mask: 0.2848  decode.loss_dice: 0.2867  decode.d0.loss_cls: 1.0628  decode.d0.loss_mask: 0.2957  decode.d0.loss_dice: 0.3120  decode.d1.loss_cls: 0.3976  decode.d1.loss_mask: 0.2816  decode.d1.loss_dice: 0.2957  decode.d2.loss_cls: 0.3318  decode.d2.loss_mask: 0.2863  decode.d2.loss_dice: 0.2824  decode.d3.loss_cls: 0.3348  decode.d3.loss_mask: 0.2835  decode.d3.loss_dice: 0.2921  decode.d4.loss_cls: 0.3838  decode.d4.loss_mask: 0.2837  decode.d4.loss_dice: 0.2919  decode.d5.loss_cls: 0.3538  decode.d5.loss_mask: 0.2765  decode.d5.loss_dice: 0.3062  decode.d6.loss_cls: 0.3745  decode.d6.loss_mask: 0.2797  decode.d6.loss_dice: 0.3202  decode.d7.loss_cls: 0.3867  decode.d7.loss_mask: 0.2842  decode.d7.loss_dice: 0.3025  decode.d8.loss_cls: 0.3695  decode.d8.loss_mask: 0.2842  decode.d8.loss_dice: 0.3046
08/06 04:09:35 - mmengine - INFO - Iter(train) [ 15500/320000]  base_lr: 9.5630e-05 lr: 9.5630e-06  eta: 1 day, 12:56:08  time: 0.4378  data_time: 0.0090  memory: 5260  grad_norm: 64.0613  loss: 9.3321  decode.loss_cls: 0.2685  decode.loss_mask: 0.1994  decode.loss_dice: 0.3837  decode.d0.loss_cls: 0.9875  decode.d0.loss_mask: 0.2047  decode.d0.loss_dice: 0.3999  decode.d1.loss_cls: 0.3124  decode.d1.loss_mask: 0.1995  decode.d1.loss_dice: 0.3655  decode.d2.loss_cls: 0.2877  decode.d2.loss_mask: 0.2028  decode.d2.loss_dice: 0.3868  decode.d3.loss_cls: 0.2872  decode.d3.loss_mask: 0.1993  decode.d3.loss_dice: 0.3532  decode.d4.loss_cls: 0.2744  decode.d4.loss_mask: 0.1999  decode.d4.loss_dice: 0.3611  decode.d5.loss_cls: 0.2893  decode.d5.loss_mask: 0.2007  decode.d5.loss_dice: 0.3743  decode.d6.loss_cls: 0.2606  decode.d6.loss_mask: 0.2049  decode.d6.loss_dice: 0.3907  decode.d7.loss_cls: 0.2804  decode.d7.loss_mask: 0.2016  decode.d7.loss_dice: 0.3730  decode.d8.loss_cls: 0.2694  decode.d8.loss_mask: 0.2035  decode.d8.loss_dice: 0.4098
08/06 04:09:57 - mmengine - INFO - Iter(train) [ 15550/320000]  base_lr: 9.5616e-05 lr: 9.5616e-06  eta: 1 day, 12:55:46  time: 0.4360  data_time: 0.0091  memory: 5260  grad_norm: 138.4016  loss: 10.5391  decode.loss_cls: 0.2492  decode.loss_mask: 0.2801  decode.loss_dice: 0.3881  decode.d0.loss_cls: 1.0245  decode.d0.loss_mask: 0.2992  decode.d0.loss_dice: 0.4275  decode.d1.loss_cls: 0.4416  decode.d1.loss_mask: 0.2829  decode.d1.loss_dice: 0.3765  decode.d2.loss_cls: 0.2758  decode.d2.loss_mask: 0.2868  decode.d2.loss_dice: 0.4108  decode.d3.loss_cls: 0.3415  decode.d3.loss_mask: 0.2795  decode.d3.loss_dice: 0.3756  decode.d4.loss_cls: 0.3337  decode.d4.loss_mask: 0.2842  decode.d4.loss_dice: 0.3963  decode.d5.loss_cls: 0.2799  decode.d5.loss_mask: 0.2816  decode.d5.loss_dice: 0.3794  decode.d6.loss_cls: 0.2836  decode.d6.loss_mask: 0.2773  decode.d6.loss_dice: 0.3797  decode.d7.loss_cls: 0.2896  decode.d7.loss_mask: 0.2839  decode.d7.loss_dice: 0.4066  decode.d8.loss_cls: 0.2311  decode.d8.loss_mask: 0.2816  decode.d8.loss_dice: 0.4109
08/06 04:10:18 - mmengine - INFO - Iter(train) [ 15600/320000]  base_lr: 9.5602e-05 lr: 9.5602e-06  eta: 1 day, 12:55:25  time: 0.4364  data_time: 0.0093  memory: 5242  grad_norm: 73.9266  loss: 9.1320  decode.loss_cls: 0.3184  decode.loss_mask: 0.2223  decode.loss_dice: 0.3117  decode.d0.loss_cls: 0.9761  decode.d0.loss_mask: 0.2216  decode.d0.loss_dice: 0.3533  decode.d1.loss_cls: 0.3101  decode.d1.loss_mask: 0.2162  decode.d1.loss_dice: 0.3138  decode.d2.loss_cls: 0.2964  decode.d2.loss_mask: 0.2165  decode.d2.loss_dice: 0.3021  decode.d3.loss_cls: 0.3222  decode.d3.loss_mask: 0.2187  decode.d3.loss_dice: 0.3174  decode.d4.loss_cls: 0.2572  decode.d4.loss_mask: 0.2168  decode.d4.loss_dice: 0.3069  decode.d5.loss_cls: 0.3286  decode.d5.loss_mask: 0.2158  decode.d5.loss_dice: 0.3204  decode.d6.loss_cls: 0.2944  decode.d6.loss_mask: 0.2154  decode.d6.loss_dice: 0.3261  decode.d7.loss_cls: 0.3268  decode.d7.loss_mask: 0.2212  decode.d7.loss_dice: 0.2941  decode.d8.loss_cls: 0.3507  decode.d8.loss_mask: 0.2195  decode.d8.loss_dice: 0.3211
08/06 04:10:40 - mmengine - INFO - Iter(train) [ 15650/320000]  base_lr: 9.5588e-05 lr: 9.5588e-06  eta: 1 day, 12:55:03  time: 0.4364  data_time: 0.0094  memory: 5275  grad_norm: 120.5566  loss: 10.2339  decode.loss_cls: 0.2628  decode.loss_mask: 0.3261  decode.loss_dice: 0.3077  decode.d0.loss_cls: 0.7995  decode.d0.loss_mask: 0.3469  decode.d0.loss_dice: 0.3152  decode.d1.loss_cls: 0.4131  decode.d1.loss_mask: 0.3278  decode.d1.loss_dice: 0.3044  decode.d2.loss_cls: 0.3079  decode.d2.loss_mask: 0.3271  decode.d2.loss_dice: 0.3046  decode.d3.loss_cls: 0.3702  decode.d3.loss_mask: 0.3225  decode.d3.loss_dice: 0.3072  decode.d4.loss_cls: 0.3643  decode.d4.loss_mask: 0.3205  decode.d4.loss_dice: 0.3109  decode.d5.loss_cls: 0.3712  decode.d5.loss_mask: 0.3188  decode.d5.loss_dice: 0.3085  decode.d6.loss_cls: 0.3850  decode.d6.loss_mask: 0.3241  decode.d6.loss_dice: 0.3127  decode.d7.loss_cls: 0.3441  decode.d7.loss_mask: 0.3192  decode.d7.loss_dice: 0.3106  decode.d8.loss_cls: 0.2668  decode.d8.loss_mask: 0.3234  decode.d8.loss_dice: 0.3109
08/06 04:11:02 - mmengine - INFO - Iter(train) [ 15700/320000]  base_lr: 9.5574e-05 lr: 9.5574e-06  eta: 1 day, 12:54:41  time: 0.4366  data_time: 0.0093  memory: 5260  grad_norm: 245.0831  loss: 9.7058  decode.loss_cls: 0.2463  decode.loss_mask: 0.2858  decode.loss_dice: 0.3055  decode.d0.loss_cls: 1.1370  decode.d0.loss_mask: 0.3317  decode.d0.loss_dice: 0.3124  decode.d1.loss_cls: 0.4506  decode.d1.loss_mask: 0.3139  decode.d1.loss_dice: 0.3031  decode.d2.loss_cls: 0.2906  decode.d2.loss_mask: 0.3124  decode.d2.loss_dice: 0.2996  decode.d3.loss_cls: 0.2908  decode.d3.loss_mask: 0.3236  decode.d3.loss_dice: 0.3043  decode.d4.loss_cls: 0.2873  decode.d4.loss_mask: 0.3205  decode.d4.loss_dice: 0.3009  decode.d5.loss_cls: 0.2059  decode.d5.loss_mask: 0.3145  decode.d5.loss_dice: 0.3107  decode.d6.loss_cls: 0.1783  decode.d6.loss_mask: 0.3275  decode.d6.loss_dice: 0.3082  decode.d7.loss_cls: 0.1900  decode.d7.loss_mask: 0.3245  decode.d7.loss_dice: 0.3111  decode.d8.loss_cls: 0.1924  decode.d8.loss_mask: 0.3169  decode.d8.loss_dice: 0.3092
08/06 04:11:24 - mmengine - INFO - Iter(train) [ 15750/320000]  base_lr: 9.5559e-05 lr: 9.5559e-06  eta: 1 day, 12:54:19  time: 0.4372  data_time: 0.0093  memory: 5240  grad_norm: 148.4345  loss: 10.2507  decode.loss_cls: 0.2475  decode.loss_mask: 0.3171  decode.loss_dice: 0.3124  decode.d0.loss_cls: 1.1794  decode.d0.loss_mask: 0.3420  decode.d0.loss_dice: 0.3489  decode.d1.loss_cls: 0.3661  decode.d1.loss_mask: 0.3312  decode.d1.loss_dice: 0.3271  decode.d2.loss_cls: 0.2833  decode.d2.loss_mask: 0.3207  decode.d2.loss_dice: 0.3255  decode.d3.loss_cls: 0.2827  decode.d3.loss_mask: 0.3270  decode.d3.loss_dice: 0.3287  decode.d4.loss_cls: 0.2701  decode.d4.loss_mask: 0.3441  decode.d4.loss_dice: 0.3302  decode.d5.loss_cls: 0.2235  decode.d5.loss_mask: 0.3289  decode.d5.loss_dice: 0.3244  decode.d6.loss_cls: 0.2433  decode.d6.loss_mask: 0.3300  decode.d6.loss_dice: 0.3239  decode.d7.loss_cls: 0.2833  decode.d7.loss_mask: 0.3277  decode.d7.loss_dice: 0.3327  decode.d8.loss_cls: 0.2964  decode.d8.loss_mask: 0.3223  decode.d8.loss_dice: 0.3303
08/06 04:11:46 - mmengine - INFO - Iter(train) [ 15800/320000]  base_lr: 9.5545e-05 lr: 9.5545e-06  eta: 1 day, 12:53:58  time: 0.4376  data_time: 0.0092  memory: 5260  grad_norm: 154.4387  loss: 7.9858  decode.loss_cls: 0.1899  decode.loss_mask: 0.2356  decode.loss_dice: 0.2734  decode.d0.loss_cls: 0.9749  decode.d0.loss_mask: 0.2414  decode.d0.loss_dice: 0.3223  decode.d1.loss_cls: 0.2260  decode.d1.loss_mask: 0.2430  decode.d1.loss_dice: 0.3115  decode.d2.loss_cls: 0.1473  decode.d2.loss_mask: 0.2536  decode.d2.loss_dice: 0.3182  decode.d3.loss_cls: 0.2102  decode.d3.loss_mask: 0.2449  decode.d3.loss_dice: 0.2850  decode.d4.loss_cls: 0.2234  decode.d4.loss_mask: 0.2318  decode.d4.loss_dice: 0.2678  decode.d5.loss_cls: 0.1886  decode.d5.loss_mask: 0.2394  decode.d5.loss_dice: 0.2685  decode.d6.loss_cls: 0.1364  decode.d6.loss_mask: 0.2345  decode.d6.loss_dice: 0.2767  decode.d7.loss_cls: 0.1867  decode.d7.loss_mask: 0.2348  decode.d7.loss_dice: 0.2741  decode.d8.loss_cls: 0.2468  decode.d8.loss_mask: 0.2341  decode.d8.loss_dice: 0.2648
08/06 04:12:08 - mmengine - INFO - Iter(train) [ 15850/320000]  base_lr: 9.5531e-05 lr: 9.5531e-06  eta: 1 day, 12:53:37  time: 0.4379  data_time: 0.0091  memory: 5260  grad_norm: 108.8278  loss: 9.1915  decode.loss_cls: 0.2488  decode.loss_mask: 0.1929  decode.loss_dice: 0.3545  decode.d0.loss_cls: 1.0364  decode.d0.loss_mask: 0.2124  decode.d0.loss_dice: 0.3969  decode.d1.loss_cls: 0.3744  decode.d1.loss_mask: 0.2017  decode.d1.loss_dice: 0.3678  decode.d2.loss_cls: 0.2523  decode.d2.loss_mask: 0.1994  decode.d2.loss_dice: 0.3910  decode.d3.loss_cls: 0.2774  decode.d3.loss_mask: 0.1936  decode.d3.loss_dice: 0.3532  decode.d4.loss_cls: 0.2683  decode.d4.loss_mask: 0.1963  decode.d4.loss_dice: 0.3545  decode.d5.loss_cls: 0.3025  decode.d5.loss_mask: 0.1960  decode.d5.loss_dice: 0.3642  decode.d6.loss_cls: 0.2631  decode.d6.loss_mask: 0.1971  decode.d6.loss_dice: 0.3620  decode.d7.loss_cls: 0.2520  decode.d7.loss_mask: 0.1952  decode.d7.loss_dice: 0.3705  decode.d8.loss_cls: 0.2578  decode.d8.loss_mask: 0.1947  decode.d8.loss_dice: 0.3646
08/06 04:12:29 - mmengine - INFO - Iter(train) [ 15900/320000]  base_lr: 9.5517e-05 lr: 9.5517e-06  eta: 1 day, 12:53:15  time: 0.4372  data_time: 0.0093  memory: 5260  grad_norm: 72.2453  loss: 7.9043  decode.loss_cls: 0.1951  decode.loss_mask: 0.2488  decode.loss_dice: 0.2727  decode.d0.loss_cls: 1.0487  decode.d0.loss_mask: 0.2551  decode.d0.loss_dice: 0.2493  decode.d1.loss_cls: 0.2836  decode.d1.loss_mask: 0.2535  decode.d1.loss_dice: 0.2738  decode.d2.loss_cls: 0.2256  decode.d2.loss_mask: 0.2491  decode.d2.loss_dice: 0.2597  decode.d3.loss_cls: 0.1856  decode.d3.loss_mask: 0.2490  decode.d3.loss_dice: 0.2535  decode.d4.loss_cls: 0.1919  decode.d4.loss_mask: 0.2496  decode.d4.loss_dice: 0.2302  decode.d5.loss_cls: 0.1839  decode.d5.loss_mask: 0.2494  decode.d5.loss_dice: 0.2503  decode.d6.loss_cls: 0.1728  decode.d6.loss_mask: 0.2494  decode.d6.loss_dice: 0.2280  decode.d7.loss_cls: 0.1818  decode.d7.loss_mask: 0.2446  decode.d7.loss_dice: 0.2470  decode.d8.loss_cls: 0.1975  decode.d8.loss_mask: 0.2491  decode.d8.loss_dice: 0.2756
08/06 04:12:51 - mmengine - INFO - Iter(train) [ 15950/320000]  base_lr: 9.5503e-05 lr: 9.5503e-06  eta: 1 day, 12:52:54  time: 0.4367  data_time: 0.0092  memory: 5240  grad_norm: 78.4608  loss: 6.4717  decode.loss_cls: 0.0952  decode.loss_mask: 0.2434  decode.loss_dice: 0.2484  decode.d0.loss_cls: 0.7942  decode.d0.loss_mask: 0.2493  decode.d0.loss_dice: 0.2688  decode.d1.loss_cls: 0.1169  decode.d1.loss_mask: 0.2483  decode.d1.loss_dice: 0.2464  decode.d2.loss_cls: 0.0892  decode.d2.loss_mask: 0.2478  decode.d2.loss_dice: 0.2422  decode.d3.loss_cls: 0.0708  decode.d3.loss_mask: 0.2466  decode.d3.loss_dice: 0.2408  decode.d4.loss_cls: 0.0751  decode.d4.loss_mask: 0.2473  decode.d4.loss_dice: 0.2347  decode.d5.loss_cls: 0.0717  decode.d5.loss_mask: 0.2475  decode.d5.loss_dice: 0.2387  decode.d6.loss_cls: 0.0824  decode.d6.loss_mask: 0.2459  decode.d6.loss_dice: 0.2410  decode.d7.loss_cls: 0.0802  decode.d7.loss_mask: 0.2477  decode.d7.loss_dice: 0.2392  decode.d8.loss_cls: 0.0809  decode.d8.loss_mask: 0.2469  decode.d8.loss_dice: 0.2442
08/06 04:13:13 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 04:13:13 - mmengine - INFO - Iter(train) [ 16000/320000]  base_lr: 9.5489e-05 lr: 9.5489e-06  eta: 1 day, 12:52:35  time: 0.4378  data_time: 0.0092  memory: 5275  grad_norm: 109.5263  loss: 9.3310  decode.loss_cls: 0.3488  decode.loss_mask: 0.2608  decode.loss_dice: 0.2646  decode.d0.loss_cls: 1.2122  decode.d0.loss_mask: 0.2279  decode.d0.loss_dice: 0.2781  decode.d1.loss_cls: 0.4164  decode.d1.loss_mask: 0.2207  decode.d1.loss_dice: 0.2692  decode.d2.loss_cls: 0.4006  decode.d2.loss_mask: 0.2218  decode.d2.loss_dice: 0.2569  decode.d3.loss_cls: 0.3193  decode.d3.loss_mask: 0.2691  decode.d3.loss_dice: 0.2521  decode.d4.loss_cls: 0.3007  decode.d4.loss_mask: 0.2704  decode.d4.loss_dice: 0.2630  decode.d5.loss_cls: 0.2951  decode.d5.loss_mask: 0.2701  decode.d5.loss_dice: 0.2533  decode.d6.loss_cls: 0.2971  decode.d6.loss_mask: 0.2682  decode.d6.loss_dice: 0.2620  decode.d7.loss_cls: 0.2899  decode.d7.loss_mask: 0.2731  decode.d7.loss_dice: 0.2530  decode.d8.loss_cls: 0.2942  decode.d8.loss_mask: 0.2662  decode.d8.loss_dice: 0.2561
08/06 04:13:35 - mmengine - INFO - Iter(train) [ 16050/320000]  base_lr: 9.5475e-05 lr: 9.5475e-06  eta: 1 day, 12:52:14  time: 0.4369  data_time: 0.0089  memory: 5260  grad_norm: 58.6837  loss: 8.0581  decode.loss_cls: 0.1950  decode.loss_mask: 0.1950  decode.loss_dice: 0.3008  decode.d0.loss_cls: 0.8621  decode.d0.loss_mask: 0.1983  decode.d0.loss_dice: 0.3260  decode.d1.loss_cls: 0.2982  decode.d1.loss_mask: 0.1917  decode.d1.loss_dice: 0.3177  decode.d2.loss_cls: 0.2808  decode.d2.loss_mask: 0.1962  decode.d2.loss_dice: 0.3034  decode.d3.loss_cls: 0.2534  decode.d3.loss_mask: 0.1930  decode.d3.loss_dice: 0.2991  decode.d4.loss_cls: 0.2608  decode.d4.loss_mask: 0.1936  decode.d4.loss_dice: 0.2966  decode.d5.loss_cls: 0.2407  decode.d5.loss_mask: 0.1933  decode.d5.loss_dice: 0.3195  decode.d6.loss_cls: 0.2218  decode.d6.loss_mask: 0.1939  decode.d6.loss_dice: 0.3222  decode.d7.loss_cls: 0.2212  decode.d7.loss_mask: 0.1937  decode.d7.loss_dice: 0.3040  decode.d8.loss_cls: 0.1954  decode.d8.loss_mask: 0.1950  decode.d8.loss_dice: 0.2957
08/06 04:13:57 - mmengine - INFO - Iter(train) [ 16100/320000]  base_lr: 9.5461e-05 lr: 9.5461e-06  eta: 1 day, 12:51:52  time: 0.4368  data_time: 0.0092  memory: 5240  grad_norm: 151.1284  loss: 10.4010  decode.loss_cls: 0.3387  decode.loss_mask: 0.2515  decode.loss_dice: 0.3779  decode.d0.loss_cls: 0.9491  decode.d0.loss_mask: 0.2548  decode.d0.loss_dice: 0.4270  decode.d1.loss_cls: 0.4811  decode.d1.loss_mask: 0.2526  decode.d1.loss_dice: 0.3990  decode.d2.loss_cls: 0.2932  decode.d2.loss_mask: 0.2556  decode.d2.loss_dice: 0.3962  decode.d3.loss_cls: 0.3696  decode.d3.loss_mask: 0.2456  decode.d3.loss_dice: 0.3843  decode.d4.loss_cls: 0.3136  decode.d4.loss_mask: 0.2427  decode.d4.loss_dice: 0.3797  decode.d5.loss_cls: 0.2936  decode.d5.loss_mask: 0.2447  decode.d5.loss_dice: 0.3801  decode.d6.loss_cls: 0.2808  decode.d6.loss_mask: 0.2518  decode.d6.loss_dice: 0.3770  decode.d7.loss_cls: 0.2953  decode.d7.loss_mask: 0.2541  decode.d7.loss_dice: 0.4101  decode.d8.loss_cls: 0.3603  decode.d8.loss_mask: 0.2515  decode.d8.loss_dice: 0.3895
08/06 04:14:19 - mmengine - INFO - Iter(train) [ 16150/320000]  base_lr: 9.5446e-05 lr: 9.5446e-06  eta: 1 day, 12:51:31  time: 0.4375  data_time: 0.0091  memory: 5260  grad_norm: 114.1042  loss: 6.3851  decode.loss_cls: 0.1000  decode.loss_mask: 0.2030  decode.loss_dice: 0.2168  decode.d0.loss_cls: 0.9106  decode.d0.loss_mask: 0.1978  decode.d0.loss_dice: 0.2720  decode.d1.loss_cls: 0.2079  decode.d1.loss_mask: 0.2035  decode.d1.loss_dice: 0.2310  decode.d2.loss_cls: 0.1678  decode.d2.loss_mask: 0.2021  decode.d2.loss_dice: 0.2301  decode.d3.loss_cls: 0.1734  decode.d3.loss_mask: 0.2044  decode.d3.loss_dice: 0.2178  decode.d4.loss_cls: 0.1153  decode.d4.loss_mask: 0.2076  decode.d4.loss_dice: 0.2249  decode.d5.loss_cls: 0.1016  decode.d5.loss_mask: 0.2030  decode.d5.loss_dice: 0.2194  decode.d6.loss_cls: 0.0985  decode.d6.loss_mask: 0.2014  decode.d6.loss_dice: 0.2254  decode.d7.loss_cls: 0.1070  decode.d7.loss_mask: 0.1985  decode.d7.loss_dice: 0.2230  decode.d8.loss_cls: 0.0916  decode.d8.loss_mask: 0.2038  decode.d8.loss_dice: 0.2259
08/06 04:14:41 - mmengine - INFO - Iter(train) [ 16200/320000]  base_lr: 9.5432e-05 lr: 9.5432e-06  eta: 1 day, 12:51:09  time: 0.4368  data_time: 0.0089  memory: 5242  grad_norm: 158.2965  loss: 9.2472  decode.loss_cls: 0.2810  decode.loss_mask: 0.2223  decode.loss_dice: 0.2839  decode.d0.loss_cls: 1.1444  decode.d0.loss_mask: 0.2293  decode.d0.loss_dice: 0.3129  decode.d1.loss_cls: 0.3761  decode.d1.loss_mask: 0.2264  decode.d1.loss_dice: 0.3105  decode.d2.loss_cls: 0.3393  decode.d2.loss_mask: 0.2314  decode.d2.loss_dice: 0.3058  decode.d3.loss_cls: 0.2580  decode.d3.loss_mask: 0.2339  decode.d3.loss_dice: 0.3107  decode.d4.loss_cls: 0.3261  decode.d4.loss_mask: 0.2305  decode.d4.loss_dice: 0.2928  decode.d5.loss_cls: 0.2886  decode.d5.loss_mask: 0.2274  decode.d5.loss_dice: 0.2857  decode.d6.loss_cls: 0.3174  decode.d6.loss_mask: 0.2248  decode.d6.loss_dice: 0.3082  decode.d7.loss_cls: 0.3112  decode.d7.loss_mask: 0.2240  decode.d7.loss_dice: 0.2942  decode.d8.loss_cls: 0.3291  decode.d8.loss_mask: 0.2235  decode.d8.loss_dice: 0.2978
08/06 04:15:03 - mmengine - INFO - Iter(train) [ 16250/320000]  base_lr: 9.5418e-05 lr: 9.5418e-06  eta: 1 day, 12:50:47  time: 0.4367  data_time: 0.0090  memory: 5258  grad_norm: 149.8967  loss: 9.9259  decode.loss_cls: 0.3404  decode.loss_mask: 0.2935  decode.loss_dice: 0.3529  decode.d0.loss_cls: 0.9877  decode.d0.loss_mask: 0.3119  decode.d0.loss_dice: 0.3612  decode.d1.loss_cls: 0.3733  decode.d1.loss_mask: 0.3193  decode.d1.loss_dice: 0.3387  decode.d2.loss_cls: 0.2639  decode.d2.loss_mask: 0.3136  decode.d2.loss_dice: 0.3396  decode.d3.loss_cls: 0.2758  decode.d3.loss_mask: 0.3127  decode.d3.loss_dice: 0.3378  decode.d4.loss_cls: 0.2559  decode.d4.loss_mask: 0.3031  decode.d4.loss_dice: 0.3427  decode.d5.loss_cls: 0.2175  decode.d5.loss_mask: 0.3155  decode.d5.loss_dice: 0.3328  decode.d6.loss_cls: 0.2043  decode.d6.loss_mask: 0.3134  decode.d6.loss_dice: 0.3285  decode.d7.loss_cls: 0.2126  decode.d7.loss_mask: 0.3185  decode.d7.loss_dice: 0.3515  decode.d8.loss_cls: 0.2340  decode.d8.loss_mask: 0.3262  decode.d8.loss_dice: 0.3472
08/06 04:15:24 - mmengine - INFO - Iter(train) [ 16300/320000]  base_lr: 9.5404e-05 lr: 9.5404e-06  eta: 1 day, 12:50:26  time: 0.4371  data_time: 0.0091  memory: 5240  grad_norm: 110.2807  loss: 8.8441  decode.loss_cls: 0.1332  decode.loss_mask: 0.2809  decode.loss_dice: 0.3323  decode.d0.loss_cls: 1.0207  decode.d0.loss_mask: 0.2817  decode.d0.loss_dice: 0.3499  decode.d1.loss_cls: 0.2245  decode.d1.loss_mask: 0.2805  decode.d1.loss_dice: 0.3413  decode.d2.loss_cls: 0.1935  decode.d2.loss_mask: 0.2831  decode.d2.loss_dice: 0.3309  decode.d3.loss_cls: 0.1864  decode.d3.loss_mask: 0.2736  decode.d3.loss_dice: 0.3312  decode.d4.loss_cls: 0.1945  decode.d4.loss_mask: 0.2737  decode.d4.loss_dice: 0.3142  decode.d5.loss_cls: 0.1794  decode.d5.loss_mask: 0.2835  decode.d5.loss_dice: 0.3364  decode.d6.loss_cls: 0.1736  decode.d6.loss_mask: 0.2839  decode.d6.loss_dice: 0.3451  decode.d7.loss_cls: 0.1794  decode.d7.loss_mask: 0.2772  decode.d7.loss_dice: 0.3211  decode.d8.loss_cls: 0.2337  decode.d8.loss_mask: 0.2768  decode.d8.loss_dice: 0.3280
08/06 04:15:46 - mmengine - INFO - Iter(train) [ 16350/320000]  base_lr: 9.5390e-05 lr: 9.5390e-06  eta: 1 day, 12:50:04  time: 0.4369  data_time: 0.0090  memory: 5260  grad_norm: 179.7667  loss: 10.1867  decode.loss_cls: 0.4282  decode.loss_mask: 0.2627  decode.loss_dice: 0.3061  decode.d0.loss_cls: 1.2164  decode.d0.loss_mask: 0.2833  decode.d0.loss_dice: 0.3141  decode.d1.loss_cls: 0.4560  decode.d1.loss_mask: 0.2759  decode.d1.loss_dice: 0.2853  decode.d2.loss_cls: 0.3351  decode.d2.loss_mask: 0.2675  decode.d2.loss_dice: 0.2889  decode.d3.loss_cls: 0.3229  decode.d3.loss_mask: 0.2679  decode.d3.loss_dice: 0.2920  decode.d4.loss_cls: 0.3168  decode.d4.loss_mask: 0.2608  decode.d4.loss_dice: 0.2925  decode.d5.loss_cls: 0.3677  decode.d5.loss_mask: 0.2647  decode.d5.loss_dice: 0.2820  decode.d6.loss_cls: 0.3418  decode.d6.loss_mask: 0.2715  decode.d6.loss_dice: 0.2978  decode.d7.loss_cls: 0.3365  decode.d7.loss_mask: 0.2816  decode.d7.loss_dice: 0.3042  decode.d8.loss_cls: 0.4146  decode.d8.loss_mask: 0.2610  decode.d8.loss_dice: 0.2909
08/06 04:16:08 - mmengine - INFO - Iter(train) [ 16400/320000]  base_lr: 9.5376e-05 lr: 9.5376e-06  eta: 1 day, 12:49:43  time: 0.4371  data_time: 0.0091  memory: 5240  grad_norm: 124.4252  loss: 9.5054  decode.loss_cls: 0.2702  decode.loss_mask: 0.3443  decode.loss_dice: 0.2846  decode.d0.loss_cls: 0.9843  decode.d0.loss_mask: 0.3641  decode.d0.loss_dice: 0.3211  decode.d1.loss_cls: 0.2602  decode.d1.loss_mask: 0.3551  decode.d1.loss_dice: 0.2923  decode.d2.loss_cls: 0.2288  decode.d2.loss_mask: 0.3311  decode.d2.loss_dice: 0.2795  decode.d3.loss_cls: 0.2165  decode.d3.loss_mask: 0.3463  decode.d3.loss_dice: 0.2970  decode.d4.loss_cls: 0.2387  decode.d4.loss_mask: 0.3461  decode.d4.loss_dice: 0.2916  decode.d5.loss_cls: 0.2241  decode.d5.loss_mask: 0.3507  decode.d5.loss_dice: 0.2974  decode.d6.loss_cls: 0.1978  decode.d6.loss_mask: 0.3443  decode.d6.loss_dice: 0.2851  decode.d7.loss_cls: 0.2265  decode.d7.loss_mask: 0.3435  decode.d7.loss_dice: 0.2800  decode.d8.loss_cls: 0.2739  decode.d8.loss_mask: 0.3438  decode.d8.loss_dice: 0.2865
08/06 04:16:30 - mmengine - INFO - Iter(train) [ 16450/320000]  base_lr: 9.5362e-05 lr: 9.5362e-06  eta: 1 day, 12:49:22  time: 0.4377  data_time: 0.0090  memory: 5242  grad_norm: 87.6917  loss: 10.5925  decode.loss_cls: 0.3299  decode.loss_mask: 0.2626  decode.loss_dice: 0.3818  decode.d0.loss_cls: 0.9738  decode.d0.loss_mask: 0.2600  decode.d0.loss_dice: 0.3748  decode.d1.loss_cls: 0.5069  decode.d1.loss_mask: 0.2618  decode.d1.loss_dice: 0.3738  decode.d2.loss_cls: 0.3731  decode.d2.loss_mask: 0.2672  decode.d2.loss_dice: 0.3770  decode.d3.loss_cls: 0.3247  decode.d3.loss_mask: 0.2668  decode.d3.loss_dice: 0.4045  decode.d4.loss_cls: 0.4165  decode.d4.loss_mask: 0.2653  decode.d4.loss_dice: 0.3576  decode.d5.loss_cls: 0.3103  decode.d5.loss_mask: 0.2621  decode.d5.loss_dice: 0.3577  decode.d6.loss_cls: 0.3037  decode.d6.loss_mask: 0.2626  decode.d6.loss_dice: 0.3738  decode.d7.loss_cls: 0.3188  decode.d7.loss_mask: 0.2686  decode.d7.loss_dice: 0.3752  decode.d8.loss_cls: 0.3588  decode.d8.loss_mask: 0.2659  decode.d8.loss_dice: 0.3570
08/06 04:16:52 - mmengine - INFO - Iter(train) [ 16500/320000]  base_lr: 9.5347e-05 lr: 9.5347e-06  eta: 1 day, 12:49:01  time: 0.4382  data_time: 0.0094  memory: 5224  grad_norm: 54.3477  loss: 8.4693  decode.loss_cls: 0.1790  decode.loss_mask: 0.2482  decode.loss_dice: 0.2928  decode.d0.loss_cls: 0.8978  decode.d0.loss_mask: 0.2543  decode.d0.loss_dice: 0.3139  decode.d1.loss_cls: 0.2633  decode.d1.loss_mask: 0.2471  decode.d1.loss_dice: 0.3205  decode.d2.loss_cls: 0.2526  decode.d2.loss_mask: 0.2467  decode.d2.loss_dice: 0.2902  decode.d3.loss_cls: 0.1698  decode.d3.loss_mask: 0.2447  decode.d3.loss_dice: 0.2958  decode.d4.loss_cls: 0.1833  decode.d4.loss_mask: 0.2452  decode.d4.loss_dice: 0.3198  decode.d5.loss_cls: 0.2338  decode.d5.loss_mask: 0.2457  decode.d5.loss_dice: 0.3095  decode.d6.loss_cls: 0.2551  decode.d6.loss_mask: 0.2431  decode.d6.loss_dice: 0.2950  decode.d7.loss_cls: 0.2543  decode.d7.loss_mask: 0.2458  decode.d7.loss_dice: 0.3421  decode.d8.loss_cls: 0.2242  decode.d8.loss_mask: 0.2420  decode.d8.loss_dice: 0.3136
08/06 04:17:14 - mmengine - INFO - Iter(train) [ 16550/320000]  base_lr: 9.5333e-05 lr: 9.5333e-06  eta: 1 day, 12:48:40  time: 0.4370  data_time: 0.0092  memory: 5224  grad_norm: 72.3830  loss: 7.8133  decode.loss_cls: 0.1095  decode.loss_mask: 0.2557  decode.loss_dice: 0.2713  decode.d0.loss_cls: 0.7805  decode.d0.loss_mask: 0.2613  decode.d0.loss_dice: 0.2996  decode.d1.loss_cls: 0.2497  decode.d1.loss_mask: 0.2374  decode.d1.loss_dice: 0.2849  decode.d2.loss_cls: 0.2088  decode.d2.loss_mask: 0.2368  decode.d2.loss_dice: 0.2815  decode.d3.loss_cls: 0.2366  decode.d3.loss_mask: 0.2337  decode.d3.loss_dice: 0.2919  decode.d4.loss_cls: 0.2163  decode.d4.loss_mask: 0.2415  decode.d4.loss_dice: 0.2663  decode.d5.loss_cls: 0.2026  decode.d5.loss_mask: 0.2398  decode.d5.loss_dice: 0.2886  decode.d6.loss_cls: 0.1810  decode.d6.loss_mask: 0.2375  decode.d6.loss_dice: 0.2680  decode.d7.loss_cls: 0.1896  decode.d7.loss_mask: 0.2338  decode.d7.loss_dice: 0.2799  decode.d8.loss_cls: 0.2167  decode.d8.loss_mask: 0.2335  decode.d8.loss_dice: 0.2790
08/06 04:17:36 - mmengine - INFO - Iter(train) [ 16600/320000]  base_lr: 9.5319e-05 lr: 9.5319e-06  eta: 1 day, 12:48:18  time: 0.4361  data_time: 0.0093  memory: 5242  grad_norm: 93.5393  loss: 8.4335  decode.loss_cls: 0.2309  decode.loss_mask: 0.2616  decode.loss_dice: 0.3148  decode.d0.loss_cls: 0.9049  decode.d0.loss_mask: 0.2770  decode.d0.loss_dice: 0.3637  decode.d1.loss_cls: 0.2209  decode.d1.loss_mask: 0.2570  decode.d1.loss_dice: 0.3233  decode.d2.loss_cls: 0.1676  decode.d2.loss_mask: 0.2571  decode.d2.loss_dice: 0.3177  decode.d3.loss_cls: 0.1626  decode.d3.loss_mask: 0.2571  decode.d3.loss_dice: 0.3103  decode.d4.loss_cls: 0.1838  decode.d4.loss_mask: 0.2584  decode.d4.loss_dice: 0.3035  decode.d5.loss_cls: 0.1792  decode.d5.loss_mask: 0.2589  decode.d5.loss_dice: 0.3053  decode.d6.loss_cls: 0.2443  decode.d6.loss_mask: 0.2572  decode.d6.loss_dice: 0.2968  decode.d7.loss_cls: 0.1708  decode.d7.loss_mask: 0.2569  decode.d7.loss_dice: 0.3046  decode.d8.loss_cls: 0.2109  decode.d8.loss_mask: 0.2611  decode.d8.loss_dice: 0.3152
08/06 04:17:58 - mmengine - INFO - Iter(train) [ 16650/320000]  base_lr: 9.5305e-05 lr: 9.5305e-06  eta: 1 day, 12:47:57  time: 0.4367  data_time: 0.0093  memory: 5224  grad_norm: 170.8945  loss: 10.6150  decode.loss_cls: 0.3066  decode.loss_mask: 0.3412  decode.loss_dice: 0.3148  decode.d0.loss_cls: 1.0939  decode.d0.loss_mask: 0.3494  decode.d0.loss_dice: 0.3404  decode.d1.loss_cls: 0.4258  decode.d1.loss_mask: 0.3323  decode.d1.loss_dice: 0.2984  decode.d2.loss_cls: 0.3494  decode.d2.loss_mask: 0.3306  decode.d2.loss_dice: 0.3056  decode.d3.loss_cls: 0.3356  decode.d3.loss_mask: 0.3393  decode.d3.loss_dice: 0.3028  decode.d4.loss_cls: 0.3605  decode.d4.loss_mask: 0.3304  decode.d4.loss_dice: 0.2879  decode.d5.loss_cls: 0.3616  decode.d5.loss_mask: 0.3378  decode.d5.loss_dice: 0.3212  decode.d6.loss_cls: 0.3252  decode.d6.loss_mask: 0.3332  decode.d6.loss_dice: 0.3011  decode.d7.loss_cls: 0.2894  decode.d7.loss_mask: 0.3254  decode.d7.loss_dice: 0.3025  decode.d8.loss_cls: 0.3490  decode.d8.loss_mask: 0.3255  decode.d8.loss_dice: 0.2984
08/06 04:18:19 - mmengine - INFO - Iter(train) [ 16700/320000]  base_lr: 9.5291e-05 lr: 9.5291e-06  eta: 1 day, 12:47:36  time: 0.4370  data_time: 0.0093  memory: 5223  grad_norm: 90.0708  loss: 9.1444  decode.loss_cls: 0.2370  decode.loss_mask: 0.3154  decode.loss_dice: 0.2729  decode.d0.loss_cls: 0.9654  decode.d0.loss_mask: 0.3181  decode.d0.loss_dice: 0.2973  decode.d1.loss_cls: 0.3587  decode.d1.loss_mask: 0.3174  decode.d1.loss_dice: 0.2849  decode.d2.loss_cls: 0.2876  decode.d2.loss_mask: 0.3094  decode.d2.loss_dice: 0.2676  decode.d3.loss_cls: 0.2325  decode.d3.loss_mask: 0.3085  decode.d3.loss_dice: 0.2680  decode.d4.loss_cls: 0.2770  decode.d4.loss_mask: 0.3096  decode.d4.loss_dice: 0.2743  decode.d5.loss_cls: 0.2643  decode.d5.loss_mask: 0.3045  decode.d5.loss_dice: 0.2669  decode.d6.loss_cls: 0.2122  decode.d6.loss_mask: 0.3145  decode.d6.loss_dice: 0.2688  decode.d7.loss_cls: 0.2363  decode.d7.loss_mask: 0.3177  decode.d7.loss_dice: 0.2629  decode.d8.loss_cls: 0.2179  decode.d8.loss_mask: 0.3077  decode.d8.loss_dice: 0.2692
08/06 04:18:41 - mmengine - INFO - Iter(train) [ 16750/320000]  base_lr: 9.5277e-05 lr: 9.5277e-06  eta: 1 day, 12:47:14  time: 0.4368  data_time: 0.0091  memory: 5260  grad_norm: 152.5617  loss: 9.2131  decode.loss_cls: 0.1707  decode.loss_mask: 0.2977  decode.loss_dice: 0.3332  decode.d0.loss_cls: 0.9253  decode.d0.loss_mask: 0.2498  decode.d0.loss_dice: 0.3230  decode.d1.loss_cls: 0.4521  decode.d1.loss_mask: 0.2421  decode.d1.loss_dice: 0.3210  decode.d2.loss_cls: 0.3454  decode.d2.loss_mask: 0.2372  decode.d2.loss_dice: 0.3190  decode.d3.loss_cls: 0.2307  decode.d3.loss_mask: 0.2452  decode.d3.loss_dice: 0.3275  decode.d4.loss_cls: 0.2337  decode.d4.loss_mask: 0.2691  decode.d4.loss_dice: 0.3518  decode.d5.loss_cls: 0.1813  decode.d5.loss_mask: 0.2930  decode.d5.loss_dice: 0.3463  decode.d6.loss_cls: 0.1614  decode.d6.loss_mask: 0.3099  decode.d6.loss_dice: 0.3712  decode.d7.loss_cls: 0.1853  decode.d7.loss_mask: 0.3010  decode.d7.loss_dice: 0.3741  decode.d8.loss_cls: 0.1635  decode.d8.loss_mask: 0.2968  decode.d8.loss_dice: 0.3550
08/06 04:19:03 - mmengine - INFO - Iter(train) [ 16800/320000]  base_lr: 9.5263e-05 lr: 9.5263e-06  eta: 1 day, 12:46:53  time: 0.4367  data_time: 0.0092  memory: 5224  grad_norm: 209.8189  loss: 9.7345  decode.loss_cls: 0.2754  decode.loss_mask: 0.3102  decode.loss_dice: 0.2979  decode.d0.loss_cls: 1.0998  decode.d0.loss_mask: 0.3057  decode.d0.loss_dice: 0.3187  decode.d1.loss_cls: 0.2817  decode.d1.loss_mask: 0.3206  decode.d1.loss_dice: 0.3069  decode.d2.loss_cls: 0.3006  decode.d2.loss_mask: 0.3126  decode.d2.loss_dice: 0.2991  decode.d3.loss_cls: 0.2640  decode.d3.loss_mask: 0.3072  decode.d3.loss_dice: 0.2915  decode.d4.loss_cls: 0.2517  decode.d4.loss_mask: 0.3217  decode.d4.loss_dice: 0.3009  decode.d5.loss_cls: 0.3080  decode.d5.loss_mask: 0.3026  decode.d5.loss_dice: 0.2880  decode.d6.loss_cls: 0.2758  decode.d6.loss_mask: 0.2962  decode.d6.loss_dice: 0.3030  decode.d7.loss_cls: 0.3028  decode.d7.loss_mask: 0.3000  decode.d7.loss_dice: 0.2870  decode.d8.loss_cls: 0.3201  decode.d8.loss_mask: 0.3017  decode.d8.loss_dice: 0.2828
08/06 04:19:25 - mmengine - INFO - Iter(train) [ 16850/320000]  base_lr: 9.5248e-05 lr: 9.5248e-06  eta: 1 day, 12:46:32  time: 0.4376  data_time: 0.0092  memory: 5242  grad_norm: 82.6747  loss: 7.0519  decode.loss_cls: 0.1290  decode.loss_mask: 0.2139  decode.loss_dice: 0.2940  decode.d0.loss_cls: 0.9420  decode.d0.loss_mask: 0.2076  decode.d0.loss_dice: 0.2786  decode.d1.loss_cls: 0.1779  decode.d1.loss_mask: 0.2008  decode.d1.loss_dice: 0.2583  decode.d2.loss_cls: 0.1389  decode.d2.loss_mask: 0.2137  decode.d2.loss_dice: 0.2855  decode.d3.loss_cls: 0.1394  decode.d3.loss_mask: 0.2017  decode.d3.loss_dice: 0.2684  decode.d4.loss_cls: 0.1375  decode.d4.loss_mask: 0.2051  decode.d4.loss_dice: 0.2764  decode.d5.loss_cls: 0.1414  decode.d5.loss_mask: 0.2040  decode.d5.loss_dice: 0.2680  decode.d6.loss_cls: 0.1224  decode.d6.loss_mask: 0.1985  decode.d6.loss_dice: 0.2686  decode.d7.loss_cls: 0.1378  decode.d7.loss_mask: 0.2133  decode.d7.loss_dice: 0.2861  decode.d8.loss_cls: 0.1396  decode.d8.loss_mask: 0.2138  decode.d8.loss_dice: 0.2896
08/06 04:19:47 - mmengine - INFO - Iter(train) [ 16900/320000]  base_lr: 9.5234e-05 lr: 9.5234e-06  eta: 1 day, 12:46:11  time: 0.4376  data_time: 0.0093  memory: 5260  grad_norm: 68.9479  loss: 9.1228  decode.loss_cls: 0.1794  decode.loss_mask: 0.2869  decode.loss_dice: 0.3600  decode.d0.loss_cls: 0.9942  decode.d0.loss_mask: 0.2994  decode.d0.loss_dice: 0.3491  decode.d1.loss_cls: 0.2723  decode.d1.loss_mask: 0.2856  decode.d1.loss_dice: 0.3270  decode.d2.loss_cls: 0.1310  decode.d2.loss_mask: 0.2913  decode.d2.loss_dice: 0.3375  decode.d3.loss_cls: 0.2322  decode.d3.loss_mask: 0.2920  decode.d3.loss_dice: 0.3331  decode.d4.loss_cls: 0.1975  decode.d4.loss_mask: 0.2932  decode.d4.loss_dice: 0.3270  decode.d5.loss_cls: 0.2160  decode.d5.loss_mask: 0.2879  decode.d5.loss_dice: 0.3227  decode.d6.loss_cls: 0.2205  decode.d6.loss_mask: 0.2909  decode.d6.loss_dice: 0.3309  decode.d7.loss_cls: 0.2287  decode.d7.loss_mask: 0.2875  decode.d7.loss_dice: 0.3107  decode.d8.loss_cls: 0.1922  decode.d8.loss_mask: 0.2865  decode.d8.loss_dice: 0.3594
08/06 04:20:09 - mmengine - INFO - Iter(train) [ 16950/320000]  base_lr: 9.5220e-05 lr: 9.5220e-06  eta: 1 day, 12:45:50  time: 0.4390  data_time: 0.0094  memory: 5240  grad_norm: 136.6573  loss: 8.6324  decode.loss_cls: 0.2016  decode.loss_mask: 0.2697  decode.loss_dice: 0.2804  decode.d0.loss_cls: 1.2174  decode.d0.loss_mask: 0.2952  decode.d0.loss_dice: 0.3111  decode.d1.loss_cls: 0.2301  decode.d1.loss_mask: 0.2750  decode.d1.loss_dice: 0.2805  decode.d2.loss_cls: 0.2054  decode.d2.loss_mask: 0.2746  decode.d2.loss_dice: 0.2857  decode.d3.loss_cls: 0.1844  decode.d3.loss_mask: 0.2707  decode.d3.loss_dice: 0.2733  decode.d4.loss_cls: 0.1887  decode.d4.loss_mask: 0.2727  decode.d4.loss_dice: 0.2833  decode.d5.loss_cls: 0.2212  decode.d5.loss_mask: 0.2689  decode.d5.loss_dice: 0.2857  decode.d6.loss_cls: 0.2066  decode.d6.loss_mask: 0.2727  decode.d6.loss_dice: 0.2798  decode.d7.loss_cls: 0.2168  decode.d7.loss_mask: 0.2719  decode.d7.loss_dice: 0.2869  decode.d8.loss_cls: 0.1681  decode.d8.loss_mask: 0.2658  decode.d8.loss_dice: 0.2882
08/06 04:20:31 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 04:20:31 - mmengine - INFO - Iter(train) [ 17000/320000]  base_lr: 9.5206e-05 lr: 9.5206e-06  eta: 1 day, 12:45:30  time: 0.4380  data_time: 0.0092  memory: 5275  grad_norm: 125.8786  loss: 8.8393  decode.loss_cls: 0.1891  decode.loss_mask: 0.2738  decode.loss_dice: 0.3145  decode.d0.loss_cls: 0.9281  decode.d0.loss_mask: 0.2771  decode.d0.loss_dice: 0.3221  decode.d1.loss_cls: 0.3077  decode.d1.loss_mask: 0.2761  decode.d1.loss_dice: 0.2744  decode.d2.loss_cls: 0.3049  decode.d2.loss_mask: 0.2655  decode.d2.loss_dice: 0.2786  decode.d3.loss_cls: 0.2075  decode.d3.loss_mask: 0.2704  decode.d3.loss_dice: 0.3034  decode.d4.loss_cls: 0.2386  decode.d4.loss_mask: 0.2717  decode.d4.loss_dice: 0.3120  decode.d5.loss_cls: 0.2686  decode.d5.loss_mask: 0.2677  decode.d5.loss_dice: 0.2747  decode.d6.loss_cls: 0.2695  decode.d6.loss_mask: 0.2670  decode.d6.loss_dice: 0.2894  decode.d7.loss_cls: 0.2518  decode.d7.loss_mask: 0.2666  decode.d7.loss_dice: 0.3164  decode.d8.loss_cls: 0.1942  decode.d8.loss_mask: 0.2689  decode.d8.loss_dice: 0.2890
08/06 04:20:53 - mmengine - INFO - Iter(train) [ 17050/320000]  base_lr: 9.5192e-05 lr: 9.5192e-06  eta: 1 day, 12:45:09  time: 0.4382  data_time: 0.0091  memory: 5223  grad_norm: 115.1520  loss: 9.9536  decode.loss_cls: 0.2987  decode.loss_mask: 0.2894  decode.loss_dice: 0.2950  decode.d0.loss_cls: 0.9924  decode.d0.loss_mask: 0.3401  decode.d0.loss_dice: 0.3451  decode.d1.loss_cls: 0.3677  decode.d1.loss_mask: 0.3016  decode.d1.loss_dice: 0.3180  decode.d2.loss_cls: 0.3830  decode.d2.loss_mask: 0.2924  decode.d2.loss_dice: 0.2818  decode.d3.loss_cls: 0.3047  decode.d3.loss_mask: 0.2907  decode.d3.loss_dice: 0.3065  decode.d4.loss_cls: 0.2943  decode.d4.loss_mask: 0.2958  decode.d4.loss_dice: 0.3003  decode.d5.loss_cls: 0.3245  decode.d5.loss_mask: 0.2935  decode.d5.loss_dice: 0.3082  decode.d6.loss_cls: 0.2686  decode.d6.loss_mask: 0.2928  decode.d6.loss_dice: 0.3019  decode.d7.loss_cls: 0.3303  decode.d7.loss_mask: 0.2923  decode.d7.loss_dice: 0.2896  decode.d8.loss_cls: 0.2677  decode.d8.loss_mask: 0.3605  decode.d8.loss_dice: 0.3263
08/06 04:21:14 - mmengine - INFO - Iter(train) [ 17100/320000]  base_lr: 9.5178e-05 lr: 9.5178e-06  eta: 1 day, 12:44:48  time: 0.4371  data_time: 0.0095  memory: 5240  grad_norm: 115.5165  loss: 9.3221  decode.loss_cls: 0.1631  decode.loss_mask: 0.3770  decode.loss_dice: 0.2878  decode.d0.loss_cls: 0.9689  decode.d0.loss_mask: 0.3498  decode.d0.loss_dice: 0.2748  decode.d1.loss_cls: 0.2519  decode.d1.loss_mask: 0.3510  decode.d1.loss_dice: 0.3012  decode.d2.loss_cls: 0.2312  decode.d2.loss_mask: 0.3447  decode.d2.loss_dice: 0.2915  decode.d3.loss_cls: 0.2222  decode.d3.loss_mask: 0.3619  decode.d3.loss_dice: 0.3084  decode.d4.loss_cls: 0.1847  decode.d4.loss_mask: 0.3502  decode.d4.loss_dice: 0.2745  decode.d5.loss_cls: 0.1977  decode.d5.loss_mask: 0.3550  decode.d5.loss_dice: 0.3181  decode.d6.loss_cls: 0.1792  decode.d6.loss_mask: 0.3554  decode.d6.loss_dice: 0.3029  decode.d7.loss_cls: 0.2054  decode.d7.loss_mask: 0.3591  decode.d7.loss_dice: 0.3079  decode.d8.loss_cls: 0.1654  decode.d8.loss_mask: 0.3874  decode.d8.loss_dice: 0.2940
08/06 04:21:36 - mmengine - INFO - Iter(train) [ 17150/320000]  base_lr: 9.5164e-05 lr: 9.5164e-06  eta: 1 day, 12:44:27  time: 0.4387  data_time: 0.0095  memory: 5242  grad_norm: 88.9164  loss: 7.2406  decode.loss_cls: 0.1423  decode.loss_mask: 0.1930  decode.loss_dice: 0.3071  decode.d0.loss_cls: 0.8332  decode.d0.loss_mask: 0.2007  decode.d0.loss_dice: 0.3250  decode.d1.loss_cls: 0.1985  decode.d1.loss_mask: 0.1969  decode.d1.loss_dice: 0.2519  decode.d2.loss_cls: 0.1618  decode.d2.loss_mask: 0.1959  decode.d2.loss_dice: 0.2671  decode.d3.loss_cls: 0.1386  decode.d3.loss_mask: 0.1962  decode.d3.loss_dice: 0.2712  decode.d4.loss_cls: 0.1841  decode.d4.loss_mask: 0.1974  decode.d4.loss_dice: 0.2962  decode.d5.loss_cls: 0.2203  decode.d5.loss_mask: 0.1943  decode.d5.loss_dice: 0.3062  decode.d6.loss_cls: 0.1659  decode.d6.loss_mask: 0.2003  decode.d6.loss_dice: 0.2837  decode.d7.loss_cls: 0.1779  decode.d7.loss_mask: 0.1962  decode.d7.loss_dice: 0.3027  decode.d8.loss_cls: 0.1503  decode.d8.loss_mask: 0.1972  decode.d8.loss_dice: 0.2883
08/06 04:21:58 - mmengine - INFO - Iter(train) [ 17200/320000]  base_lr: 9.5150e-05 lr: 9.5150e-06  eta: 1 day, 12:44:06  time: 0.4375  data_time: 0.0093  memory: 5260  grad_norm: 89.5740  loss: 10.1018  decode.loss_cls: 0.3104  decode.loss_mask: 0.3118  decode.loss_dice: 0.2983  decode.d0.loss_cls: 1.0020  decode.d0.loss_mask: 0.3396  decode.d0.loss_dice: 0.2847  decode.d1.loss_cls: 0.4129  decode.d1.loss_mask: 0.3277  decode.d1.loss_dice: 0.2798  decode.d2.loss_cls: 0.3188  decode.d2.loss_mask: 0.3273  decode.d2.loss_dice: 0.3234  decode.d3.loss_cls: 0.3659  decode.d3.loss_mask: 0.3239  decode.d3.loss_dice: 0.2648  decode.d4.loss_cls: 0.3815  decode.d4.loss_mask: 0.3222  decode.d4.loss_dice: 0.2863  decode.d5.loss_cls: 0.3031  decode.d5.loss_mask: 0.3180  decode.d5.loss_dice: 0.2684  decode.d6.loss_cls: 0.3056  decode.d6.loss_mask: 0.3144  decode.d6.loss_dice: 0.2962  decode.d7.loss_cls: 0.3284  decode.d7.loss_mask: 0.3177  decode.d7.loss_dice: 0.3019  decode.d8.loss_cls: 0.2642  decode.d8.loss_mask: 0.3156  decode.d8.loss_dice: 0.2869
08/06 04:22:20 - mmengine - INFO - Iter(train) [ 17250/320000]  base_lr: 9.5135e-05 lr: 9.5135e-06  eta: 1 day, 12:43:45  time: 0.4382  data_time: 0.0092  memory: 5242  grad_norm: 122.1191  loss: 7.2526  decode.loss_cls: 0.1118  decode.loss_mask: 0.2677  decode.loss_dice: 0.2330  decode.d0.loss_cls: 0.9037  decode.d0.loss_mask: 0.2823  decode.d0.loss_dice: 0.2790  decode.d1.loss_cls: 0.1430  decode.d1.loss_mask: 0.2713  decode.d1.loss_dice: 0.2411  decode.d2.loss_cls: 0.1358  decode.d2.loss_mask: 0.2817  decode.d2.loss_dice: 0.2456  decode.d3.loss_cls: 0.1013  decode.d3.loss_mask: 0.2745  decode.d3.loss_dice: 0.2377  decode.d4.loss_cls: 0.0892  decode.d4.loss_mask: 0.2726  decode.d4.loss_dice: 0.2426  decode.d5.loss_cls: 0.1209  decode.d5.loss_mask: 0.2746  decode.d5.loss_dice: 0.2490  decode.d6.loss_cls: 0.1588  decode.d6.loss_mask: 0.2650  decode.d6.loss_dice: 0.2452  decode.d7.loss_cls: 0.1518  decode.d7.loss_mask: 0.2577  decode.d7.loss_dice: 0.2392  decode.d8.loss_cls: 0.1628  decode.d8.loss_mask: 0.2681  decode.d8.loss_dice: 0.2455
08/06 04:22:42 - mmengine - INFO - Iter(train) [ 17300/320000]  base_lr: 9.5121e-05 lr: 9.5121e-06  eta: 1 day, 12:43:24  time: 0.4373  data_time: 0.0093  memory: 5242  grad_norm: 122.8677  loss: 11.8285  decode.loss_cls: 0.3169  decode.loss_mask: 0.3179  decode.loss_dice: 0.3538  decode.d0.loss_cls: 1.1162  decode.d0.loss_mask: 0.3120  decode.d0.loss_dice: 0.3696  decode.d1.loss_cls: 0.5549  decode.d1.loss_mask: 0.2936  decode.d1.loss_dice: 0.3632  decode.d2.loss_cls: 0.4875  decode.d2.loss_mask: 0.3142  decode.d2.loss_dice: 0.3546  decode.d3.loss_cls: 0.4567  decode.d3.loss_mask: 0.3071  decode.d3.loss_dice: 0.3499  decode.d4.loss_cls: 0.4419  decode.d4.loss_mask: 0.3123  decode.d4.loss_dice: 0.3888  decode.d5.loss_cls: 0.3440  decode.d5.loss_mask: 0.3204  decode.d5.loss_dice: 0.4072  decode.d6.loss_cls: 0.4645  decode.d6.loss_mask: 0.3023  decode.d6.loss_dice: 0.3498  decode.d7.loss_cls: 0.4232  decode.d7.loss_mask: 0.3117  decode.d7.loss_dice: 0.3683  decode.d8.loss_cls: 0.4462  decode.d8.loss_mask: 0.3048  decode.d8.loss_dice: 0.3751
08/06 04:23:04 - mmengine - INFO - Iter(train) [ 17350/320000]  base_lr: 9.5107e-05 lr: 9.5107e-06  eta: 1 day, 12:43:03  time: 0.4377  data_time: 0.0092  memory: 5242  grad_norm: 129.4847  loss: 9.9240  decode.loss_cls: 0.4019  decode.loss_mask: 0.2549  decode.loss_dice: 0.2948  decode.d0.loss_cls: 1.0651  decode.d0.loss_mask: 0.2762  decode.d0.loss_dice: 0.3189  decode.d1.loss_cls: 0.4000  decode.d1.loss_mask: 0.2665  decode.d1.loss_dice: 0.2961  decode.d2.loss_cls: 0.4069  decode.d2.loss_mask: 0.2413  decode.d2.loss_dice: 0.2898  decode.d3.loss_cls: 0.3802  decode.d3.loss_mask: 0.2426  decode.d3.loss_dice: 0.2883  decode.d4.loss_cls: 0.3315  decode.d4.loss_mask: 0.2648  decode.d4.loss_dice: 0.3125  decode.d5.loss_cls: 0.2939  decode.d5.loss_mask: 0.2580  decode.d5.loss_dice: 0.2943  decode.d6.loss_cls: 0.3222  decode.d6.loss_mask: 0.2649  decode.d6.loss_dice: 0.2878  decode.d7.loss_cls: 0.3720  decode.d7.loss_mask: 0.2558  decode.d7.loss_dice: 0.2702  decode.d8.loss_cls: 0.4356  decode.d8.loss_mask: 0.2556  decode.d8.loss_dice: 0.2814
08/06 04:23:26 - mmengine - INFO - Iter(train) [ 17400/320000]  base_lr: 9.5093e-05 lr: 9.5093e-06  eta: 1 day, 12:42:42  time: 0.4386  data_time: 0.0092  memory: 5260  grad_norm: 87.0967  loss: 8.4303  decode.loss_cls: 0.2215  decode.loss_mask: 0.2558  decode.loss_dice: 0.2738  decode.d0.loss_cls: 0.9189  decode.d0.loss_mask: 0.2640  decode.d0.loss_dice: 0.2938  decode.d1.loss_cls: 0.2803  decode.d1.loss_mask: 0.2553  decode.d1.loss_dice: 0.2636  decode.d2.loss_cls: 0.2345  decode.d2.loss_mask: 0.2623  decode.d2.loss_dice: 0.2608  decode.d3.loss_cls: 0.2374  decode.d3.loss_mask: 0.2515  decode.d3.loss_dice: 0.2716  decode.d4.loss_cls: 0.2461  decode.d4.loss_mask: 0.2517  decode.d4.loss_dice: 0.2749  decode.d5.loss_cls: 0.2551  decode.d5.loss_mask: 0.2570  decode.d5.loss_dice: 0.2632  decode.d6.loss_cls: 0.2556  decode.d6.loss_mask: 0.2539  decode.d6.loss_dice: 0.2802  decode.d7.loss_cls: 0.2714  decode.d7.loss_mask: 0.2591  decode.d7.loss_dice: 0.2647  decode.d8.loss_cls: 0.2250  decode.d8.loss_mask: 0.2582  decode.d8.loss_dice: 0.2691
08/06 04:23:48 - mmengine - INFO - Iter(train) [ 17450/320000]  base_lr: 9.5079e-05 lr: 9.5079e-06  eta: 1 day, 12:42:21  time: 0.4374  data_time: 0.0093  memory: 5242  grad_norm: 205.2258  loss: 9.1980  decode.loss_cls: 0.1424  decode.loss_mask: 0.3589  decode.loss_dice: 0.3055  decode.d0.loss_cls: 0.8980  decode.d0.loss_mask: 0.3784  decode.d0.loss_dice: 0.3467  decode.d1.loss_cls: 0.2014  decode.d1.loss_mask: 0.3683  decode.d1.loss_dice: 0.3285  decode.d2.loss_cls: 0.1465  decode.d2.loss_mask: 0.3570  decode.d2.loss_dice: 0.3250  decode.d3.loss_cls: 0.1528  decode.d3.loss_mask: 0.3608  decode.d3.loss_dice: 0.3119  decode.d4.loss_cls: 0.1537  decode.d4.loss_mask: 0.3643  decode.d4.loss_dice: 0.3285  decode.d5.loss_cls: 0.1639  decode.d5.loss_mask: 0.3664  decode.d5.loss_dice: 0.3302  decode.d6.loss_cls: 0.1497  decode.d6.loss_mask: 0.3686  decode.d6.loss_dice: 0.3209  decode.d7.loss_cls: 0.1555  decode.d7.loss_mask: 0.3618  decode.d7.loss_dice: 0.3265  decode.d8.loss_cls: 0.1519  decode.d8.loss_mask: 0.3569  decode.d8.loss_dice: 0.3171
08/06 04:24:10 - mmengine - INFO - Iter(train) [ 17500/320000]  base_lr: 9.5065e-05 lr: 9.5065e-06  eta: 1 day, 12:42:00  time: 0.4383  data_time: 0.0092  memory: 5242  grad_norm: 136.4828  loss: 9.3312  decode.loss_cls: 0.2741  decode.loss_mask: 0.1979  decode.loss_dice: 0.3297  decode.d0.loss_cls: 0.8566  decode.d0.loss_mask: 0.2025  decode.d0.loss_dice: 0.3222  decode.d1.loss_cls: 0.3307  decode.d1.loss_mask: 0.1956  decode.d1.loss_dice: 0.2875  decode.d2.loss_cls: 0.3452  decode.d2.loss_mask: 0.2185  decode.d2.loss_dice: 0.3222  decode.d3.loss_cls: 0.3072  decode.d3.loss_mask: 0.2026  decode.d3.loss_dice: 0.3102  decode.d4.loss_cls: 0.3972  decode.d4.loss_mask: 0.2098  decode.d4.loss_dice: 0.3005  decode.d5.loss_cls: 0.3810  decode.d5.loss_mask: 0.2295  decode.d5.loss_dice: 0.3319  decode.d6.loss_cls: 0.3987  decode.d6.loss_mask: 0.2106  decode.d6.loss_dice: 0.3143  decode.d7.loss_cls: 0.3901  decode.d7.loss_mask: 0.2222  decode.d7.loss_dice: 0.3141  decode.d8.loss_cls: 0.3854  decode.d8.loss_mask: 0.2280  decode.d8.loss_dice: 0.3153
08/06 04:24:32 - mmengine - INFO - Iter(train) [ 17550/320000]  base_lr: 9.5051e-05 lr: 9.5051e-06  eta: 1 day, 12:41:40  time: 0.4387  data_time: 0.0092  memory: 5260  grad_norm: 122.1506  loss: 6.9183  decode.loss_cls: 0.1073  decode.loss_mask: 0.2233  decode.loss_dice: 0.2555  decode.d0.loss_cls: 0.9043  decode.d0.loss_mask: 0.2531  decode.d0.loss_dice: 0.2682  decode.d1.loss_cls: 0.1137  decode.d1.loss_mask: 0.2340  decode.d1.loss_dice: 0.2703  decode.d2.loss_cls: 0.1145  decode.d2.loss_mask: 0.2395  decode.d2.loss_dice: 0.2655  decode.d3.loss_cls: 0.1283  decode.d3.loss_mask: 0.2336  decode.d3.loss_dice: 0.2618  decode.d4.loss_cls: 0.1098  decode.d4.loss_mask: 0.2411  decode.d4.loss_dice: 0.2636  decode.d5.loss_cls: 0.1112  decode.d5.loss_mask: 0.2395  decode.d5.loss_dice: 0.2662  decode.d6.loss_cls: 0.1105  decode.d6.loss_mask: 0.2317  decode.d6.loss_dice: 0.2530  decode.d7.loss_cls: 0.1284  decode.d7.loss_mask: 0.2233  decode.d7.loss_dice: 0.2601  decode.d8.loss_cls: 0.1206  decode.d8.loss_mask: 0.2292  decode.d8.loss_dice: 0.2570
08/06 04:24:54 - mmengine - INFO - Iter(train) [ 17600/320000]  base_lr: 9.5036e-05 lr: 9.5036e-06  eta: 1 day, 12:41:22  time: 0.4554  data_time: 0.0092  memory: 5260  grad_norm: 209.6913  loss: 11.8207  decode.loss_cls: 0.2700  decode.loss_mask: 0.4116  decode.loss_dice: 0.4064  decode.d0.loss_cls: 1.0819  decode.d0.loss_mask: 0.3471  decode.d0.loss_dice: 0.3909  decode.d1.loss_cls: 0.4641  decode.d1.loss_mask: 0.3440  decode.d1.loss_dice: 0.3915  decode.d2.loss_cls: 0.3254  decode.d2.loss_mask: 0.3653  decode.d2.loss_dice: 0.4064  decode.d3.loss_cls: 0.2645  decode.d3.loss_mask: 0.4178  decode.d3.loss_dice: 0.4099  decode.d4.loss_cls: 0.3512  decode.d4.loss_mask: 0.3826  decode.d4.loss_dice: 0.4023  decode.d5.loss_cls: 0.3233  decode.d5.loss_mask: 0.3710  decode.d5.loss_dice: 0.3949  decode.d6.loss_cls: 0.2442  decode.d6.loss_mask: 0.4174  decode.d6.loss_dice: 0.4039  decode.d7.loss_cls: 0.3293  decode.d7.loss_mask: 0.4074  decode.d7.loss_dice: 0.4218  decode.d8.loss_cls: 0.2639  decode.d8.loss_mask: 0.4111  decode.d8.loss_dice: 0.4000
08/06 04:25:15 - mmengine - INFO - Iter(train) [ 17650/320000]  base_lr: 9.5022e-05 lr: 9.5022e-06  eta: 1 day, 12:41:01  time: 0.4379  data_time: 0.0093  memory: 5242  grad_norm: 86.1106  loss: 10.4516  decode.loss_cls: 0.3119  decode.loss_mask: 0.2748  decode.loss_dice: 0.3691  decode.d0.loss_cls: 1.0395  decode.d0.loss_mask: 0.2768  decode.d0.loss_dice: 0.3701  decode.d1.loss_cls: 0.3420  decode.d1.loss_mask: 0.2764  decode.d1.loss_dice: 0.3738  decode.d2.loss_cls: 0.3766  decode.d2.loss_mask: 0.2710  decode.d2.loss_dice: 0.3537  decode.d3.loss_cls: 0.3601  decode.d3.loss_mask: 0.2706  decode.d3.loss_dice: 0.3683  decode.d4.loss_cls: 0.3886  decode.d4.loss_mask: 0.2726  decode.d4.loss_dice: 0.3634  decode.d5.loss_cls: 0.3149  decode.d5.loss_mask: 0.2732  decode.d5.loss_dice: 0.3554  decode.d6.loss_cls: 0.3413  decode.d6.loss_mask: 0.2769  decode.d6.loss_dice: 0.3626  decode.d7.loss_cls: 0.3247  decode.d7.loss_mask: 0.2744  decode.d7.loss_dice: 0.3524  decode.d8.loss_cls: 0.2744  decode.d8.loss_mask: 0.2737  decode.d8.loss_dice: 0.3686
08/06 04:25:37 - mmengine - INFO - Iter(train) [ 17700/320000]  base_lr: 9.5008e-05 lr: 9.5008e-06  eta: 1 day, 12:40:40  time: 0.4385  data_time: 0.0091  memory: 5275  grad_norm: 81.5283  loss: 7.8423  decode.loss_cls: 0.0882  decode.loss_mask: 0.2676  decode.loss_dice: 0.3001  decode.d0.loss_cls: 0.8508  decode.d0.loss_mask: 0.2793  decode.d0.loss_dice: 0.3185  decode.d1.loss_cls: 0.2857  decode.d1.loss_mask: 0.2688  decode.d1.loss_dice: 0.3116  decode.d2.loss_cls: 0.1590  decode.d2.loss_mask: 0.2640  decode.d2.loss_dice: 0.2997  decode.d3.loss_cls: 0.1294  decode.d3.loss_mask: 0.2622  decode.d3.loss_dice: 0.3030  decode.d4.loss_cls: 0.0856  decode.d4.loss_mask: 0.2711  decode.d4.loss_dice: 0.3027  decode.d5.loss_cls: 0.1194  decode.d5.loss_mask: 0.2732  decode.d5.loss_dice: 0.3017  decode.d6.loss_cls: 0.1691  decode.d6.loss_mask: 0.2649  decode.d6.loss_dice: 0.2880  decode.d7.loss_cls: 0.1379  decode.d7.loss_mask: 0.2684  decode.d7.loss_dice: 0.3102  decode.d8.loss_cls: 0.0814  decode.d8.loss_mask: 0.2707  decode.d8.loss_dice: 0.3102
08/06 04:25:59 - mmengine - INFO - Iter(train) [ 17750/320000]  base_lr: 9.4994e-05 lr: 9.4994e-06  eta: 1 day, 12:40:20  time: 0.4376  data_time: 0.0092  memory: 5242  grad_norm: 110.2326  loss: 10.2510  decode.loss_cls: 0.4303  decode.loss_mask: 0.3078  decode.loss_dice: 0.3102  decode.d0.loss_cls: 0.9174  decode.d0.loss_mask: 0.3183  decode.d0.loss_dice: 0.3250  decode.d1.loss_cls: 0.3064  decode.d1.loss_mask: 0.3052  decode.d1.loss_dice: 0.2986  decode.d2.loss_cls: 0.3848  decode.d2.loss_mask: 0.3009  decode.d2.loss_dice: 0.2965  decode.d3.loss_cls: 0.3808  decode.d3.loss_mask: 0.3058  decode.d3.loss_dice: 0.3019  decode.d4.loss_cls: 0.3158  decode.d4.loss_mask: 0.3078  decode.d4.loss_dice: 0.3004  decode.d5.loss_cls: 0.3109  decode.d5.loss_mask: 0.3046  decode.d5.loss_dice: 0.3134  decode.d6.loss_cls: 0.3065  decode.d6.loss_mask: 0.3075  decode.d6.loss_dice: 0.3070  decode.d7.loss_cls: 0.3212  decode.d7.loss_mask: 0.3145  decode.d7.loss_dice: 0.3097  decode.d8.loss_cls: 0.4260  decode.d8.loss_mask: 0.3096  decode.d8.loss_dice: 0.3062
08/06 04:26:21 - mmengine - INFO - Iter(train) [ 17800/320000]  base_lr: 9.4980e-05 lr: 9.4980e-06  eta: 1 day, 12:39:59  time: 0.4384  data_time: 0.0092  memory: 5242  grad_norm: 205.0024  loss: 10.2040  decode.loss_cls: 0.2383  decode.loss_mask: 0.3093  decode.loss_dice: 0.3384  decode.d0.loss_cls: 0.9402  decode.d0.loss_mask: 0.3530  decode.d0.loss_dice: 0.3667  decode.d1.loss_cls: 0.4545  decode.d1.loss_mask: 0.3043  decode.d1.loss_dice: 0.3311  decode.d2.loss_cls: 0.2973  decode.d2.loss_mask: 0.3090  decode.d2.loss_dice: 0.3294  decode.d3.loss_cls: 0.2903  decode.d3.loss_mask: 0.3015  decode.d3.loss_dice: 0.3254  decode.d4.loss_cls: 0.3358  decode.d4.loss_mask: 0.3015  decode.d4.loss_dice: 0.3275  decode.d5.loss_cls: 0.3013  decode.d5.loss_mask: 0.3079  decode.d5.loss_dice: 0.3430  decode.d6.loss_cls: 0.3124  decode.d6.loss_mask: 0.3056  decode.d6.loss_dice: 0.3288  decode.d7.loss_cls: 0.2955  decode.d7.loss_mask: 0.3054  decode.d7.loss_dice: 0.3403  decode.d8.loss_cls: 0.2616  decode.d8.loss_mask: 0.3101  decode.d8.loss_dice: 0.3387
08/06 04:26:43 - mmengine - INFO - Iter(train) [ 17850/320000]  base_lr: 9.4966e-05 lr: 9.4966e-06  eta: 1 day, 12:39:39  time: 0.4388  data_time: 0.0092  memory: 5236  grad_norm: 85.6383  loss: 8.6950  decode.loss_cls: 0.1376  decode.loss_mask: 0.2771  decode.loss_dice: 0.3597  decode.d0.loss_cls: 0.8232  decode.d0.loss_mask: 0.2813  decode.d0.loss_dice: 0.3422  decode.d1.loss_cls: 0.2228  decode.d1.loss_mask: 0.2795  decode.d1.loss_dice: 0.3396  decode.d2.loss_cls: 0.1522  decode.d2.loss_mask: 0.2932  decode.d2.loss_dice: 0.3719  decode.d3.loss_cls: 0.1676  decode.d3.loss_mask: 0.2962  decode.d3.loss_dice: 0.3529  decode.d4.loss_cls: 0.1912  decode.d4.loss_mask: 0.2807  decode.d4.loss_dice: 0.3352  decode.d5.loss_cls: 0.1861  decode.d5.loss_mask: 0.2805  decode.d5.loss_dice: 0.3500  decode.d6.loss_cls: 0.1753  decode.d6.loss_mask: 0.2747  decode.d6.loss_dice: 0.3421  decode.d7.loss_cls: 0.2122  decode.d7.loss_mask: 0.2692  decode.d7.loss_dice: 0.3369  decode.d8.loss_cls: 0.1421  decode.d8.loss_mask: 0.2822  decode.d8.loss_dice: 0.3399
08/06 04:27:05 - mmengine - INFO - Iter(train) [ 17900/320000]  base_lr: 9.4952e-05 lr: 9.4952e-06  eta: 1 day, 12:39:18  time: 0.4380  data_time: 0.0090  memory: 5258  grad_norm: 104.8159  loss: 9.7156  decode.loss_cls: 0.2954  decode.loss_mask: 0.2788  decode.loss_dice: 0.3157  decode.d0.loss_cls: 1.0843  decode.d0.loss_mask: 0.2897  decode.d0.loss_dice: 0.3237  decode.d1.loss_cls: 0.3305  decode.d1.loss_mask: 0.2912  decode.d1.loss_dice: 0.3282  decode.d2.loss_cls: 0.2846  decode.d2.loss_mask: 0.2848  decode.d2.loss_dice: 0.3182  decode.d3.loss_cls: 0.2427  decode.d3.loss_mask: 0.2908  decode.d3.loss_dice: 0.2897  decode.d4.loss_cls: 0.2850  decode.d4.loss_mask: 0.2798  decode.d4.loss_dice: 0.3083  decode.d5.loss_cls: 0.2568  decode.d5.loss_mask: 0.2831  decode.d5.loss_dice: 0.3540  decode.d6.loss_cls: 0.2739  decode.d6.loss_mask: 0.2877  decode.d6.loss_dice: 0.3515  decode.d7.loss_cls: 0.2787  decode.d7.loss_mask: 0.2827  decode.d7.loss_dice: 0.3251  decode.d8.loss_cls: 0.2838  decode.d8.loss_mask: 0.2823  decode.d8.loss_dice: 0.3344
08/06 04:27:27 - mmengine - INFO - Iter(train) [ 17950/320000]  base_lr: 9.4937e-05 lr: 9.4937e-06  eta: 1 day, 12:38:57  time: 0.4388  data_time: 0.0092  memory: 5299  grad_norm: 137.4590  loss: 10.2123  decode.loss_cls: 0.1893  decode.loss_mask: 0.3304  decode.loss_dice: 0.4082  decode.d0.loss_cls: 0.9629  decode.d0.loss_mask: 0.3508  decode.d0.loss_dice: 0.4217  decode.d1.loss_cls: 0.2399  decode.d1.loss_mask: 0.3364  decode.d1.loss_dice: 0.4136  decode.d2.loss_cls: 0.2108  decode.d2.loss_mask: 0.3330  decode.d2.loss_dice: 0.4150  decode.d3.loss_cls: 0.2232  decode.d3.loss_mask: 0.3266  decode.d3.loss_dice: 0.3996  decode.d4.loss_cls: 0.1967  decode.d4.loss_mask: 0.3372  decode.d4.loss_dice: 0.4289  decode.d5.loss_cls: 0.1672  decode.d5.loss_mask: 0.3425  decode.d5.loss_dice: 0.4149  decode.d6.loss_cls: 0.1824  decode.d6.loss_mask: 0.3254  decode.d6.loss_dice: 0.3992  decode.d7.loss_cls: 0.1961  decode.d7.loss_mask: 0.3282  decode.d7.loss_dice: 0.4202  decode.d8.loss_cls: 0.1709  decode.d8.loss_mask: 0.3251  decode.d8.loss_dice: 0.4158
08/06 04:27:49 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 04:27:49 - mmengine - INFO - Iter(train) [ 18000/320000]  base_lr: 9.4923e-05 lr: 9.4923e-06  eta: 1 day, 12:38:36  time: 0.4378  data_time: 0.0092  memory: 5260  grad_norm: 200.5560  loss: 10.3407  decode.loss_cls: 0.3297  decode.loss_mask: 0.2747  decode.loss_dice: 0.3387  decode.d0.loss_cls: 0.9742  decode.d0.loss_mask: 0.2698  decode.d0.loss_dice: 0.3645  decode.d1.loss_cls: 0.3364  decode.d1.loss_mask: 0.2648  decode.d1.loss_dice: 0.3199  decode.d2.loss_cls: 0.3579  decode.d2.loss_mask: 0.2619  decode.d2.loss_dice: 0.3256  decode.d3.loss_cls: 0.3542  decode.d3.loss_mask: 0.2636  decode.d3.loss_dice: 0.3427  decode.d4.loss_cls: 0.4135  decode.d4.loss_mask: 0.2644  decode.d4.loss_dice: 0.3654  decode.d5.loss_cls: 0.3559  decode.d5.loss_mask: 0.2800  decode.d5.loss_dice: 0.3494  decode.d6.loss_cls: 0.3685  decode.d6.loss_mask: 0.2666  decode.d6.loss_dice: 0.3553  decode.d7.loss_cls: 0.3773  decode.d7.loss_mask: 0.2627  decode.d7.loss_dice: 0.3436  decode.d8.loss_cls: 0.3910  decode.d8.loss_mask: 0.2580  decode.d8.loss_dice: 0.3104
08/06 04:28:11 - mmengine - INFO - Iter(train) [ 18050/320000]  base_lr: 9.4909e-05 lr: 9.4909e-06  eta: 1 day, 12:38:15  time: 0.4371  data_time: 0.0092  memory: 5221  grad_norm: 88.0710  loss: 9.3958  decode.loss_cls: 0.2893  decode.loss_mask: 0.2877  decode.loss_dice: 0.2966  decode.d0.loss_cls: 0.8674  decode.d0.loss_mask: 0.2953  decode.d0.loss_dice: 0.2876  decode.d1.loss_cls: 0.3547  decode.d1.loss_mask: 0.2744  decode.d1.loss_dice: 0.2940  decode.d2.loss_cls: 0.3523  decode.d2.loss_mask: 0.2725  decode.d2.loss_dice: 0.2787  decode.d3.loss_cls: 0.2777  decode.d3.loss_mask: 0.2771  decode.d3.loss_dice: 0.2772  decode.d4.loss_cls: 0.3052  decode.d4.loss_mask: 0.2716  decode.d4.loss_dice: 0.2788  decode.d5.loss_cls: 0.3124  decode.d5.loss_mask: 0.2733  decode.d5.loss_dice: 0.2874  decode.d6.loss_cls: 0.3178  decode.d6.loss_mask: 0.2789  decode.d6.loss_dice: 0.2863  decode.d7.loss_cls: 0.2989  decode.d7.loss_mask: 0.2801  decode.d7.loss_dice: 0.2889  decode.d8.loss_cls: 0.3607  decode.d8.loss_mask: 0.2760  decode.d8.loss_dice: 0.2972
08/06 04:28:33 - mmengine - INFO - Iter(train) [ 18100/320000]  base_lr: 9.4895e-05 lr: 9.4895e-06  eta: 1 day, 12:37:54  time: 0.4375  data_time: 0.0090  memory: 5260  grad_norm: 94.3587  loss: 8.7449  decode.loss_cls: 0.1732  decode.loss_mask: 0.2683  decode.loss_dice: 0.3208  decode.d0.loss_cls: 1.0142  decode.d0.loss_mask: 0.2842  decode.d0.loss_dice: 0.3525  decode.d1.loss_cls: 0.2664  decode.d1.loss_mask: 0.2272  decode.d1.loss_dice: 0.3197  decode.d2.loss_cls: 0.3263  decode.d2.loss_mask: 0.2214  decode.d2.loss_dice: 0.3276  decode.d3.loss_cls: 0.2140  decode.d3.loss_mask: 0.2645  decode.d3.loss_dice: 0.3198  decode.d4.loss_cls: 0.1925  decode.d4.loss_mask: 0.2748  decode.d4.loss_dice: 0.3126  decode.d5.loss_cls: 0.1587  decode.d5.loss_mask: 0.2765  decode.d5.loss_dice: 0.3276  decode.d6.loss_cls: 0.1598  decode.d6.loss_mask: 0.2754  decode.d6.loss_dice: 0.3348  decode.d7.loss_cls: 0.1601  decode.d7.loss_mask: 0.2637  decode.d7.loss_dice: 0.3239  decode.d8.loss_cls: 0.1910  decode.d8.loss_mask: 0.2657  decode.d8.loss_dice: 0.3277
08/06 04:28:54 - mmengine - INFO - Iter(train) [ 18150/320000]  base_lr: 9.4881e-05 lr: 9.4881e-06  eta: 1 day, 12:37:32  time: 0.4378  data_time: 0.0092  memory: 5236  grad_norm: 76.6317  loss: 8.4994  decode.loss_cls: 0.2337  decode.loss_mask: 0.2278  decode.loss_dice: 0.3324  decode.d0.loss_cls: 0.9230  decode.d0.loss_mask: 0.2311  decode.d0.loss_dice: 0.3233  decode.d1.loss_cls: 0.3108  decode.d1.loss_mask: 0.2238  decode.d1.loss_dice: 0.3014  decode.d2.loss_cls: 0.2199  decode.d2.loss_mask: 0.2276  decode.d2.loss_dice: 0.3316  decode.d3.loss_cls: 0.2260  decode.d3.loss_mask: 0.2289  decode.d3.loss_dice: 0.3158  decode.d4.loss_cls: 0.2127  decode.d4.loss_mask: 0.2269  decode.d4.loss_dice: 0.3407  decode.d5.loss_cls: 0.1983  decode.d5.loss_mask: 0.2218  decode.d5.loss_dice: 0.3249  decode.d6.loss_cls: 0.2388  decode.d6.loss_mask: 0.2209  decode.d6.loss_dice: 0.2995  decode.d7.loss_cls: 0.2194  decode.d7.loss_mask: 0.2278  decode.d7.loss_dice: 0.3064  decode.d8.loss_cls: 0.2585  decode.d8.loss_mask: 0.2242  decode.d8.loss_dice: 0.3216
08/06 04:29:16 - mmengine - INFO - Iter(train) [ 18200/320000]  base_lr: 9.4867e-05 lr: 9.4867e-06  eta: 1 day, 12:37:11  time: 0.4370  data_time: 0.0090  memory: 5258  grad_norm: 147.3455  loss: 8.2344  decode.loss_cls: 0.1745  decode.loss_mask: 0.2387  decode.loss_dice: 0.2753  decode.d0.loss_cls: 1.0194  decode.d0.loss_mask: 0.2276  decode.d0.loss_dice: 0.2862  decode.d1.loss_cls: 0.3143  decode.d1.loss_mask: 0.2255  decode.d1.loss_dice: 0.2767  decode.d2.loss_cls: 0.2411  decode.d2.loss_mask: 0.2359  decode.d2.loss_dice: 0.2780  decode.d3.loss_cls: 0.1915  decode.d3.loss_mask: 0.2346  decode.d3.loss_dice: 0.2906  decode.d4.loss_cls: 0.2213  decode.d4.loss_mask: 0.2381  decode.d4.loss_dice: 0.2974  decode.d5.loss_cls: 0.2349  decode.d5.loss_mask: 0.2344  decode.d5.loss_dice: 0.2866  decode.d6.loss_cls: 0.2219  decode.d6.loss_mask: 0.2264  decode.d6.loss_dice: 0.2772  decode.d7.loss_cls: 0.2433  decode.d7.loss_mask: 0.2272  decode.d7.loss_dice: 0.2849  decode.d8.loss_cls: 0.2301  decode.d8.loss_mask: 0.2289  decode.d8.loss_dice: 0.2719
08/06 04:29:38 - mmengine - INFO - Iter(train) [ 18250/320000]  base_lr: 9.4853e-05 lr: 9.4853e-06  eta: 1 day, 12:36:49  time: 0.4369  data_time: 0.0092  memory: 5242  grad_norm: 149.1384  loss: 13.0472  decode.loss_cls: 0.6519  decode.loss_mask: 0.3333  decode.loss_dice: 0.3291  decode.d0.loss_cls: 1.2112  decode.d0.loss_mask: 0.3299  decode.d0.loss_dice: 0.3389  decode.d1.loss_cls: 0.6093  decode.d1.loss_mask: 0.3182  decode.d1.loss_dice: 0.3104  decode.d2.loss_cls: 0.4945  decode.d2.loss_mask: 0.3104  decode.d2.loss_dice: 0.3128  decode.d3.loss_cls: 0.5336  decode.d3.loss_mask: 0.3137  decode.d3.loss_dice: 0.3251  decode.d4.loss_cls: 0.5197  decode.d4.loss_mask: 0.3004  decode.d4.loss_dice: 0.3135  decode.d5.loss_cls: 0.6256  decode.d5.loss_mask: 0.3060  decode.d5.loss_dice: 0.3122  decode.d6.loss_cls: 0.7299  decode.d6.loss_mask: 0.3014  decode.d6.loss_dice: 0.3059  decode.d7.loss_cls: 0.7108  decode.d7.loss_mask: 0.3097  decode.d7.loss_dice: 0.3144  decode.d8.loss_cls: 0.6264  decode.d8.loss_mask: 0.3191  decode.d8.loss_dice: 0.3301
08/06 04:30:00 - mmengine - INFO - Iter(train) [ 18300/320000]  base_lr: 9.4838e-05 lr: 9.4838e-06  eta: 1 day, 12:36:28  time: 0.4375  data_time: 0.0091  memory: 5242  grad_norm: 73.8367  loss: 8.4948  decode.loss_cls: 0.1657  decode.loss_mask: 0.2607  decode.loss_dice: 0.2943  decode.d0.loss_cls: 1.0104  decode.d0.loss_mask: 0.2631  decode.d0.loss_dice: 0.2813  decode.d1.loss_cls: 0.2764  decode.d1.loss_mask: 0.2640  decode.d1.loss_dice: 0.3221  decode.d2.loss_cls: 0.2542  decode.d2.loss_mask: 0.2556  decode.d2.loss_dice: 0.3007  decode.d3.loss_cls: 0.2822  decode.d3.loss_mask: 0.2543  decode.d3.loss_dice: 0.3047  decode.d4.loss_cls: 0.1826  decode.d4.loss_mask: 0.2566  decode.d4.loss_dice: 0.2876  decode.d5.loss_cls: 0.2276  decode.d5.loss_mask: 0.2607  decode.d5.loss_dice: 0.2709  decode.d6.loss_cls: 0.1874  decode.d6.loss_mask: 0.2656  decode.d6.loss_dice: 0.3005  decode.d7.loss_cls: 0.1905  decode.d7.loss_mask: 0.2560  decode.d7.loss_dice: 0.3018  decode.d8.loss_cls: 0.1608  decode.d8.loss_mask: 0.2601  decode.d8.loss_dice: 0.2961
08/06 04:30:22 - mmengine - INFO - Iter(train) [ 18350/320000]  base_lr: 9.4824e-05 lr: 9.4824e-06  eta: 1 day, 12:36:07  time: 0.4383  data_time: 0.0093  memory: 5242  grad_norm: 158.6087  loss: 8.3820  decode.loss_cls: 0.1570  decode.loss_mask: 0.2819  decode.loss_dice: 0.3100  decode.d0.loss_cls: 0.9283  decode.d0.loss_mask: 0.2756  decode.d0.loss_dice: 0.3415  decode.d1.loss_cls: 0.2781  decode.d1.loss_mask: 0.2932  decode.d1.loss_dice: 0.3239  decode.d2.loss_cls: 0.1957  decode.d2.loss_mask: 0.2847  decode.d2.loss_dice: 0.3061  decode.d3.loss_cls: 0.2014  decode.d3.loss_mask: 0.2841  decode.d3.loss_dice: 0.3082  decode.d4.loss_cls: 0.1372  decode.d4.loss_mask: 0.2803  decode.d4.loss_dice: 0.3274  decode.d5.loss_cls: 0.1040  decode.d5.loss_mask: 0.2804  decode.d5.loss_dice: 0.3244  decode.d6.loss_cls: 0.0879  decode.d6.loss_mask: 0.2759  decode.d6.loss_dice: 0.3228  decode.d7.loss_cls: 0.1556  decode.d7.loss_mask: 0.2825  decode.d7.loss_dice: 0.3043  decode.d8.loss_cls: 0.1371  decode.d8.loss_mask: 0.2793  decode.d8.loss_dice: 0.3132
08/06 04:30:44 - mmengine - INFO - Iter(train) [ 18400/320000]  base_lr: 9.4810e-05 lr: 9.4810e-06  eta: 1 day, 12:35:45  time: 0.4364  data_time: 0.0090  memory: 5224  grad_norm: 146.7899  loss: 9.7632  decode.loss_cls: 0.4050  decode.loss_mask: 0.2573  decode.loss_dice: 0.2783  decode.d0.loss_cls: 0.9833  decode.d0.loss_mask: 0.2768  decode.d0.loss_dice: 0.3557  decode.d1.loss_cls: 0.4133  decode.d1.loss_mask: 0.2637  decode.d1.loss_dice: 0.2904  decode.d2.loss_cls: 0.3257  decode.d2.loss_mask: 0.2608  decode.d2.loss_dice: 0.2939  decode.d3.loss_cls: 0.3271  decode.d3.loss_mask: 0.2563  decode.d3.loss_dice: 0.2923  decode.d4.loss_cls: 0.3370  decode.d4.loss_mask: 0.2562  decode.d4.loss_dice: 0.2566  decode.d5.loss_cls: 0.3893  decode.d5.loss_mask: 0.2574  decode.d5.loss_dice: 0.2824  decode.d6.loss_cls: 0.3845  decode.d6.loss_mask: 0.2584  decode.d6.loss_dice: 0.2824  decode.d7.loss_cls: 0.3415  decode.d7.loss_mask: 0.2602  decode.d7.loss_dice: 0.2880  decode.d8.loss_cls: 0.3586  decode.d8.loss_mask: 0.2579  decode.d8.loss_dice: 0.2729
08/06 04:31:06 - mmengine - INFO - Iter(train) [ 18450/320000]  base_lr: 9.4796e-05 lr: 9.4796e-06  eta: 1 day, 12:35:24  time: 0.4380  data_time: 0.0090  memory: 5275  grad_norm: 89.1050  loss: 8.9006  decode.loss_cls: 0.2443  decode.loss_mask: 0.2102  decode.loss_dice: 0.2966  decode.d0.loss_cls: 1.0817  decode.d0.loss_mask: 0.2239  decode.d0.loss_dice: 0.3151  decode.d1.loss_cls: 0.3156  decode.d1.loss_mask: 0.2053  decode.d1.loss_dice: 0.3253  decode.d2.loss_cls: 0.3045  decode.d2.loss_mask: 0.2013  decode.d2.loss_dice: 0.3086  decode.d3.loss_cls: 0.2732  decode.d3.loss_mask: 0.2055  decode.d3.loss_dice: 0.3126  decode.d4.loss_cls: 0.3412  decode.d4.loss_mask: 0.2001  decode.d4.loss_dice: 0.2995  decode.d5.loss_cls: 0.2655  decode.d5.loss_mask: 0.2182  decode.d5.loss_dice: 0.2817  decode.d6.loss_cls: 0.2910  decode.d6.loss_mask: 0.2016  decode.d6.loss_dice: 0.3193  decode.d7.loss_cls: 0.3074  decode.d7.loss_mask: 0.2068  decode.d7.loss_dice: 0.3162  decode.d8.loss_cls: 0.3148  decode.d8.loss_mask: 0.2039  decode.d8.loss_dice: 0.3099
08/06 04:31:28 - mmengine - INFO - Iter(train) [ 18500/320000]  base_lr: 9.4782e-05 lr: 9.4782e-06  eta: 1 day, 12:35:03  time: 0.4365  data_time: 0.0091  memory: 5242  grad_norm: 80.5804  loss: 8.0876  decode.loss_cls: 0.1631  decode.loss_mask: 0.2513  decode.loss_dice: 0.2777  decode.d0.loss_cls: 0.9314  decode.d0.loss_mask: 0.2589  decode.d0.loss_dice: 0.2977  decode.d1.loss_cls: 0.2141  decode.d1.loss_mask: 0.2531  decode.d1.loss_dice: 0.2820  decode.d2.loss_cls: 0.2148  decode.d2.loss_mask: 0.2540  decode.d2.loss_dice: 0.2848  decode.d3.loss_cls: 0.2004  decode.d3.loss_mask: 0.2513  decode.d3.loss_dice: 0.2732  decode.d4.loss_cls: 0.2108  decode.d4.loss_mask: 0.2543  decode.d4.loss_dice: 0.2729  decode.d5.loss_cls: 0.2171  decode.d5.loss_mask: 0.2523  decode.d5.loss_dice: 0.2756  decode.d6.loss_cls: 0.2011  decode.d6.loss_mask: 0.2511  decode.d6.loss_dice: 0.2792  decode.d7.loss_cls: 0.1980  decode.d7.loss_mask: 0.2540  decode.d7.loss_dice: 0.2866  decode.d8.loss_cls: 0.1784  decode.d8.loss_mask: 0.2562  decode.d8.loss_dice: 0.2922
08/06 04:31:50 - mmengine - INFO - Iter(train) [ 18550/320000]  base_lr: 9.4768e-05 lr: 9.4768e-06  eta: 1 day, 12:34:42  time: 0.4375  data_time: 0.0092  memory: 5240  grad_norm: 110.0285  loss: 9.7547  decode.loss_cls: 0.3248  decode.loss_mask: 0.2496  decode.loss_dice: 0.3063  decode.d0.loss_cls: 0.9238  decode.d0.loss_mask: 0.2392  decode.d0.loss_dice: 0.3674  decode.d1.loss_cls: 0.3879  decode.d1.loss_mask: 0.2289  decode.d1.loss_dice: 0.3090  decode.d2.loss_cls: 0.2833  decode.d2.loss_mask: 0.2249  decode.d2.loss_dice: 0.3128  decode.d3.loss_cls: 0.3457  decode.d3.loss_mask: 0.2311  decode.d3.loss_dice: 0.3284  decode.d4.loss_cls: 0.3486  decode.d4.loss_mask: 0.2641  decode.d4.loss_dice: 0.3329  decode.d5.loss_cls: 0.3412  decode.d5.loss_mask: 0.2704  decode.d5.loss_dice: 0.3652  decode.d6.loss_cls: 0.3038  decode.d6.loss_mask: 0.2558  decode.d6.loss_dice: 0.3207  decode.d7.loss_cls: 0.3314  decode.d7.loss_mask: 0.2598  decode.d7.loss_dice: 0.3429  decode.d8.loss_cls: 0.3617  decode.d8.loss_mask: 0.2395  decode.d8.loss_dice: 0.3537
08/06 04:32:11 - mmengine - INFO - Iter(train) [ 18600/320000]  base_lr: 9.4753e-05 lr: 9.4753e-06  eta: 1 day, 12:34:20  time: 0.4365  data_time: 0.0091  memory: 5258  grad_norm: 97.7482  loss: 7.3717  decode.loss_cls: 0.1452  decode.loss_mask: 0.2217  decode.loss_dice: 0.2562  decode.d0.loss_cls: 0.9255  decode.d0.loss_mask: 0.2230  decode.d0.loss_dice: 0.2789  decode.d1.loss_cls: 0.2205  decode.d1.loss_mask: 0.2213  decode.d1.loss_dice: 0.2665  decode.d2.loss_cls: 0.1318  decode.d2.loss_mask: 0.2225  decode.d2.loss_dice: 0.2837  decode.d3.loss_cls: 0.1410  decode.d3.loss_mask: 0.2308  decode.d3.loss_dice: 0.2784  decode.d4.loss_cls: 0.1942  decode.d4.loss_mask: 0.2184  decode.d4.loss_dice: 0.2448  decode.d5.loss_cls: 0.1711  decode.d5.loss_mask: 0.2390  decode.d5.loss_dice: 0.2780  decode.d6.loss_cls: 0.2077  decode.d6.loss_mask: 0.2196  decode.d6.loss_dice: 0.2468  decode.d7.loss_cls: 0.1714  decode.d7.loss_mask: 0.2211  decode.d7.loss_dice: 0.2577  decode.d8.loss_cls: 0.1810  decode.d8.loss_mask: 0.2197  decode.d8.loss_dice: 0.2544
08/06 04:32:33 - mmengine - INFO - Iter(train) [ 18650/320000]  base_lr: 9.4739e-05 lr: 9.4739e-06  eta: 1 day, 12:33:59  time: 0.4371  data_time: 0.0090  memory: 5242  grad_norm: 117.4652  loss: 8.2178  decode.loss_cls: 0.1944  decode.loss_mask: 0.2464  decode.loss_dice: 0.2860  decode.d0.loss_cls: 0.9496  decode.d0.loss_mask: 0.2414  decode.d0.loss_dice: 0.3043  decode.d1.loss_cls: 0.2014  decode.d1.loss_mask: 0.2391  decode.d1.loss_dice: 0.2911  decode.d2.loss_cls: 0.2404  decode.d2.loss_mask: 0.2424  decode.d2.loss_dice: 0.3091  decode.d3.loss_cls: 0.2269  decode.d3.loss_mask: 0.2423  decode.d3.loss_dice: 0.2894  decode.d4.loss_cls: 0.2426  decode.d4.loss_mask: 0.2475  decode.d4.loss_dice: 0.2957  decode.d5.loss_cls: 0.2427  decode.d5.loss_mask: 0.2414  decode.d5.loss_dice: 0.2849  decode.d6.loss_cls: 0.1637  decode.d6.loss_mask: 0.2513  decode.d6.loss_dice: 0.2918  decode.d7.loss_cls: 0.1832  decode.d7.loss_mask: 0.2405  decode.d7.loss_dice: 0.2841  decode.d8.loss_cls: 0.2197  decode.d8.loss_mask: 0.2409  decode.d8.loss_dice: 0.2835
08/06 04:32:55 - mmengine - INFO - Iter(train) [ 18700/320000]  base_lr: 9.4725e-05 lr: 9.4725e-06  eta: 1 day, 12:33:37  time: 0.4379  data_time: 0.0092  memory: 5275  grad_norm: 133.8227  loss: 9.8393  decode.loss_cls: 0.3417  decode.loss_mask: 0.2287  decode.loss_dice: 0.3111  decode.d0.loss_cls: 1.1474  decode.d0.loss_mask: 0.2371  decode.d0.loss_dice: 0.3111  decode.d1.loss_cls: 0.4529  decode.d1.loss_mask: 0.2209  decode.d1.loss_dice: 0.3172  decode.d2.loss_cls: 0.3991  decode.d2.loss_mask: 0.2226  decode.d2.loss_dice: 0.3145  decode.d3.loss_cls: 0.3676  decode.d3.loss_mask: 0.2241  decode.d3.loss_dice: 0.3140  decode.d4.loss_cls: 0.3548  decode.d4.loss_mask: 0.2250  decode.d4.loss_dice: 0.2693  decode.d5.loss_cls: 0.3468  decode.d5.loss_mask: 0.2406  decode.d5.loss_dice: 0.3515  decode.d6.loss_cls: 0.3097  decode.d6.loss_mask: 0.2254  decode.d6.loss_dice: 0.3005  decode.d7.loss_cls: 0.3054  decode.d7.loss_mask: 0.2357  decode.d7.loss_dice: 0.3447  decode.d8.loss_cls: 0.3215  decode.d8.loss_mask: 0.2510  decode.d8.loss_dice: 0.3476
08/06 04:33:17 - mmengine - INFO - Iter(train) [ 18750/320000]  base_lr: 9.4711e-05 lr: 9.4711e-06  eta: 1 day, 12:33:16  time: 0.4373  data_time: 0.0093  memory: 5242  grad_norm: 64.4212  loss: 9.0750  decode.loss_cls: 0.1621  decode.loss_mask: 0.3396  decode.loss_dice: 0.3457  decode.d0.loss_cls: 1.0848  decode.d0.loss_mask: 0.3476  decode.d0.loss_dice: 0.3426  decode.d1.loss_cls: 0.1575  decode.d1.loss_mask: 0.3358  decode.d1.loss_dice: 0.3167  decode.d2.loss_cls: 0.1587  decode.d2.loss_mask: 0.3379  decode.d2.loss_dice: 0.3223  decode.d3.loss_cls: 0.1680  decode.d3.loss_mask: 0.3388  decode.d3.loss_dice: 0.3003  decode.d4.loss_cls: 0.1522  decode.d4.loss_mask: 0.3317  decode.d4.loss_dice: 0.3193  decode.d5.loss_cls: 0.1445  decode.d5.loss_mask: 0.3386  decode.d5.loss_dice: 0.3157  decode.d6.loss_cls: 0.1730  decode.d6.loss_mask: 0.3335  decode.d6.loss_dice: 0.3141  decode.d7.loss_cls: 0.1488  decode.d7.loss_mask: 0.3329  decode.d7.loss_dice: 0.2898  decode.d8.loss_cls: 0.1784  decode.d8.loss_mask: 0.3425  decode.d8.loss_dice: 0.3015
08/06 04:33:39 - mmengine - INFO - Iter(train) [ 18800/320000]  base_lr: 9.4697e-05 lr: 9.4697e-06  eta: 1 day, 12:32:54  time: 0.4369  data_time: 0.0093  memory: 5260  grad_norm: 100.3684  loss: 9.3565  decode.loss_cls: 0.1765  decode.loss_mask: 0.2929  decode.loss_dice: 0.3464  decode.d0.loss_cls: 0.8396  decode.d0.loss_mask: 0.3024  decode.d0.loss_dice: 0.3615  decode.d1.loss_cls: 0.3493  decode.d1.loss_mask: 0.2896  decode.d1.loss_dice: 0.3189  decode.d2.loss_cls: 0.2825  decode.d2.loss_mask: 0.2918  decode.d2.loss_dice: 0.3480  decode.d3.loss_cls: 0.2395  decode.d3.loss_mask: 0.2920  decode.d3.loss_dice: 0.3244  decode.d4.loss_cls: 0.2499  decode.d4.loss_mask: 0.2845  decode.d4.loss_dice: 0.3345  decode.d5.loss_cls: 0.2779  decode.d5.loss_mask: 0.2869  decode.d5.loss_dice: 0.3135  decode.d6.loss_cls: 0.2299  decode.d6.loss_mask: 0.2906  decode.d6.loss_dice: 0.3223  decode.d7.loss_cls: 0.2354  decode.d7.loss_mask: 0.2945  decode.d7.loss_dice: 0.3309  decode.d8.loss_cls: 0.2182  decode.d8.loss_mask: 0.2924  decode.d8.loss_dice: 0.3399
08/06 04:34:01 - mmengine - INFO - Iter(train) [ 18850/320000]  base_lr: 9.4683e-05 lr: 9.4683e-06  eta: 1 day, 12:32:33  time: 0.4379  data_time: 0.0093  memory: 5242  grad_norm: 73.7382  loss: 7.4889  decode.loss_cls: 0.1632  decode.loss_mask: 0.2150  decode.loss_dice: 0.2768  decode.d0.loss_cls: 0.9533  decode.d0.loss_mask: 0.2286  decode.d0.loss_dice: 0.3054  decode.d1.loss_cls: 0.1931  decode.d1.loss_mask: 0.2189  decode.d1.loss_dice: 0.2780  decode.d2.loss_cls: 0.2011  decode.d2.loss_mask: 0.2164  decode.d2.loss_dice: 0.2641  decode.d3.loss_cls: 0.1750  decode.d3.loss_mask: 0.2148  decode.d3.loss_dice: 0.2657  decode.d4.loss_cls: 0.2169  decode.d4.loss_mask: 0.2186  decode.d4.loss_dice: 0.2699  decode.d5.loss_cls: 0.1792  decode.d5.loss_mask: 0.2151  decode.d5.loss_dice: 0.2659  decode.d6.loss_cls: 0.1585  decode.d6.loss_mask: 0.2180  decode.d6.loss_dice: 0.2662  decode.d7.loss_cls: 0.1909  decode.d7.loss_mask: 0.2189  decode.d7.loss_dice: 0.2698  decode.d8.loss_cls: 0.1307  decode.d8.loss_mask: 0.2172  decode.d8.loss_dice: 0.2838
08/06 04:34:23 - mmengine - INFO - Iter(train) [ 18900/320000]  base_lr: 9.4669e-05 lr: 9.4669e-06  eta: 1 day, 12:32:12  time: 0.4376  data_time: 0.0092  memory: 5299  grad_norm: 160.1982  loss: 11.5918  decode.loss_cls: 0.5144  decode.loss_mask: 0.2326  decode.loss_dice: 0.3356  decode.d0.loss_cls: 1.0836  decode.d0.loss_mask: 0.2557  decode.d0.loss_dice: 0.3850  decode.d1.loss_cls: 0.5924  decode.d1.loss_mask: 0.2323  decode.d1.loss_dice: 0.3301  decode.d2.loss_cls: 0.5126  decode.d2.loss_mask: 0.2290  decode.d2.loss_dice: 0.3142  decode.d3.loss_cls: 0.5500  decode.d3.loss_mask: 0.2343  decode.d3.loss_dice: 0.3251  decode.d4.loss_cls: 0.5209  decode.d4.loss_mask: 0.2340  decode.d4.loss_dice: 0.3150  decode.d5.loss_cls: 0.4940  decode.d5.loss_mask: 0.2353  decode.d5.loss_dice: 0.3417  decode.d6.loss_cls: 0.5475  decode.d6.loss_mask: 0.2349  decode.d6.loss_dice: 0.3494  decode.d7.loss_cls: 0.5298  decode.d7.loss_mask: 0.2326  decode.d7.loss_dice: 0.3378  decode.d8.loss_cls: 0.5451  decode.d8.loss_mask: 0.2310  decode.d8.loss_dice: 0.3157
08/06 04:34:44 - mmengine - INFO - Iter(train) [ 18950/320000]  base_lr: 9.4654e-05 lr: 9.4654e-06  eta: 1 day, 12:31:51  time: 0.4383  data_time: 0.0093  memory: 5260  grad_norm: 98.0310  loss: 8.5617  decode.loss_cls: 0.2753  decode.loss_mask: 0.2251  decode.loss_dice: 0.2836  decode.d0.loss_cls: 0.9037  decode.d0.loss_mask: 0.2486  decode.d0.loss_dice: 0.3111  decode.d1.loss_cls: 0.2578  decode.d1.loss_mask: 0.2336  decode.d1.loss_dice: 0.2564  decode.d2.loss_cls: 0.3157  decode.d2.loss_mask: 0.2361  decode.d2.loss_dice: 0.2824  decode.d3.loss_cls: 0.2799  decode.d3.loss_mask: 0.2326  decode.d3.loss_dice: 0.2617  decode.d4.loss_cls: 0.2734  decode.d4.loss_mask: 0.2340  decode.d4.loss_dice: 0.2721  decode.d5.loss_cls: 0.2569  decode.d5.loss_mask: 0.2346  decode.d5.loss_dice: 0.3099  decode.d6.loss_cls: 0.2838  decode.d6.loss_mask: 0.2301  decode.d6.loss_dice: 0.2862  decode.d7.loss_cls: 0.2646  decode.d7.loss_mask: 0.2255  decode.d7.loss_dice: 0.3091  decode.d8.loss_cls: 0.2515  decode.d8.loss_mask: 0.2226  decode.d8.loss_dice: 0.3040
08/06 04:35:06 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 04:35:06 - mmengine - INFO - Iter(train) [ 19000/320000]  base_lr: 9.4640e-05 lr: 9.4640e-06  eta: 1 day, 12:31:29  time: 0.4376  data_time: 0.0093  memory: 5260  grad_norm: 62.3056  loss: 6.2853  decode.loss_cls: 0.0735  decode.loss_mask: 0.2318  decode.loss_dice: 0.2473  decode.d0.loss_cls: 0.7837  decode.d0.loss_mask: 0.2345  decode.d0.loss_dice: 0.2523  decode.d1.loss_cls: 0.0851  decode.d1.loss_mask: 0.2316  decode.d1.loss_dice: 0.2434  decode.d2.loss_cls: 0.0774  decode.d2.loss_mask: 0.2324  decode.d2.loss_dice: 0.2449  decode.d3.loss_cls: 0.0569  decode.d3.loss_mask: 0.2338  decode.d3.loss_dice: 0.2522  decode.d4.loss_cls: 0.0750  decode.d4.loss_mask: 0.2299  decode.d4.loss_dice: 0.2547  decode.d5.loss_cls: 0.0956  decode.d5.loss_mask: 0.2298  decode.d5.loss_dice: 0.2482  decode.d6.loss_cls: 0.0724  decode.d6.loss_mask: 0.2308  decode.d6.loss_dice: 0.2523  decode.d7.loss_cls: 0.0768  decode.d7.loss_mask: 0.2277  decode.d7.loss_dice: 0.2437  decode.d8.loss_cls: 0.0947  decode.d8.loss_mask: 0.2337  decode.d8.loss_dice: 0.2393
08/06 04:35:28 - mmengine - INFO - Iter(train) [ 19050/320000]  base_lr: 9.4626e-05 lr: 9.4626e-06  eta: 1 day, 12:31:09  time: 0.4378  data_time: 0.0092  memory: 5260  grad_norm: 69.2039  loss: 7.1028  decode.loss_cls: 0.1993  decode.loss_mask: 0.2103  decode.loss_dice: 0.2197  decode.d0.loss_cls: 0.9418  decode.d0.loss_mask: 0.2133  decode.d0.loss_dice: 0.2134  decode.d1.loss_cls: 0.2719  decode.d1.loss_mask: 0.2074  decode.d1.loss_dice: 0.2110  decode.d2.loss_cls: 0.2661  decode.d2.loss_mask: 0.2060  decode.d2.loss_dice: 0.2080  decode.d3.loss_cls: 0.1766  decode.d3.loss_mask: 0.2206  decode.d3.loss_dice: 0.1993  decode.d4.loss_cls: 0.2015  decode.d4.loss_mask: 0.2109  decode.d4.loss_dice: 0.2177  decode.d5.loss_cls: 0.1963  decode.d5.loss_mask: 0.2142  decode.d5.loss_dice: 0.2210  decode.d6.loss_cls: 0.1890  decode.d6.loss_mask: 0.2148  decode.d6.loss_dice: 0.2293  decode.d7.loss_cls: 0.2181  decode.d7.loss_mask: 0.2111  decode.d7.loss_dice: 0.2207  decode.d8.loss_cls: 0.1622  decode.d8.loss_mask: 0.2232  decode.d8.loss_dice: 0.2081
08/06 04:35:50 - mmengine - INFO - Iter(train) [ 19100/320000]  base_lr: 9.4612e-05 lr: 9.4612e-06  eta: 1 day, 12:30:47  time: 0.4379  data_time: 0.0092  memory: 5260  grad_norm: 148.4240  loss: 8.6560  decode.loss_cls: 0.1500  decode.loss_mask: 0.2868  decode.loss_dice: 0.3015  decode.d0.loss_cls: 1.0423  decode.d0.loss_mask: 0.3061  decode.d0.loss_dice: 0.3130  decode.d1.loss_cls: 0.1988  decode.d1.loss_mask: 0.2999  decode.d1.loss_dice: 0.3096  decode.d2.loss_cls: 0.2064  decode.d2.loss_mask: 0.2914  decode.d2.loss_dice: 0.2868  decode.d3.loss_cls: 0.1799  decode.d3.loss_mask: 0.2937  decode.d3.loss_dice: 0.3129  decode.d4.loss_cls: 0.1848  decode.d4.loss_mask: 0.2918  decode.d4.loss_dice: 0.3007  decode.d5.loss_cls: 0.1819  decode.d5.loss_mask: 0.2820  decode.d5.loss_dice: 0.3022  decode.d6.loss_cls: 0.1926  decode.d6.loss_mask: 0.2859  decode.d6.loss_dice: 0.3132  decode.d7.loss_cls: 0.1890  decode.d7.loss_mask: 0.2891  decode.d7.loss_dice: 0.3119  decode.d8.loss_cls: 0.1725  decode.d8.loss_mask: 0.2851  decode.d8.loss_dice: 0.2944
08/06 04:36:12 - mmengine - INFO - Iter(train) [ 19150/320000]  base_lr: 9.4598e-05 lr: 9.4598e-06  eta: 1 day, 12:30:26  time: 0.4368  data_time: 0.0091  memory: 5242  grad_norm: 66.9007  loss: 8.0138  decode.loss_cls: 0.0875  decode.loss_mask: 0.3157  decode.loss_dice: 0.2891  decode.d0.loss_cls: 0.8356  decode.d0.loss_mask: 0.3238  decode.d0.loss_dice: 0.2928  decode.d1.loss_cls: 0.1532  decode.d1.loss_mask: 0.3161  decode.d1.loss_dice: 0.2764  decode.d2.loss_cls: 0.1551  decode.d2.loss_mask: 0.3185  decode.d2.loss_dice: 0.2838  decode.d3.loss_cls: 0.1778  decode.d3.loss_mask: 0.3089  decode.d3.loss_dice: 0.2922  decode.d4.loss_cls: 0.1527  decode.d4.loss_mask: 0.3077  decode.d4.loss_dice: 0.2797  decode.d5.loss_cls: 0.1304  decode.d5.loss_mask: 0.3105  decode.d5.loss_dice: 0.2817  decode.d6.loss_cls: 0.1624  decode.d6.loss_mask: 0.3055  decode.d6.loss_dice: 0.2894  decode.d7.loss_cls: 0.0967  decode.d7.loss_mask: 0.3101  decode.d7.loss_dice: 0.2796  decode.d8.loss_cls: 0.0875  decode.d8.loss_mask: 0.3088  decode.d8.loss_dice: 0.2849
08/06 04:36:34 - mmengine - INFO - Iter(train) [ 19200/320000]  base_lr: 9.4584e-05 lr: 9.4584e-06  eta: 1 day, 12:30:05  time: 0.4383  data_time: 0.0093  memory: 5242  grad_norm: 120.0204  loss: 9.0103  decode.loss_cls: 0.1345  decode.loss_mask: 0.2622  decode.loss_dice: 0.3828  decode.d0.loss_cls: 0.9925  decode.d0.loss_mask: 0.2890  decode.d0.loss_dice: 0.4333  decode.d1.loss_cls: 0.2280  decode.d1.loss_mask: 0.2764  decode.d1.loss_dice: 0.3714  decode.d2.loss_cls: 0.1633  decode.d2.loss_mask: 0.2729  decode.d2.loss_dice: 0.3957  decode.d3.loss_cls: 0.1626  decode.d3.loss_mask: 0.2637  decode.d3.loss_dice: 0.3858  decode.d4.loss_cls: 0.1460  decode.d4.loss_mask: 0.2642  decode.d4.loss_dice: 0.4012  decode.d5.loss_cls: 0.1493  decode.d5.loss_mask: 0.2591  decode.d5.loss_dice: 0.3906  decode.d6.loss_cls: 0.1368  decode.d6.loss_mask: 0.2635  decode.d6.loss_dice: 0.4101  decode.d7.loss_cls: 0.1441  decode.d7.loss_mask: 0.2587  decode.d7.loss_dice: 0.3681  decode.d8.loss_cls: 0.1489  decode.d8.loss_mask: 0.2649  decode.d8.loss_dice: 0.3907
08/06 04:36:56 - mmengine - INFO - Iter(train) [ 19250/320000]  base_lr: 9.4570e-05 lr: 9.4570e-06  eta: 1 day, 12:29:47  time: 0.4371  data_time: 0.0090  memory: 5242  grad_norm: 44.6808  loss: 6.8511  decode.loss_cls: 0.0670  decode.loss_mask: 0.2403  decode.loss_dice: 0.2704  decode.d0.loss_cls: 0.8933  decode.d0.loss_mask: 0.2435  decode.d0.loss_dice: 0.2683  decode.d1.loss_cls: 0.1678  decode.d1.loss_mask: 0.2398  decode.d1.loss_dice: 0.2603  decode.d2.loss_cls: 0.1365  decode.d2.loss_mask: 0.2408  decode.d2.loss_dice: 0.2574  decode.d3.loss_cls: 0.1177  decode.d3.loss_mask: 0.2442  decode.d3.loss_dice: 0.2509  decode.d4.loss_cls: 0.1256  decode.d4.loss_mask: 0.2435  decode.d4.loss_dice: 0.2396  decode.d5.loss_cls: 0.0954  decode.d5.loss_mask: 0.2404  decode.d5.loss_dice: 0.2542  decode.d6.loss_cls: 0.0844  decode.d6.loss_mask: 0.2359  decode.d6.loss_dice: 0.2577  decode.d7.loss_cls: 0.0697  decode.d7.loss_mask: 0.2398  decode.d7.loss_dice: 0.2692  decode.d8.loss_cls: 0.0888  decode.d8.loss_mask: 0.2403  decode.d8.loss_dice: 0.2683
08/06 04:37:18 - mmengine - INFO - Iter(train) [ 19300/320000]  base_lr: 9.4555e-05 lr: 9.4555e-06  eta: 1 day, 12:29:26  time: 0.4383  data_time: 0.0092  memory: 5275  grad_norm: 96.6766  loss: 7.5724  decode.loss_cls: 0.0712  decode.loss_mask: 0.3141  decode.loss_dice: 0.2713  decode.d0.loss_cls: 0.8540  decode.d0.loss_mask: 0.3307  decode.d0.loss_dice: 0.2827  decode.d1.loss_cls: 0.1121  decode.d1.loss_mask: 0.3118  decode.d1.loss_dice: 0.2741  decode.d2.loss_cls: 0.0870  decode.d2.loss_mask: 0.3140  decode.d2.loss_dice: 0.2675  decode.d3.loss_cls: 0.1576  decode.d3.loss_mask: 0.3150  decode.d3.loss_dice: 0.2690  decode.d4.loss_cls: 0.0832  decode.d4.loss_mask: 0.3144  decode.d4.loss_dice: 0.2705  decode.d5.loss_cls: 0.1192  decode.d5.loss_mask: 0.3119  decode.d5.loss_dice: 0.2669  decode.d6.loss_cls: 0.0773  decode.d6.loss_mask: 0.3119  decode.d6.loss_dice: 0.2650  decode.d7.loss_cls: 0.0882  decode.d7.loss_mask: 0.3121  decode.d7.loss_dice: 0.2650  decode.d8.loss_cls: 0.0682  decode.d8.loss_mask: 0.3156  decode.d8.loss_dice: 0.2707
08/06 04:37:40 - mmengine - INFO - Iter(train) [ 19350/320000]  base_lr: 9.4541e-05 lr: 9.4541e-06  eta: 1 day, 12:29:06  time: 0.4393  data_time: 0.0093  memory: 5240  grad_norm: 112.9164  loss: 9.8834  decode.loss_cls: 0.2645  decode.loss_mask: 0.3152  decode.loss_dice: 0.3161  decode.d0.loss_cls: 1.0364  decode.d0.loss_mask: 0.3105  decode.d0.loss_dice: 0.3543  decode.d1.loss_cls: 0.3823  decode.d1.loss_mask: 0.3074  decode.d1.loss_dice: 0.3168  decode.d2.loss_cls: 0.2864  decode.d2.loss_mask: 0.3035  decode.d2.loss_dice: 0.3032  decode.d3.loss_cls: 0.2546  decode.d3.loss_mask: 0.3059  decode.d3.loss_dice: 0.3135  decode.d4.loss_cls: 0.2653  decode.d4.loss_mask: 0.3082  decode.d4.loss_dice: 0.3120  decode.d5.loss_cls: 0.3417  decode.d5.loss_mask: 0.3146  decode.d5.loss_dice: 0.3169  decode.d6.loss_cls: 0.2711  decode.d6.loss_mask: 0.3159  decode.d6.loss_dice: 0.3302  decode.d7.loss_cls: 0.2682  decode.d7.loss_mask: 0.3076  decode.d7.loss_dice: 0.3204  decode.d8.loss_cls: 0.2327  decode.d8.loss_mask: 0.3087  decode.d8.loss_dice: 0.2992
08/06 04:38:02 - mmengine - INFO - Iter(train) [ 19400/320000]  base_lr: 9.4527e-05 lr: 9.4527e-06  eta: 1 day, 12:28:45  time: 0.4390  data_time: 0.0093  memory: 5299  grad_norm: 59.8535  loss: 8.3946  decode.loss_cls: 0.3041  decode.loss_mask: 0.2225  decode.loss_dice: 0.2856  decode.d0.loss_cls: 0.9531  decode.d0.loss_mask: 0.2300  decode.d0.loss_dice: 0.2966  decode.d1.loss_cls: 0.3253  decode.d1.loss_mask: 0.2305  decode.d1.loss_dice: 0.3059  decode.d2.loss_cls: 0.2477  decode.d2.loss_mask: 0.2246  decode.d2.loss_dice: 0.2923  decode.d3.loss_cls: 0.2093  decode.d3.loss_mask: 0.2316  decode.d3.loss_dice: 0.3106  decode.d4.loss_cls: 0.2758  decode.d4.loss_mask: 0.2232  decode.d4.loss_dice: 0.2910  decode.d5.loss_cls: 0.2023  decode.d5.loss_mask: 0.2221  decode.d5.loss_dice: 0.2910  decode.d6.loss_cls: 0.2204  decode.d6.loss_mask: 0.2240  decode.d6.loss_dice: 0.2830  decode.d7.loss_cls: 0.2232  decode.d7.loss_mask: 0.2242  decode.d7.loss_dice: 0.3036  decode.d8.loss_cls: 0.2372  decode.d8.loss_mask: 0.2217  decode.d8.loss_dice: 0.2823
08/06 04:38:24 - mmengine - INFO - Iter(train) [ 19450/320000]  base_lr: 9.4513e-05 lr: 9.4513e-06  eta: 1 day, 12:28:25  time: 0.4397  data_time: 0.0093  memory: 5260  grad_norm: 135.6852  loss: 8.3789  decode.loss_cls: 0.1709  decode.loss_mask: 0.2280  decode.loss_dice: 0.3352  decode.d0.loss_cls: 0.8778  decode.d0.loss_mask: 0.2395  decode.d0.loss_dice: 0.3722  decode.d1.loss_cls: 0.2435  decode.d1.loss_mask: 0.2434  decode.d1.loss_dice: 0.3287  decode.d2.loss_cls: 0.2051  decode.d2.loss_mask: 0.2355  decode.d2.loss_dice: 0.3512  decode.d3.loss_cls: 0.2305  decode.d3.loss_mask: 0.2370  decode.d3.loss_dice: 0.3173  decode.d4.loss_cls: 0.2019  decode.d4.loss_mask: 0.2351  decode.d4.loss_dice: 0.3228  decode.d5.loss_cls: 0.2101  decode.d5.loss_mask: 0.2321  decode.d5.loss_dice: 0.3144  decode.d6.loss_cls: 0.1774  decode.d6.loss_mask: 0.2612  decode.d6.loss_dice: 0.3215  decode.d7.loss_cls: 0.1955  decode.d7.loss_mask: 0.2333  decode.d7.loss_dice: 0.3109  decode.d8.loss_cls: 0.1856  decode.d8.loss_mask: 0.2445  decode.d8.loss_dice: 0.3171
08/06 04:38:46 - mmengine - INFO - Iter(train) [ 19500/320000]  base_lr: 9.4499e-05 lr: 9.4499e-06  eta: 1 day, 12:28:04  time: 0.4394  data_time: 0.0093  memory: 5224  grad_norm: 85.9429  loss: 8.2399  decode.loss_cls: 0.1974  decode.loss_mask: 0.2195  decode.loss_dice: 0.2824  decode.d0.loss_cls: 1.0217  decode.d0.loss_mask: 0.2244  decode.d0.loss_dice: 0.3080  decode.d1.loss_cls: 0.2824  decode.d1.loss_mask: 0.2171  decode.d1.loss_dice: 0.2875  decode.d2.loss_cls: 0.2912  decode.d2.loss_mask: 0.2196  decode.d2.loss_dice: 0.2873  decode.d3.loss_cls: 0.2326  decode.d3.loss_mask: 0.2183  decode.d3.loss_dice: 0.2956  decode.d4.loss_cls: 0.2066  decode.d4.loss_mask: 0.2214  decode.d4.loss_dice: 0.2910  decode.d5.loss_cls: 0.1994  decode.d5.loss_mask: 0.2195  decode.d5.loss_dice: 0.2846  decode.d6.loss_cls: 0.2473  decode.d6.loss_mask: 0.2164  decode.d6.loss_dice: 0.2925  decode.d7.loss_cls: 0.2074  decode.d7.loss_mask: 0.2192  decode.d7.loss_dice: 0.2982  decode.d8.loss_cls: 0.2404  decode.d8.loss_mask: 0.2168  decode.d8.loss_dice: 0.2942
08/06 04:39:08 - mmengine - INFO - Iter(train) [ 19550/320000]  base_lr: 9.4485e-05 lr: 9.4485e-06  eta: 1 day, 12:27:44  time: 0.4382  data_time: 0.0092  memory: 5242  grad_norm: 47.7555  loss: 8.1477  decode.loss_cls: 0.2049  decode.loss_mask: 0.2415  decode.loss_dice: 0.2930  decode.d0.loss_cls: 0.9018  decode.d0.loss_mask: 0.2529  decode.d0.loss_dice: 0.3187  decode.d1.loss_cls: 0.2121  decode.d1.loss_mask: 0.2425  decode.d1.loss_dice: 0.3173  decode.d2.loss_cls: 0.1550  decode.d2.loss_mask: 0.2438  decode.d2.loss_dice: 0.2953  decode.d3.loss_cls: 0.2011  decode.d3.loss_mask: 0.2462  decode.d3.loss_dice: 0.2997  decode.d4.loss_cls: 0.2172  decode.d4.loss_mask: 0.2450  decode.d4.loss_dice: 0.3084  decode.d5.loss_cls: 0.1558  decode.d5.loss_mask: 0.2437  decode.d5.loss_dice: 0.3229  decode.d6.loss_cls: 0.2244  decode.d6.loss_mask: 0.2482  decode.d6.loss_dice: 0.3023  decode.d7.loss_cls: 0.1923  decode.d7.loss_mask: 0.2439  decode.d7.loss_dice: 0.3054  decode.d8.loss_cls: 0.1620  decode.d8.loss_mask: 0.2414  decode.d8.loss_dice: 0.3089
08/06 04:39:30 - mmengine - INFO - Iter(train) [ 19600/320000]  base_lr: 9.4470e-05 lr: 9.4470e-06  eta: 1 day, 12:27:23  time: 0.4385  data_time: 0.0094  memory: 5242  grad_norm: 76.0580  loss: 7.2600  decode.loss_cls: 0.1401  decode.loss_mask: 0.2452  decode.loss_dice: 0.2532  decode.d0.loss_cls: 0.8731  decode.d0.loss_mask: 0.2622  decode.d0.loss_dice: 0.2520  decode.d1.loss_cls: 0.1838  decode.d1.loss_mask: 0.2497  decode.d1.loss_dice: 0.2601  decode.d2.loss_cls: 0.0983  decode.d2.loss_mask: 0.2565  decode.d2.loss_dice: 0.2779  decode.d3.loss_cls: 0.1052  decode.d3.loss_mask: 0.2549  decode.d3.loss_dice: 0.2666  decode.d4.loss_cls: 0.1582  decode.d4.loss_mask: 0.2487  decode.d4.loss_dice: 0.2690  decode.d5.loss_cls: 0.1439  decode.d5.loss_mask: 0.2470  decode.d5.loss_dice: 0.2588  decode.d6.loss_cls: 0.1563  decode.d6.loss_mask: 0.2405  decode.d6.loss_dice: 0.2580  decode.d7.loss_cls: 0.1499  decode.d7.loss_mask: 0.2456  decode.d7.loss_dice: 0.2297  decode.d8.loss_cls: 0.1645  decode.d8.loss_mask: 0.2444  decode.d8.loss_dice: 0.2669
08/06 04:39:51 - mmengine - INFO - Iter(train) [ 19650/320000]  base_lr: 9.4456e-05 lr: 9.4456e-06  eta: 1 day, 12:27:03  time: 0.4385  data_time: 0.0092  memory: 5240  grad_norm: 90.6356  loss: 9.2454  decode.loss_cls: 0.2352  decode.loss_mask: 0.2526  decode.loss_dice: 0.3631  decode.d0.loss_cls: 0.9565  decode.d0.loss_mask: 0.2658  decode.d0.loss_dice: 0.3693  decode.d1.loss_cls: 0.2998  decode.d1.loss_mask: 0.2580  decode.d1.loss_dice: 0.3558  decode.d2.loss_cls: 0.2600  decode.d2.loss_mask: 0.2563  decode.d2.loss_dice: 0.3574  decode.d3.loss_cls: 0.2282  decode.d3.loss_mask: 0.2524  decode.d3.loss_dice: 0.3504  decode.d4.loss_cls: 0.1646  decode.d4.loss_mask: 0.2590  decode.d4.loss_dice: 0.3859  decode.d5.loss_cls: 0.2069  decode.d5.loss_mask: 0.2537  decode.d5.loss_dice: 0.3658  decode.d6.loss_cls: 0.1927  decode.d6.loss_mask: 0.2567  decode.d6.loss_dice: 0.3746  decode.d7.loss_cls: 0.2363  decode.d7.loss_mask: 0.2572  decode.d7.loss_dice: 0.3705  decode.d8.loss_cls: 0.2379  decode.d8.loss_mask: 0.2519  decode.d8.loss_dice: 0.3712
08/06 04:40:14 - mmengine - INFO - Iter(train) [ 19700/320000]  base_lr: 9.4442e-05 lr: 9.4442e-06  eta: 1 day, 12:26:44  time: 0.4402  data_time: 0.0093  memory: 5240  grad_norm: 156.8067  loss: 9.4031  decode.loss_cls: 0.2829  decode.loss_mask: 0.3302  decode.loss_dice: 0.3248  decode.d0.loss_cls: 0.9461  decode.d0.loss_mask: 0.3346  decode.d0.loss_dice: 0.3393  decode.d1.loss_cls: 0.2044  decode.d1.loss_mask: 0.3337  decode.d1.loss_dice: 0.3391  decode.d2.loss_cls: 0.2316  decode.d2.loss_mask: 0.3284  decode.d2.loss_dice: 0.3115  decode.d3.loss_cls: 0.1553  decode.d3.loss_mask: 0.3238  decode.d3.loss_dice: 0.3418  decode.d4.loss_cls: 0.2048  decode.d4.loss_mask: 0.3268  decode.d4.loss_dice: 0.3245  decode.d5.loss_cls: 0.1451  decode.d5.loss_mask: 0.3350  decode.d5.loss_dice: 0.3407  decode.d6.loss_cls: 0.1847  decode.d6.loss_mask: 0.3622  decode.d6.loss_dice: 0.3592  decode.d7.loss_cls: 0.1523  decode.d7.loss_mask: 0.3410  decode.d7.loss_dice: 0.3602  decode.d8.loss_cls: 0.1861  decode.d8.loss_mask: 0.3278  decode.d8.loss_dice: 0.3254
08/06 04:40:36 - mmengine - INFO - Iter(train) [ 19750/320000]  base_lr: 9.4428e-05 lr: 9.4428e-06  eta: 1 day, 12:26:24  time: 0.4401  data_time: 0.0093  memory: 5260  grad_norm: 55.9731  loss: 7.8648  decode.loss_cls: 0.1877  decode.loss_mask: 0.1877  decode.loss_dice: 0.3025  decode.d0.loss_cls: 1.0069  decode.d0.loss_mask: 0.1843  decode.d0.loss_dice: 0.2803  decode.d1.loss_cls: 0.2878  decode.d1.loss_mask: 0.1859  decode.d1.loss_dice: 0.2814  decode.d2.loss_cls: 0.3010  decode.d2.loss_mask: 0.1866  decode.d2.loss_dice: 0.2764  decode.d3.loss_cls: 0.2215  decode.d3.loss_mask: 0.1876  decode.d3.loss_dice: 0.2745  decode.d4.loss_cls: 0.2315  decode.d4.loss_mask: 0.1879  decode.d4.loss_dice: 0.2851  decode.d5.loss_cls: 0.2291  decode.d5.loss_mask: 0.1875  decode.d5.loss_dice: 0.2846  decode.d6.loss_cls: 0.1710  decode.d6.loss_mask: 0.1901  decode.d6.loss_dice: 0.3177  decode.d7.loss_cls: 0.2393  decode.d7.loss_mask: 0.1893  decode.d7.loss_dice: 0.2875  decode.d8.loss_cls: 0.2436  decode.d8.loss_mask: 0.1894  decode.d8.loss_dice: 0.2790
08/06 04:40:58 - mmengine - INFO - Iter(train) [ 19800/320000]  base_lr: 9.4414e-05 lr: 9.4414e-06  eta: 1 day, 12:26:05  time: 0.4403  data_time: 0.0093  memory: 5260  grad_norm: 67.6626  loss: 10.0242  decode.loss_cls: 0.0923  decode.loss_mask: 0.4248  decode.loss_dice: 0.3724  decode.d0.loss_cls: 0.8468  decode.d0.loss_mask: 0.4507  decode.d0.loss_dice: 0.3973  decode.d1.loss_cls: 0.1956  decode.d1.loss_mask: 0.4122  decode.d1.loss_dice: 0.3477  decode.d2.loss_cls: 0.1784  decode.d2.loss_mask: 0.4239  decode.d2.loss_dice: 0.3637  decode.d3.loss_cls: 0.1513  decode.d3.loss_mask: 0.4284  decode.d3.loss_dice: 0.3371  decode.d4.loss_cls: 0.0835  decode.d4.loss_mask: 0.4392  decode.d4.loss_dice: 0.3730  decode.d5.loss_cls: 0.1458  decode.d5.loss_mask: 0.4198  decode.d5.loss_dice: 0.3609  decode.d6.loss_cls: 0.1661  decode.d6.loss_mask: 0.4142  decode.d6.loss_dice: 0.3681  decode.d7.loss_cls: 0.1452  decode.d7.loss_mask: 0.4167  decode.d7.loss_dice: 0.3441  decode.d8.loss_cls: 0.1259  decode.d8.loss_mask: 0.4177  decode.d8.loss_dice: 0.3815
08/06 04:41:20 - mmengine - INFO - Iter(train) [ 19850/320000]  base_lr: 9.4400e-05 lr: 9.4400e-06  eta: 1 day, 12:25:46  time: 0.4409  data_time: 0.0094  memory: 5260  grad_norm: 123.9810  loss: 10.3427  decode.loss_cls: 0.2563  decode.loss_mask: 0.3275  decode.loss_dice: 0.3049  decode.d0.loss_cls: 0.9868  decode.d0.loss_mask: 0.3333  decode.d0.loss_dice: 0.3541  decode.d1.loss_cls: 0.4050  decode.d1.loss_mask: 0.3129  decode.d1.loss_dice: 0.3151  decode.d2.loss_cls: 0.3035  decode.d2.loss_mask: 0.3137  decode.d2.loss_dice: 0.3264  decode.d3.loss_cls: 0.3163  decode.d3.loss_mask: 0.3135  decode.d3.loss_dice: 0.3149  decode.d4.loss_cls: 0.3680  decode.d4.loss_mask: 0.3120  decode.d4.loss_dice: 0.3170  decode.d5.loss_cls: 0.3234  decode.d5.loss_mask: 0.3131  decode.d5.loss_dice: 0.3236  decode.d6.loss_cls: 0.3457  decode.d6.loss_mask: 0.3094  decode.d6.loss_dice: 0.3248  decode.d7.loss_cls: 0.3214  decode.d7.loss_mask: 0.3170  decode.d7.loss_dice: 0.3166  decode.d8.loss_cls: 0.3327  decode.d8.loss_mask: 0.3221  decode.d8.loss_dice: 0.3118
08/06 04:41:42 - mmengine - INFO - Iter(train) [ 19900/320000]  base_lr: 9.4386e-05 lr: 9.4386e-06  eta: 1 day, 12:25:26  time: 0.4399  data_time: 0.0092  memory: 5224  grad_norm: 60.4655  loss: 6.9062  decode.loss_cls: 0.2180  decode.loss_mask: 0.1960  decode.loss_dice: 0.2050  decode.d0.loss_cls: 0.9440  decode.d0.loss_mask: 0.2015  decode.d0.loss_dice: 0.2295  decode.d1.loss_cls: 0.2283  decode.d1.loss_mask: 0.1941  decode.d1.loss_dice: 0.2290  decode.d2.loss_cls: 0.1816  decode.d2.loss_mask: 0.1954  decode.d2.loss_dice: 0.2306  decode.d3.loss_cls: 0.1919  decode.d3.loss_mask: 0.1928  decode.d3.loss_dice: 0.2085  decode.d4.loss_cls: 0.1727  decode.d4.loss_mask: 0.1963  decode.d4.loss_dice: 0.2150  decode.d5.loss_cls: 0.1759  decode.d5.loss_mask: 0.1961  decode.d5.loss_dice: 0.2311  decode.d6.loss_cls: 0.2060  decode.d6.loss_mask: 0.1980  decode.d6.loss_dice: 0.2298  decode.d7.loss_cls: 0.2084  decode.d7.loss_mask: 0.1950  decode.d7.loss_dice: 0.2113  decode.d8.loss_cls: 0.2052  decode.d8.loss_mask: 0.1959  decode.d8.loss_dice: 0.2233
08/06 04:42:04 - mmengine - INFO - Iter(train) [ 19950/320000]  base_lr: 9.4371e-05 lr: 9.4371e-06  eta: 1 day, 12:25:07  time: 0.4393  data_time: 0.0090  memory: 5240  grad_norm: 71.8136  loss: 8.5865  decode.loss_cls: 0.0548  decode.loss_mask: 0.2785  decode.loss_dice: 0.3592  decode.d0.loss_cls: 0.9864  decode.d0.loss_mask: 0.2886  decode.d0.loss_dice: 0.4016  decode.d1.loss_cls: 0.1781  decode.d1.loss_mask: 0.2860  decode.d1.loss_dice: 0.3769  decode.d2.loss_cls: 0.1603  decode.d2.loss_mask: 0.2861  decode.d2.loss_dice: 0.3836  decode.d3.loss_cls: 0.1184  decode.d3.loss_mask: 0.2864  decode.d3.loss_dice: 0.3570  decode.d4.loss_cls: 0.1786  decode.d4.loss_mask: 0.2835  decode.d4.loss_dice: 0.3638  decode.d5.loss_cls: 0.0485  decode.d5.loss_mask: 0.2827  decode.d5.loss_dice: 0.3926  decode.d6.loss_cls: 0.0970  decode.d6.loss_mask: 0.2797  decode.d6.loss_dice: 0.3653  decode.d7.loss_cls: 0.1513  decode.d7.loss_mask: 0.2763  decode.d7.loss_dice: 0.3547  decode.d8.loss_cls: 0.0493  decode.d8.loss_mask: 0.2771  decode.d8.loss_dice: 0.3843
08/06 04:42:26 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 04:42:26 - mmengine - INFO - Iter(train) [ 20000/320000]  base_lr: 9.4357e-05 lr: 9.4357e-06  eta: 1 day, 12:24:47  time: 0.4397  data_time: 0.0091  memory: 5275  grad_norm: 47.2363  loss: 5.7867  decode.loss_cls: 0.0738  decode.loss_mask: 0.1913  decode.loss_dice: 0.2238  decode.d0.loss_cls: 0.8024  decode.d0.loss_mask: 0.2083  decode.d0.loss_dice: 0.2382  decode.d1.loss_cls: 0.1095  decode.d1.loss_mask: 0.1963  decode.d1.loss_dice: 0.2425  decode.d2.loss_cls: 0.0983  decode.d2.loss_mask: 0.1962  decode.d2.loss_dice: 0.2359  decode.d3.loss_cls: 0.0669  decode.d3.loss_mask: 0.1893  decode.d3.loss_dice: 0.2256  decode.d4.loss_cls: 0.0684  decode.d4.loss_mask: 0.1912  decode.d4.loss_dice: 0.2213  decode.d5.loss_cls: 0.0628  decode.d5.loss_mask: 0.1895  decode.d5.loss_dice: 0.2193  decode.d6.loss_cls: 0.0958  decode.d6.loss_mask: 0.1901  decode.d6.loss_dice: 0.2182  decode.d7.loss_cls: 0.0988  decode.d7.loss_mask: 0.1899  decode.d7.loss_dice: 0.2215  decode.d8.loss_cls: 0.1094  decode.d8.loss_mask: 0.1896  decode.d8.loss_dice: 0.2228
08/06 04:42:48 - mmengine - INFO - Iter(train) [ 20050/320000]  base_lr: 9.4343e-05 lr: 9.4343e-06  eta: 1 day, 12:24:27  time: 0.4397  data_time: 0.0091  memory: 5242  grad_norm: 88.9217  loss: 10.0551  decode.loss_cls: 0.1503  decode.loss_mask: 0.3134  decode.loss_dice: 0.4166  decode.d0.loss_cls: 1.0828  decode.d0.loss_mask: 0.3081  decode.d0.loss_dice: 0.4107  decode.d1.loss_cls: 0.1962  decode.d1.loss_mask: 0.3200  decode.d1.loss_dice: 0.4164  decode.d2.loss_cls: 0.1504  decode.d2.loss_mask: 0.3089  decode.d2.loss_dice: 0.3878  decode.d3.loss_cls: 0.2069  decode.d3.loss_mask: 0.3127  decode.d3.loss_dice: 0.3846  decode.d4.loss_cls: 0.2066  decode.d4.loss_mask: 0.3082  decode.d4.loss_dice: 0.4195  decode.d5.loss_cls: 0.2452  decode.d5.loss_mask: 0.3172  decode.d5.loss_dice: 0.4294  decode.d6.loss_cls: 0.1751  decode.d6.loss_mask: 0.3589  decode.d6.loss_dice: 0.4234  decode.d7.loss_cls: 0.1583  decode.d7.loss_mask: 0.3161  decode.d7.loss_dice: 0.4174  decode.d8.loss_cls: 0.1671  decode.d8.loss_mask: 0.3279  decode.d8.loss_dice: 0.4189
08/06 04:43:10 - mmengine - INFO - Iter(train) [ 20100/320000]  base_lr: 9.4329e-05 lr: 9.4329e-06  eta: 1 day, 12:24:07  time: 0.4403  data_time: 0.0090  memory: 5242  grad_norm: 45.9991  loss: 7.0317  decode.loss_cls: 0.1694  decode.loss_mask: 0.1976  decode.loss_dice: 0.2082  decode.d0.loss_cls: 0.9739  decode.d0.loss_mask: 0.2056  decode.d0.loss_dice: 0.2461  decode.d1.loss_cls: 0.2013  decode.d1.loss_mask: 0.2005  decode.d1.loss_dice: 0.2362  decode.d2.loss_cls: 0.1878  decode.d2.loss_mask: 0.1984  decode.d2.loss_dice: 0.2141  decode.d3.loss_cls: 0.2274  decode.d3.loss_mask: 0.1977  decode.d3.loss_dice: 0.2260  decode.d4.loss_cls: 0.2095  decode.d4.loss_mask: 0.1986  decode.d4.loss_dice: 0.2248  decode.d5.loss_cls: 0.2130  decode.d5.loss_mask: 0.1977  decode.d5.loss_dice: 0.2371  decode.d6.loss_cls: 0.2175  decode.d6.loss_mask: 0.1949  decode.d6.loss_dice: 0.2226  decode.d7.loss_cls: 0.2007  decode.d7.loss_mask: 0.1973  decode.d7.loss_dice: 0.2215  decode.d8.loss_cls: 0.1971  decode.d8.loss_mask: 0.1972  decode.d8.loss_dice: 0.2120
08/06 04:43:32 - mmengine - INFO - Iter(train) [ 20150/320000]  base_lr: 9.4315e-05 lr: 9.4315e-06  eta: 1 day, 12:23:47  time: 0.4388  data_time: 0.0091  memory: 5260  grad_norm: 54.9264  loss: 6.1200  decode.loss_cls: 0.0789  decode.loss_mask: 0.2140  decode.loss_dice: 0.2417  decode.d0.loss_cls: 0.8768  decode.d0.loss_mask: 0.2328  decode.d0.loss_dice: 0.2675  decode.d1.loss_cls: 0.1108  decode.d1.loss_mask: 0.2179  decode.d1.loss_dice: 0.2419  decode.d2.loss_cls: 0.0751  decode.d2.loss_mask: 0.2182  decode.d2.loss_dice: 0.2428  decode.d3.loss_cls: 0.0532  decode.d3.loss_mask: 0.2162  decode.d3.loss_dice: 0.2380  decode.d4.loss_cls: 0.0472  decode.d4.loss_mask: 0.2191  decode.d4.loss_dice: 0.2417  decode.d5.loss_cls: 0.0505  decode.d5.loss_mask: 0.2204  decode.d5.loss_dice: 0.2417  decode.d6.loss_cls: 0.0506  decode.d6.loss_mask: 0.2170  decode.d6.loss_dice: 0.2405  decode.d7.loss_cls: 0.0763  decode.d7.loss_mask: 0.2182  decode.d7.loss_dice: 0.2478  decode.d8.loss_cls: 0.0698  decode.d8.loss_mask: 0.2144  decode.d8.loss_dice: 0.2390
08/06 04:43:54 - mmengine - INFO - Iter(train) [ 20200/320000]  base_lr: 9.4301e-05 lr: 9.4301e-06  eta: 1 day, 12:23:28  time: 0.4400  data_time: 0.0090  memory: 5260  grad_norm: 55.6633  loss: 6.3073  decode.loss_cls: 0.0651  decode.loss_mask: 0.2211  decode.loss_dice: 0.2282  decode.d0.loss_cls: 0.9778  decode.d0.loss_mask: 0.2281  decode.d0.loss_dice: 0.2404  decode.d1.loss_cls: 0.1772  decode.d1.loss_mask: 0.2239  decode.d1.loss_dice: 0.2322  decode.d2.loss_cls: 0.0866  decode.d2.loss_mask: 0.2194  decode.d2.loss_dice: 0.2368  decode.d3.loss_cls: 0.0640  decode.d3.loss_mask: 0.2203  decode.d3.loss_dice: 0.2372  decode.d4.loss_cls: 0.0613  decode.d4.loss_mask: 0.2208  decode.d4.loss_dice: 0.2271  decode.d5.loss_cls: 0.0779  decode.d5.loss_mask: 0.2212  decode.d5.loss_dice: 0.2311  decode.d6.loss_cls: 0.0843  decode.d6.loss_mask: 0.2224  decode.d6.loss_dice: 0.2278  decode.d7.loss_cls: 0.0754  decode.d7.loss_mask: 0.2247  decode.d7.loss_dice: 0.2229  decode.d8.loss_cls: 0.0777  decode.d8.loss_mask: 0.2425  decode.d8.loss_dice: 0.2317
08/06 04:44:15 - mmengine - INFO - Iter(train) [ 20250/320000]  base_lr: 9.4286e-05 lr: 9.4286e-06  eta: 1 day, 12:23:07  time: 0.4381  data_time: 0.0092  memory: 5223  grad_norm: 84.5668  loss: 9.1505  decode.loss_cls: 0.1826  decode.loss_mask: 0.3228  decode.loss_dice: 0.3034  decode.d0.loss_cls: 0.8850  decode.d0.loss_mask: 0.3328  decode.d0.loss_dice: 0.3136  decode.d1.loss_cls: 0.2739  decode.d1.loss_mask: 0.3275  decode.d1.loss_dice: 0.2965  decode.d2.loss_cls: 0.2363  decode.d2.loss_mask: 0.3268  decode.d2.loss_dice: 0.2973  decode.d3.loss_cls: 0.2172  decode.d3.loss_mask: 0.3235  decode.d3.loss_dice: 0.2710  decode.d4.loss_cls: 0.2645  decode.d4.loss_mask: 0.3247  decode.d4.loss_dice: 0.2913  decode.d5.loss_cls: 0.2262  decode.d5.loss_mask: 0.3270  decode.d5.loss_dice: 0.2772  decode.d6.loss_cls: 0.2638  decode.d6.loss_mask: 0.3280  decode.d6.loss_dice: 0.2818  decode.d7.loss_cls: 0.2098  decode.d7.loss_mask: 0.3216  decode.d7.loss_dice: 0.2782  decode.d8.loss_cls: 0.1963  decode.d8.loss_mask: 0.3260  decode.d8.loss_dice: 0.3238
08/06 04:44:37 - mmengine - INFO - Iter(train) [ 20300/320000]  base_lr: 9.4272e-05 lr: 9.4272e-06  eta: 1 day, 12:22:47  time: 0.4389  data_time: 0.0091  memory: 5242  grad_norm: 118.8607  loss: 7.5832  decode.loss_cls: 0.1981  decode.loss_mask: 0.2166  decode.loss_dice: 0.2870  decode.d0.loss_cls: 0.8089  decode.d0.loss_mask: 0.1949  decode.d0.loss_dice: 0.2880  decode.d1.loss_cls: 0.2215  decode.d1.loss_mask: 0.2086  decode.d1.loss_dice: 0.2862  decode.d2.loss_cls: 0.2227  decode.d2.loss_mask: 0.1964  decode.d2.loss_dice: 0.2790  decode.d3.loss_cls: 0.2328  decode.d3.loss_mask: 0.1978  decode.d3.loss_dice: 0.2687  decode.d4.loss_cls: 0.2229  decode.d4.loss_mask: 0.2122  decode.d4.loss_dice: 0.2706  decode.d5.loss_cls: 0.2483  decode.d5.loss_mask: 0.2042  decode.d5.loss_dice: 0.2632  decode.d6.loss_cls: 0.1824  decode.d6.loss_mask: 0.2122  decode.d6.loss_dice: 0.2771  decode.d7.loss_cls: 0.1882  decode.d7.loss_mask: 0.2157  decode.d7.loss_dice: 0.2814  decode.d8.loss_cls: 0.1804  decode.d8.loss_mask: 0.2199  decode.d8.loss_dice: 0.2976
08/06 04:44:59 - mmengine - INFO - Iter(train) [ 20350/320000]  base_lr: 9.4258e-05 lr: 9.4258e-06  eta: 1 day, 12:22:27  time: 0.4393  data_time: 0.0091  memory: 5258  grad_norm: 65.5380  loss: 11.9729  decode.loss_cls: 0.5177  decode.loss_mask: 0.2622  decode.loss_dice: 0.3596  decode.d0.loss_cls: 1.0264  decode.d0.loss_mask: 0.2668  decode.d0.loss_dice: 0.3917  decode.d1.loss_cls: 0.5260  decode.d1.loss_mask: 0.2648  decode.d1.loss_dice: 0.3827  decode.d2.loss_cls: 0.5180  decode.d2.loss_mask: 0.2609  decode.d2.loss_dice: 0.3715  decode.d3.loss_cls: 0.5037  decode.d3.loss_mask: 0.2570  decode.d3.loss_dice: 0.3773  decode.d4.loss_cls: 0.5074  decode.d4.loss_mask: 0.2591  decode.d4.loss_dice: 0.3339  decode.d5.loss_cls: 0.4747  decode.d5.loss_mask: 0.2660  decode.d5.loss_dice: 0.3573  decode.d6.loss_cls: 0.5180  decode.d6.loss_mask: 0.2583  decode.d6.loss_dice: 0.3585  decode.d7.loss_cls: 0.4820  decode.d7.loss_mask: 0.2629  decode.d7.loss_dice: 0.4040  decode.d8.loss_cls: 0.5313  decode.d8.loss_mask: 0.2713  decode.d8.loss_dice: 0.4020
08/06 04:45:21 - mmengine - INFO - Iter(train) [ 20400/320000]  base_lr: 9.4244e-05 lr: 9.4244e-06  eta: 1 day, 12:22:06  time: 0.4390  data_time: 0.0092  memory: 5260  grad_norm: 83.8088  loss: 7.5685  decode.loss_cls: 0.1061  decode.loss_mask: 0.2792  decode.loss_dice: 0.3061  decode.d0.loss_cls: 0.8774  decode.d0.loss_mask: 0.2671  decode.d0.loss_dice: 0.3274  decode.d1.loss_cls: 0.1383  decode.d1.loss_mask: 0.2726  decode.d1.loss_dice: 0.3297  decode.d2.loss_cls: 0.1696  decode.d2.loss_mask: 0.2615  decode.d2.loss_dice: 0.2700  decode.d3.loss_cls: 0.1575  decode.d3.loss_mask: 0.2619  decode.d3.loss_dice: 0.2676  decode.d4.loss_cls: 0.1626  decode.d4.loss_mask: 0.2616  decode.d4.loss_dice: 0.2736  decode.d5.loss_cls: 0.0864  decode.d5.loss_mask: 0.2632  decode.d5.loss_dice: 0.3058  decode.d6.loss_cls: 0.1342  decode.d6.loss_mask: 0.2586  decode.d6.loss_dice: 0.2710  decode.d7.loss_cls: 0.1004  decode.d7.loss_mask: 0.2637  decode.d7.loss_dice: 0.2661  decode.d8.loss_cls: 0.0981  decode.d8.loss_mask: 0.2633  decode.d8.loss_dice: 0.2682
08/06 04:45:43 - mmengine - INFO - Iter(train) [ 20450/320000]  base_lr: 9.4230e-05 lr: 9.4230e-06  eta: 1 day, 12:21:46  time: 0.4391  data_time: 0.0089  memory: 5242  grad_norm: 81.8195  loss: 8.3114  decode.loss_cls: 0.2181  decode.loss_mask: 0.2129  decode.loss_dice: 0.2896  decode.d0.loss_cls: 0.8801  decode.d0.loss_mask: 0.2229  decode.d0.loss_dice: 0.2976  decode.d1.loss_cls: 0.3260  decode.d1.loss_mask: 0.2147  decode.d1.loss_dice: 0.2859  decode.d2.loss_cls: 0.2175  decode.d2.loss_mask: 0.2142  decode.d2.loss_dice: 0.2700  decode.d3.loss_cls: 0.2668  decode.d3.loss_mask: 0.2190  decode.d3.loss_dice: 0.2850  decode.d4.loss_cls: 0.3327  decode.d4.loss_mask: 0.2176  decode.d4.loss_dice: 0.2971  decode.d5.loss_cls: 0.2868  decode.d5.loss_mask: 0.2192  decode.d5.loss_dice: 0.2690  decode.d6.loss_cls: 0.2621  decode.d6.loss_mask: 0.2157  decode.d6.loss_dice: 0.2614  decode.d7.loss_cls: 0.3149  decode.d7.loss_mask: 0.2166  decode.d7.loss_dice: 0.2753  decode.d8.loss_cls: 0.2487  decode.d8.loss_mask: 0.2138  decode.d8.loss_dice: 0.2602
08/06 04:46:05 - mmengine - INFO - Iter(train) [ 20500/320000]  base_lr: 9.4216e-05 lr: 9.4216e-06  eta: 1 day, 12:21:26  time: 0.4398  data_time: 0.0094  memory: 5260  grad_norm: 66.1840  loss: 8.2177  decode.loss_cls: 0.1163  decode.loss_mask: 0.2645  decode.loss_dice: 0.3227  decode.d0.loss_cls: 0.8462  decode.d0.loss_mask: 0.2757  decode.d0.loss_dice: 0.3115  decode.d1.loss_cls: 0.1688  decode.d1.loss_mask: 0.2654  decode.d1.loss_dice: 0.3019  decode.d2.loss_cls: 0.1333  decode.d2.loss_mask: 0.2686  decode.d2.loss_dice: 0.3169  decode.d3.loss_cls: 0.1935  decode.d3.loss_mask: 0.2623  decode.d3.loss_dice: 0.2982  decode.d4.loss_cls: 0.1884  decode.d4.loss_mask: 0.2631  decode.d4.loss_dice: 0.3309  decode.d5.loss_cls: 0.1593  decode.d5.loss_mask: 0.2616  decode.d5.loss_dice: 0.3183  decode.d6.loss_cls: 0.2077  decode.d6.loss_mask: 0.2610  decode.d6.loss_dice: 0.2954  decode.d7.loss_cls: 0.1674  decode.d7.loss_mask: 0.2590  decode.d7.loss_dice: 0.3204  decode.d8.loss_cls: 0.2395  decode.d8.loss_mask: 0.2619  decode.d8.loss_dice: 0.3379
08/06 04:46:27 - mmengine - INFO - Iter(train) [ 20550/320000]  base_lr: 9.4202e-05 lr: 9.4202e-06  eta: 1 day, 12:21:07  time: 0.4403  data_time: 0.0095  memory: 5260  grad_norm: 113.6484  loss: 9.6491  decode.loss_cls: 0.2615  decode.loss_mask: 0.3182  decode.loss_dice: 0.3351  decode.d0.loss_cls: 0.9748  decode.d0.loss_mask: 0.3270  decode.d0.loss_dice: 0.3304  decode.d1.loss_cls: 0.2829  decode.d1.loss_mask: 0.3218  decode.d1.loss_dice: 0.3462  decode.d2.loss_cls: 0.1788  decode.d2.loss_mask: 0.3098  decode.d2.loss_dice: 0.3351  decode.d3.loss_cls: 0.2283  decode.d3.loss_mask: 0.3056  decode.d3.loss_dice: 0.3251  decode.d4.loss_cls: 0.2962  decode.d4.loss_mask: 0.2884  decode.d4.loss_dice: 0.3165  decode.d5.loss_cls: 0.1882  decode.d5.loss_mask: 0.3504  decode.d5.loss_dice: 0.3599  decode.d6.loss_cls: 0.1935  decode.d6.loss_mask: 0.3180  decode.d6.loss_dice: 0.3350  decode.d7.loss_cls: 0.2124  decode.d7.loss_mask: 0.3463  decode.d7.loss_dice: 0.3605  decode.d8.loss_cls: 0.2110  decode.d8.loss_mask: 0.3355  decode.d8.loss_dice: 0.3570
08/06 04:46:49 - mmengine - INFO - Iter(train) [ 20600/320000]  base_lr: 9.4187e-05 lr: 9.4187e-06  eta: 1 day, 12:20:47  time: 0.4401  data_time: 0.0093  memory: 5223  grad_norm: 80.2936  loss: 6.6301  decode.loss_cls: 0.0815  decode.loss_mask: 0.2248  decode.loss_dice: 0.2620  decode.d0.loss_cls: 0.9201  decode.d0.loss_mask: 0.2203  decode.d0.loss_dice: 0.2502  decode.d1.loss_cls: 0.1271  decode.d1.loss_mask: 0.2212  decode.d1.loss_dice: 0.2477  decode.d2.loss_cls: 0.1257  decode.d2.loss_mask: 0.2215  decode.d2.loss_dice: 0.2459  decode.d3.loss_cls: 0.1009  decode.d3.loss_mask: 0.2258  decode.d3.loss_dice: 0.2562  decode.d4.loss_cls: 0.1067  decode.d4.loss_mask: 0.2230  decode.d4.loss_dice: 0.2526  decode.d5.loss_cls: 0.0894  decode.d5.loss_mask: 0.2156  decode.d5.loss_dice: 0.2417  decode.d6.loss_cls: 0.1146  decode.d6.loss_mask: 0.2216  decode.d6.loss_dice: 0.2511  decode.d7.loss_cls: 0.1106  decode.d7.loss_mask: 0.2158  decode.d7.loss_dice: 0.2464  decode.d8.loss_cls: 0.1554  decode.d8.loss_mask: 0.2167  decode.d8.loss_dice: 0.2381
08/06 04:47:11 - mmengine - INFO - Iter(train) [ 20650/320000]  base_lr: 9.4173e-05 lr: 9.4173e-06  eta: 1 day, 12:20:28  time: 0.4424  data_time: 0.0093  memory: 5242  grad_norm: 241.5526  loss: 9.6334  decode.loss_cls: 0.3501  decode.loss_mask: 0.2306  decode.loss_dice: 0.3389  decode.d0.loss_cls: 1.0338  decode.d0.loss_mask: 0.2181  decode.d0.loss_dice: 0.3331  decode.d1.loss_cls: 0.4193  decode.d1.loss_mask: 0.2647  decode.d1.loss_dice: 0.3110  decode.d2.loss_cls: 0.3350  decode.d2.loss_mask: 0.2425  decode.d2.loss_dice: 0.3429  decode.d3.loss_cls: 0.3425  decode.d3.loss_mask: 0.2475  decode.d3.loss_dice: 0.3343  decode.d4.loss_cls: 0.2612  decode.d4.loss_mask: 0.2437  decode.d4.loss_dice: 0.3273  decode.d5.loss_cls: 0.2393  decode.d5.loss_mask: 0.2753  decode.d5.loss_dice: 0.3368  decode.d6.loss_cls: 0.2706  decode.d6.loss_mask: 0.2706  decode.d6.loss_dice: 0.3263  decode.d7.loss_cls: 0.2556  decode.d7.loss_mask: 0.2675  decode.d7.loss_dice: 0.3157  decode.d8.loss_cls: 0.3313  decode.d8.loss_mask: 0.2538  decode.d8.loss_dice: 0.3140
08/06 04:47:33 - mmengine - INFO - Iter(train) [ 20700/320000]  base_lr: 9.4159e-05 lr: 9.4159e-06  eta: 1 day, 12:20:09  time: 0.4400  data_time: 0.0092  memory: 5260  grad_norm: 58.9784  loss: 8.8394  decode.loss_cls: 0.3028  decode.loss_mask: 0.2076  decode.loss_dice: 0.3111  decode.d0.loss_cls: 0.9315  decode.d0.loss_mask: 0.2046  decode.d0.loss_dice: 0.3117  decode.d1.loss_cls: 0.2971  decode.d1.loss_mask: 0.2070  decode.d1.loss_dice: 0.3009  decode.d2.loss_cls: 0.3429  decode.d2.loss_mask: 0.2086  decode.d2.loss_dice: 0.3110  decode.d3.loss_cls: 0.3109  decode.d3.loss_mask: 0.2058  decode.d3.loss_dice: 0.2947  decode.d4.loss_cls: 0.3076  decode.d4.loss_mask: 0.2046  decode.d4.loss_dice: 0.2951  decode.d5.loss_cls: 0.2995  decode.d5.loss_mask: 0.2036  decode.d5.loss_dice: 0.3039  decode.d6.loss_cls: 0.3229  decode.d6.loss_mask: 0.2071  decode.d6.loss_dice: 0.2991  decode.d7.loss_cls: 0.3369  decode.d7.loss_mask: 0.2055  decode.d7.loss_dice: 0.2980  decode.d8.loss_cls: 0.3061  decode.d8.loss_mask: 0.2051  decode.d8.loss_dice: 0.2961
08/06 04:47:55 - mmengine - INFO - Iter(train) [ 20750/320000]  base_lr: 9.4145e-05 lr: 9.4145e-06  eta: 1 day, 12:19:49  time: 0.4397  data_time: 0.0092  memory: 5222  grad_norm: 268.0128  loss: 12.7471  decode.loss_cls: 0.4125  decode.loss_mask: 0.3466  decode.loss_dice: 0.4032  decode.d0.loss_cls: 1.1137  decode.d0.loss_mask: 0.3505  decode.d0.loss_dice: 0.4505  decode.d1.loss_cls: 0.4146  decode.d1.loss_mask: 0.3437  decode.d1.loss_dice: 0.4007  decode.d2.loss_cls: 0.3983  decode.d2.loss_mask: 0.3384  decode.d2.loss_dice: 0.4212  decode.d3.loss_cls: 0.4765  decode.d3.loss_mask: 0.3148  decode.d3.loss_dice: 0.4030  decode.d4.loss_cls: 0.4328  decode.d4.loss_mask: 0.3532  decode.d4.loss_dice: 0.4149  decode.d5.loss_cls: 0.5329  decode.d5.loss_mask: 0.3484  decode.d5.loss_dice: 0.4059  decode.d6.loss_cls: 0.5264  decode.d6.loss_mask: 0.3432  decode.d6.loss_dice: 0.3999  decode.d7.loss_cls: 0.4400  decode.d7.loss_mask: 0.3512  decode.d7.loss_dice: 0.4157  decode.d8.loss_cls: 0.4145  decode.d8.loss_mask: 0.3517  decode.d8.loss_dice: 0.4279
08/06 04:48:17 - mmengine - INFO - Iter(train) [ 20800/320000]  base_lr: 9.4131e-05 lr: 9.4131e-06  eta: 1 day, 12:19:29  time: 0.4394  data_time: 0.0094  memory: 5240  grad_norm: 135.8448  loss: 8.8020  decode.loss_cls: 0.3003  decode.loss_mask: 0.2778  decode.loss_dice: 0.2819  decode.d0.loss_cls: 1.0274  decode.d0.loss_mask: 0.2846  decode.d0.loss_dice: 0.2785  decode.d1.loss_cls: 0.2672  decode.d1.loss_mask: 0.2678  decode.d1.loss_dice: 0.2707  decode.d2.loss_cls: 0.2125  decode.d2.loss_mask: 0.2620  decode.d2.loss_dice: 0.2597  decode.d3.loss_cls: 0.1834  decode.d3.loss_mask: 0.2602  decode.d3.loss_dice: 0.2710  decode.d4.loss_cls: 0.2067  decode.d4.loss_mask: 0.2632  decode.d4.loss_dice: 0.2743  decode.d5.loss_cls: 0.2522  decode.d5.loss_mask: 0.2713  decode.d5.loss_dice: 0.2958  decode.d6.loss_cls: 0.3070  decode.d6.loss_mask: 0.2877  decode.d6.loss_dice: 0.2923  decode.d7.loss_cls: 0.2853  decode.d7.loss_mask: 0.2696  decode.d7.loss_dice: 0.2866  decode.d8.loss_cls: 0.2688  decode.d8.loss_mask: 0.2694  decode.d8.loss_dice: 0.2669
08/06 04:48:39 - mmengine - INFO - Iter(train) [ 20850/320000]  base_lr: 9.4117e-05 lr: 9.4117e-06  eta: 1 day, 12:19:10  time: 0.4396  data_time: 0.0093  memory: 5260  grad_norm: 74.7986  loss: 7.4190  decode.loss_cls: 0.1761  decode.loss_mask: 0.2707  decode.loss_dice: 0.2763  decode.d0.loss_cls: 0.8488  decode.d0.loss_mask: 0.2781  decode.d0.loss_dice: 0.3125  decode.d1.loss_cls: 0.0936  decode.d1.loss_mask: 0.2700  decode.d1.loss_dice: 0.2905  decode.d2.loss_cls: 0.1149  decode.d2.loss_mask: 0.2686  decode.d2.loss_dice: 0.2827  decode.d3.loss_cls: 0.0934  decode.d3.loss_mask: 0.2708  decode.d3.loss_dice: 0.2884  decode.d4.loss_cls: 0.0879  decode.d4.loss_mask: 0.2679  decode.d4.loss_dice: 0.2936  decode.d5.loss_cls: 0.0968  decode.d5.loss_mask: 0.2654  decode.d5.loss_dice: 0.2839  decode.d6.loss_cls: 0.0926  decode.d6.loss_mask: 0.2654  decode.d6.loss_dice: 0.2901  decode.d7.loss_cls: 0.0992  decode.d7.loss_mask: 0.2685  decode.d7.loss_dice: 0.2881  decode.d8.loss_cls: 0.1295  decode.d8.loss_mask: 0.2666  decode.d8.loss_dice: 0.2880
08/06 04:49:02 - mmengine - INFO - Iter(train) [ 20900/320000]  base_lr: 9.4102e-05 lr: 9.4102e-06  eta: 1 day, 12:18:52  time: 0.4401  data_time: 0.0094  memory: 5260  grad_norm: 106.8089  loss: 8.9504  decode.loss_cls: 0.1925  decode.loss_mask: 0.3007  decode.loss_dice: 0.2992  decode.d0.loss_cls: 0.8320  decode.d0.loss_mask: 0.3189  decode.d0.loss_dice: 0.3348  decode.d1.loss_cls: 0.2879  decode.d1.loss_mask: 0.3032  decode.d1.loss_dice: 0.2923  decode.d2.loss_cls: 0.2101  decode.d2.loss_mask: 0.3022  decode.d2.loss_dice: 0.2835  decode.d3.loss_cls: 0.2264  decode.d3.loss_mask: 0.3086  decode.d3.loss_dice: 0.2959  decode.d4.loss_cls: 0.2151  decode.d4.loss_mask: 0.3076  decode.d4.loss_dice: 0.2876  decode.d5.loss_cls: 0.2410  decode.d5.loss_mask: 0.3055  decode.d5.loss_dice: 0.2981  decode.d6.loss_cls: 0.2513  decode.d6.loss_mask: 0.3063  decode.d6.loss_dice: 0.3009  decode.d7.loss_cls: 0.1917  decode.d7.loss_mask: 0.3011  decode.d7.loss_dice: 0.2918  decode.d8.loss_cls: 0.2659  decode.d8.loss_mask: 0.2995  decode.d8.loss_dice: 0.2990
08/06 04:49:24 - mmengine - INFO - Iter(train) [ 20950/320000]  base_lr: 9.4088e-05 lr: 9.4088e-06  eta: 1 day, 12:18:33  time: 0.4402  data_time: 0.0093  memory: 5242  grad_norm: 67.0419  loss: 7.9054  decode.loss_cls: 0.2083  decode.loss_mask: 0.2478  decode.loss_dice: 0.2989  decode.d0.loss_cls: 0.7244  decode.d0.loss_mask: 0.2559  decode.d0.loss_dice: 0.3086  decode.d1.loss_cls: 0.2045  decode.d1.loss_mask: 0.2484  decode.d1.loss_dice: 0.2928  decode.d2.loss_cls: 0.1909  decode.d2.loss_mask: 0.2469  decode.d2.loss_dice: 0.2780  decode.d3.loss_cls: 0.1844  decode.d3.loss_mask: 0.2492  decode.d3.loss_dice: 0.2978  decode.d4.loss_cls: 0.1856  decode.d4.loss_mask: 0.2493  decode.d4.loss_dice: 0.2951  decode.d5.loss_cls: 0.2038  decode.d5.loss_mask: 0.2442  decode.d5.loss_dice: 0.3011  decode.d6.loss_cls: 0.1803  decode.d6.loss_mask: 0.2450  decode.d6.loss_dice: 0.2952  decode.d7.loss_cls: 0.1930  decode.d7.loss_mask: 0.2433  decode.d7.loss_dice: 0.2878  decode.d8.loss_cls: 0.1918  decode.d8.loss_mask: 0.2517  decode.d8.loss_dice: 0.3016
08/06 04:49:46 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 04:49:46 - mmengine - INFO - Iter(train) [ 21000/320000]  base_lr: 9.4074e-05 lr: 9.4074e-06  eta: 1 day, 12:18:13  time: 0.4392  data_time: 0.0090  memory: 5242  grad_norm: 52.8216  loss: 6.8509  decode.loss_cls: 0.1283  decode.loss_mask: 0.2045  decode.loss_dice: 0.2588  decode.d0.loss_cls: 0.9387  decode.d0.loss_mask: 0.2050  decode.d0.loss_dice: 0.2838  decode.d1.loss_cls: 0.0957  decode.d1.loss_mask: 0.2053  decode.d1.loss_dice: 0.2634  decode.d2.loss_cls: 0.0927  decode.d2.loss_mask: 0.2043  decode.d2.loss_dice: 0.2649  decode.d3.loss_cls: 0.1492  decode.d3.loss_mask: 0.2059  decode.d3.loss_dice: 0.2759  decode.d4.loss_cls: 0.1531  decode.d4.loss_mask: 0.2011  decode.d4.loss_dice: 0.2629  decode.d5.loss_cls: 0.1460  decode.d5.loss_mask: 0.2019  decode.d5.loss_dice: 0.2595  decode.d6.loss_cls: 0.1461  decode.d6.loss_mask: 0.2058  decode.d6.loss_dice: 0.2690  decode.d7.loss_cls: 0.1423  decode.d7.loss_mask: 0.2043  decode.d7.loss_dice: 0.2627  decode.d8.loss_cls: 0.1367  decode.d8.loss_mask: 0.2057  decode.d8.loss_dice: 0.2774
08/06 04:50:08 - mmengine - INFO - Iter(train) [ 21050/320000]  base_lr: 9.4060e-05 lr: 9.4060e-06  eta: 1 day, 12:17:53  time: 0.4402  data_time: 0.0093  memory: 5258  grad_norm: 52.0999  loss: 6.3646  decode.loss_cls: 0.0863  decode.loss_mask: 0.2053  decode.loss_dice: 0.2629  decode.d0.loss_cls: 0.8625  decode.d0.loss_mask: 0.2069  decode.d0.loss_dice: 0.2710  decode.d1.loss_cls: 0.0910  decode.d1.loss_mask: 0.2101  decode.d1.loss_dice: 0.2696  decode.d2.loss_cls: 0.0976  decode.d2.loss_mask: 0.2080  decode.d2.loss_dice: 0.2668  decode.d3.loss_cls: 0.0960  decode.d3.loss_mask: 0.2087  decode.d3.loss_dice: 0.2626  decode.d4.loss_cls: 0.0850  decode.d4.loss_mask: 0.2103  decode.d4.loss_dice: 0.2660  decode.d5.loss_cls: 0.0798  decode.d5.loss_mask: 0.2066  decode.d5.loss_dice: 0.2640  decode.d6.loss_cls: 0.0687  decode.d6.loss_mask: 0.2056  decode.d6.loss_dice: 0.2660  decode.d7.loss_cls: 0.0793  decode.d7.loss_mask: 0.2059  decode.d7.loss_dice: 0.2609  decode.d8.loss_cls: 0.0939  decode.d8.loss_mask: 0.2056  decode.d8.loss_dice: 0.2617
08/06 04:50:30 - mmengine - INFO - Iter(train) [ 21100/320000]  base_lr: 9.4046e-05 lr: 9.4046e-06  eta: 1 day, 12:17:33  time: 0.4400  data_time: 0.0091  memory: 5260  grad_norm: 93.5073  loss: 6.6899  decode.loss_cls: 0.1352  decode.loss_mask: 0.2327  decode.loss_dice: 0.2236  decode.d0.loss_cls: 0.7653  decode.d0.loss_mask: 0.2431  decode.d0.loss_dice: 0.2419  decode.d1.loss_cls: 0.1187  decode.d1.loss_mask: 0.2391  decode.d1.loss_dice: 0.2331  decode.d2.loss_cls: 0.1578  decode.d2.loss_mask: 0.2310  decode.d2.loss_dice: 0.2356  decode.d3.loss_cls: 0.1426  decode.d3.loss_mask: 0.2344  decode.d3.loss_dice: 0.2305  decode.d4.loss_cls: 0.1170  decode.d4.loss_mask: 0.2377  decode.d4.loss_dice: 0.2274  decode.d5.loss_cls: 0.1460  decode.d5.loss_mask: 0.2422  decode.d5.loss_dice: 0.2449  decode.d6.loss_cls: 0.1528  decode.d6.loss_mask: 0.2338  decode.d6.loss_dice: 0.2412  decode.d7.loss_cls: 0.1208  decode.d7.loss_mask: 0.2339  decode.d7.loss_dice: 0.2374  decode.d8.loss_cls: 0.1254  decode.d8.loss_mask: 0.2356  decode.d8.loss_dice: 0.2291
