==========================================
SLURM_JOB_ID = 2470938
SLURM_NODELIST = gnode070
SLURM_JOB_GPUS = 1
==========================================
08/06 02:16:35 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.23 (main, Jun  5 2025, 13:40:20) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 268722126
    GPU 0: NVIDIA GeForce RTX 2080 Ti
    CUDA_HOME: /opt/cuda-12.1/
    NVCC: Cuda compilation tools, release 12.1, V12.1.66
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.1.2
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.2
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: True
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 268722126
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

08/06 02:16:36 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=2, enable=False)
crop_size = (
    512,
    1024,
)
data_preprocessor = dict(
    bgr_to_rgb=True,
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    pad_val=0,
    seg_pad_val=255,
    size=(
        512,
        1024,
    ),
    std=[
        58.395,
        57.12,
        57.375,
    ],
    test_cfg=dict(size_divisor=32),
    type='SegDataPreProcessor')
data_root = '/scratch/seg_benchmark/splits_flat/'
dataset_type = 'YourDataset_BIG'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=40000,
        save_best='mIoU',
        type='CheckpointHook'),
    logger=dict(interval=50, log_metric_by_epoch=False, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'))
default_scope = 'mmseg'
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=True,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
img_ratios = [
    0.5,
    0.75,
    1.0,
    1.25,
    1.5,
    1.75,
]
img_suffix = '.jpg'
launcher = 'none'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=False)
model = dict(
    backbone=dict(
        deep_stem=False,
        depth=50,
        frozen_stages=-1,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=False, type='SyncBN'),
        num_stages=4,
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        style='pytorch',
        type='ResNet'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_val=0,
        seg_pad_val=255,
        size=(
            512,
            1024,
        ),
        std=[
            58.395,
            57.12,
            57.375,
        ],
        test_cfg=dict(size_divisor=32),
        type='SegDataPreProcessor'),
    decode_head=dict(
        align_corners=False,
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                0.1,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='mmdet.DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=True),
        num_classes=51,
        num_queries=100,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                init_cfg=None,
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=1024,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        im2col_step=64,
                        init_cfg=None,
                        norm_cfg=None,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            init_cfg=None,
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='mmdet.MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        train_cfg=dict(
            assigner=dict(
                match_costs=[
                    dict(type='mmdet.ClassificationCost', weight=2.0),
                    dict(
                        type='mmdet.CrossEntropyLossCost',
                        use_sigmoid=True,
                        weight=5.0),
                    dict(
                        eps=1.0,
                        pred_act=True,
                        type='mmdet.DiceCost',
                        weight=5.0),
                ],
                type='mmdet.HungarianAssigner'),
            importance_sample_ratio=0.75,
            num_points=12544,
            oversample_ratio=3.0,
            sampler=dict(type='mmdet.MaskPseudoSampler')),
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    attn_drop=0.0,
                    batch_first=True,
                    dropout_layer=None,
                    embed_dims=256,
                    num_heads=8,
                    proj_drop=0.0),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    add_identity=True,
                    dropout_layer=None,
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    attn_drop=0.0,
                    batch_first=True,
                    dropout_layer=None,
                    embed_dims=256,
                    num_heads=8,
                    proj_drop=0.0)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(mode='whole'),
    train_cfg=dict(),
    type='EncoderDecoder')
num_classes = 51
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(decay_mult=1.0, lr_mult=0.1),
            level_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_feat=dict(decay_mult=0.0, lr_mult=1.0)),
        norm_decay_mult=0.0),
    type='OptimWrapper')
optimizer = dict(
    betas=(
        0.9,
        0.999,
    ),
    eps=1e-08,
    lr=0.0001,
    type='AdamW',
    weight_decay=0.05)
param_scheduler = [
    dict(
        begin=0,
        by_epoch=False,
        end=320000,
        eta_min=0,
        power=0.9,
        type='PolyLR'),
]
randomness = dict(seed=268722126)
resume = False
seg_map_suffix = '.png'
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_prefix=dict(img_path='test/images', seg_map_path='test/masks'),
        data_root='/scratch/seg_benchmark/splits_flat/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                2048,
                1024,
            ), type='Resize'),
            dict(type='LoadAnnotations'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=8,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    classwise=True, iou_metrics=[
        'mIoU',
    ], type='IoUNanAbsent')
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        2048,
        1024,
    ), type='Resize'),
    dict(type='LoadAnnotations'),
    dict(type='PackSegInputs'),
]
train_cfg = dict(
    max_iters=320000, type='IterBasedTrainLoop', val_interval=40000)
train_dataloader = dict(
    batch_size=2,
    dataset=dict(
        data_prefix=dict(img_path='train/images', seg_map_path='train/masks'),
        data_root='/scratch/seg_benchmark/splits_flat/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(
                max_size=4096,
                resize_type='ResizeShortestEdge',
                scales=[
                    512,
                    614,
                    716,
                    819,
                    921,
                    1024,
                    1126,
                    1228,
                    1331,
                    1433,
                    1536,
                    1638,
                    1740,
                    1843,
                    1945,
                    2048,
                ],
                type='RandomChoiceResize'),
            dict(
                cat_max_ratio=0.75, crop_size=(
                    512,
                    1024,
                ), type='RandomCrop'),
            dict(prob=0.5, type='RandomFlip'),
            dict(type='PhotoMetricDistortion'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='InfiniteSampler'))
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(
        max_size=4096,
        resize_type='ResizeShortestEdge',
        scales=[
            512,
            614,
            716,
            819,
            921,
            1024,
            1126,
            1228,
            1331,
            1433,
            1536,
            1638,
            1740,
            1843,
            1945,
            2048,
        ],
        type='RandomChoiceResize'),
    dict(cat_max_ratio=0.75, crop_size=(
        512,
        1024,
    ), type='RandomCrop'),
    dict(prob=0.5, type='RandomFlip'),
    dict(type='PhotoMetricDistortion'),
    dict(type='PackSegInputs'),
]
tta_model = dict(type='SegTTAModel')
tta_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        transforms=[
            [
                dict(keep_ratio=True, scale_factor=0.5, type='Resize'),
                dict(keep_ratio=True, scale_factor=0.75, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.0, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.25, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.5, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.75, type='Resize'),
            ],
            [
                dict(direction='horizontal', prob=0.0, type='RandomFlip'),
                dict(direction='horizontal', prob=1.0, type='RandomFlip'),
            ],
            [
                dict(type='LoadAnnotations'),
            ],
            [
                dict(type='PackSegInputs'),
            ],
        ],
        type='TestTimeAug'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_prefix=dict(img_path='test/images', seg_map_path='test/masks'),
        data_root='/scratch/seg_benchmark/splits_flat/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                2048,
                1024,
            ), type='Resize'),
            dict(type='LoadAnnotations'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=8,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    iou_metrics=[
        'mIoU',
    ], type='IoUMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='SegLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = '/scratch/seg_benchmark/NEW/mask2former_R50_320K'

08/06 02:16:41 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
08/06 02:16:41 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.downsample.1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.0.downsample.1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.1.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer1.2.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.downsample.1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.0.downsample.1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.1.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.2.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer2.3.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.downsample.1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.0.downsample.1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn3.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.1.bn3.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.2.bn1.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.2.bn1.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr_mult=0.1
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:decay_mult=1.0
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.2.bn2.weight is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - WARNING - backbone.layer3.2.bn2.bias is skipped since its requires_grad=False
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr=1e-05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:weight_decay=0.05
08/06 02:16:41 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.2.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.2.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.3.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.4.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer3.5.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.downsample.1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.0.downsample.1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.1.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn1.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn1.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn2.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn2.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr=1e-05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:weight_decay=0.05
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr_mult=0.1
08/06 02:16:42 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:decay_mult=1.0
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn3.weight is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - WARNING - backbone.layer4.2.bn3.bias is skipped since its requires_grad=False
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.bias:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr=0.0001
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr_mult=1.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:decay_mult=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr=0.0001
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr_mult=1.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:decay_mult=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr=0.0001
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:weight_decay=0.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr_mult=1.0
08/06 02:16:42 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:decay_mult=0.0
08/06 02:16:42 - mmengine - WARNING - The prefix is not set in metric class IoUMetric.
08/06 02:16:42 - mmengine - INFO - load model from: torchvision://resnet50
08/06 02:16:42 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
08/06 02:16:46 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

08/06 02:16:46 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
08/06 02:16:46 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
08/06 02:16:46 - mmengine - INFO - Checkpoints will be saved to /scratch/seg_benchmark/NEW/mask2former_R50_320K.
/home2/yasharora120/miniconda3/envs/mmseg/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400441250/work/aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
08/06 02:17:16 - mmengine - INFO - Iter(train) [    50/320000]  base_lr: 9.9986e-05 lr: 9.9986e-06  eta: 2 days, 5:08:50  time: 0.4298  data_time: 0.0092  memory: 10587  grad_norm: 183.2636  loss: 92.9535  decode.loss_cls: 4.0095  decode.loss_mask: 2.2549  decode.loss_dice: 3.9745  decode.d0.loss_cls: 7.9950  decode.d0.loss_mask: 1.6893  decode.d0.loss_dice: 3.3309  decode.d1.loss_cls: 3.5442  decode.d1.loss_mask: 1.6469  decode.d1.loss_dice: 3.2318  decode.d2.loss_cls: 3.4345  decode.d2.loss_mask: 1.6396  decode.d2.loss_dice: 3.2074  decode.d3.loss_cls: 3.3822  decode.d3.loss_mask: 1.7013  decode.d3.loss_dice: 3.2107  decode.d4.loss_cls: 3.3065  decode.d4.loss_mask: 1.6747  decode.d4.loss_dice: 3.2904  decode.d5.loss_cls: 3.4944  decode.d5.loss_mask: 1.7029  decode.d5.loss_dice: 3.3384  decode.d6.loss_cls: 3.5047  decode.d6.loss_mask: 1.8024  decode.d6.loss_dice: 3.4708  decode.d7.loss_cls: 3.6863  decode.d7.loss_mask: 1.8941  decode.d7.loss_dice: 3.7199  decode.d8.loss_cls: 3.8917  decode.d8.loss_mask: 2.0631  decode.d8.loss_dice: 3.8607
08/06 02:17:37 - mmengine - INFO - Iter(train) [   100/320000]  base_lr: 9.9972e-05 lr: 9.9972e-06  eta: 1 day, 21:41:58  time: 0.4312  data_time: 0.0091  memory: 5260  grad_norm: 215.9784  loss: 71.3652  decode.loss_cls: 2.9681  decode.loss_mask: 1.3867  decode.loss_dice: 2.6143  decode.d0.loss_cls: 7.7737  decode.d0.loss_mask: 1.2754  decode.d0.loss_dice: 2.6706  decode.d1.loss_cls: 3.0295  decode.d1.loss_mask: 1.2729  decode.d1.loss_dice: 2.4263  decode.d2.loss_cls: 2.9109  decode.d2.loss_mask: 1.2391  decode.d2.loss_dice: 2.3386  decode.d3.loss_cls: 2.8028  decode.d3.loss_mask: 1.2924  decode.d3.loss_dice: 2.4304  decode.d4.loss_cls: 2.7899  decode.d4.loss_mask: 1.3313  decode.d4.loss_dice: 2.4120  decode.d5.loss_cls: 2.7976  decode.d5.loss_mask: 1.3130  decode.d5.loss_dice: 2.4458  decode.d6.loss_cls: 2.7010  decode.d6.loss_mask: 1.3893  decode.d6.loss_dice: 2.4347  decode.d7.loss_cls: 2.7975  decode.d7.loss_mask: 1.3624  decode.d7.loss_dice: 2.4555  decode.d8.loss_cls: 2.9194  decode.d8.loss_mask: 1.2798  decode.d8.loss_dice: 2.5041
08/06 02:17:59 - mmengine - INFO - Iter(train) [   150/320000]  base_lr: 9.9958e-05 lr: 9.9958e-06  eta: 1 day, 19:15:53  time: 0.4332  data_time: 0.0091  memory: 5260  grad_norm: 207.9954  loss: 61.3996  decode.loss_cls: 2.8618  decode.loss_mask: 1.1160  decode.loss_dice: 1.8397  decode.d0.loss_cls: 7.6095  decode.d0.loss_mask: 1.1976  decode.d0.loss_dice: 2.1397  decode.d1.loss_cls: 2.8133  decode.d1.loss_mask: 1.0924  decode.d1.loss_dice: 1.7774  decode.d2.loss_cls: 2.7379  decode.d2.loss_mask: 1.0689  decode.d2.loss_dice: 1.7101  decode.d3.loss_cls: 2.7320  decode.d3.loss_mask: 1.0504  decode.d3.loss_dice: 1.7103  decode.d4.loss_cls: 2.8426  decode.d4.loss_mask: 1.0488  decode.d4.loss_dice: 1.6752  decode.d5.loss_cls: 2.7305  decode.d5.loss_mask: 1.0342  decode.d5.loss_dice: 1.6862  decode.d6.loss_cls: 2.7602  decode.d6.loss_mask: 1.0882  decode.d6.loss_dice: 1.7240  decode.d7.loss_cls: 2.7922  decode.d7.loss_mask: 1.0864  decode.d7.loss_dice: 1.7569  decode.d8.loss_cls: 2.8668  decode.d8.loss_mask: 1.0805  decode.d8.loss_dice: 1.7700
08/06 02:18:20 - mmengine - INFO - Iter(train) [   200/320000]  base_lr: 9.9944e-05 lr: 9.9944e-06  eta: 1 day, 18:03:37  time: 0.4328  data_time: 0.0089  memory: 5240  grad_norm: 327.2682  loss: 56.1046  decode.loss_cls: 2.6932  decode.loss_mask: 1.1377  decode.loss_dice: 1.4273  decode.d0.loss_cls: 7.4872  decode.d0.loss_mask: 1.0732  decode.d0.loss_dice: 1.8283  decode.d1.loss_cls: 2.7273  decode.d1.loss_mask: 1.0404  decode.d1.loss_dice: 1.3811  decode.d2.loss_cls: 2.7731  decode.d2.loss_mask: 0.9285  decode.d2.loss_dice: 1.2679  decode.d3.loss_cls: 2.6820  decode.d3.loss_mask: 0.9399  decode.d3.loss_dice: 1.3039  decode.d4.loss_cls: 2.7278  decode.d4.loss_mask: 0.9964  decode.d4.loss_dice: 1.2987  decode.d5.loss_cls: 2.7129  decode.d5.loss_mask: 0.9965  decode.d5.loss_dice: 1.2972  decode.d6.loss_cls: 2.6895  decode.d6.loss_mask: 0.9868  decode.d6.loss_dice: 1.3194  decode.d7.loss_cls: 2.7328  decode.d7.loss_mask: 1.0304  decode.d7.loss_dice: 1.3704  decode.d8.loss_cls: 2.6728  decode.d8.loss_mask: 1.1481  decode.d8.loss_dice: 1.4339
08/06 02:18:42 - mmengine - INFO - Iter(train) [   250/320000]  base_lr: 9.9930e-05 lr: 9.9930e-06  eta: 1 day, 17:20:44  time: 0.4340  data_time: 0.0089  memory: 5242  grad_norm: 232.7529  loss: 56.0446  decode.loss_cls: 2.8953  decode.loss_mask: 0.9571  decode.loss_dice: 1.4374  decode.d0.loss_cls: 7.4452  decode.d0.loss_mask: 0.9181  decode.d0.loss_dice: 1.8180  decode.d1.loss_cls: 2.9368  decode.d1.loss_mask: 0.8344  decode.d1.loss_dice: 1.4301  decode.d2.loss_cls: 2.8264  decode.d2.loss_mask: 0.8098  decode.d2.loss_dice: 1.3361  decode.d3.loss_cls: 2.8802  decode.d3.loss_mask: 0.7894  decode.d3.loss_dice: 1.3366  decode.d4.loss_cls: 2.8217  decode.d4.loss_mask: 0.8263  decode.d4.loss_dice: 1.3365  decode.d5.loss_cls: 2.8927  decode.d5.loss_mask: 0.8111  decode.d5.loss_dice: 1.3257  decode.d6.loss_cls: 2.8508  decode.d6.loss_mask: 0.8623  decode.d6.loss_dice: 1.3273  decode.d7.loss_cls: 2.8742  decode.d7.loss_mask: 0.9003  decode.d7.loss_dice: 1.3541  decode.d8.loss_cls: 2.8743  decode.d8.loss_mask: 0.9522  decode.d8.loss_dice: 1.3841
08/06 02:19:04 - mmengine - INFO - Iter(train) [   300/320000]  base_lr: 9.9916e-05 lr: 9.9916e-06  eta: 1 day, 16:52:23  time: 0.4336  data_time: 0.0089  memory: 5260  grad_norm: 215.9315  loss: 48.5751  decode.loss_cls: 2.4033  decode.loss_mask: 0.9940  decode.loss_dice: 1.1254  decode.d0.loss_cls: 7.2143  decode.d0.loss_mask: 0.9395  decode.d0.loss_dice: 1.4419  decode.d1.loss_cls: 2.4498  decode.d1.loss_mask: 0.9232  decode.d1.loss_dice: 1.1072  decode.d2.loss_cls: 2.4183  decode.d2.loss_mask: 0.8796  decode.d2.loss_dice: 0.9888  decode.d3.loss_cls: 2.3668  decode.d3.loss_mask: 0.8663  decode.d3.loss_dice: 0.9776  decode.d4.loss_cls: 2.4288  decode.d4.loss_mask: 0.8135  decode.d4.loss_dice: 0.9171  decode.d5.loss_cls: 2.4691  decode.d5.loss_mask: 0.8479  decode.d5.loss_dice: 0.9691  decode.d6.loss_cls: 2.4191  decode.d6.loss_mask: 0.8615  decode.d6.loss_dice: 0.9662  decode.d7.loss_cls: 2.4396  decode.d7.loss_mask: 0.8729  decode.d7.loss_dice: 1.0304  decode.d8.loss_cls: 2.4398  decode.d8.loss_mask: 0.9621  decode.d8.loss_dice: 1.0420
08/06 02:19:26 - mmengine - INFO - Iter(train) [   350/320000]  base_lr: 9.9902e-05 lr: 9.9902e-06  eta: 1 day, 16:32:54  time: 0.4356  data_time: 0.0091  memory: 5240  grad_norm: 182.9954  loss: 45.7380  decode.loss_cls: 2.5844  decode.loss_mask: 0.6825  decode.loss_dice: 0.9401  decode.d0.loss_cls: 6.9975  decode.d0.loss_mask: 0.8084  decode.d0.loss_dice: 1.3737  decode.d1.loss_cls: 2.5919  decode.d1.loss_mask: 0.6437  decode.d1.loss_dice: 1.0046  decode.d2.loss_cls: 2.4930  decode.d2.loss_mask: 0.6133  decode.d2.loss_dice: 0.9224  decode.d3.loss_cls: 2.5312  decode.d3.loss_mask: 0.6084  decode.d3.loss_dice: 0.8996  decode.d4.loss_cls: 2.6254  decode.d4.loss_mask: 0.5850  decode.d4.loss_dice: 0.8600  decode.d5.loss_cls: 2.5525  decode.d5.loss_mask: 0.6046  decode.d5.loss_dice: 0.8392  decode.d6.loss_cls: 2.5647  decode.d6.loss_mask: 0.5748  decode.d6.loss_dice: 0.8335  decode.d7.loss_cls: 2.5378  decode.d7.loss_mask: 0.5742  decode.d7.loss_dice: 0.8348  decode.d8.loss_cls: 2.5135  decode.d8.loss_mask: 0.6311  decode.d8.loss_dice: 0.9119
08/06 02:19:47 - mmengine - INFO - Iter(train) [   400/320000]  base_lr: 9.9888e-05 lr: 9.9888e-06  eta: 1 day, 16:18:16  time: 0.4356  data_time: 0.0090  memory: 5260  grad_norm: 222.2104  loss: 44.7097  decode.loss_cls: 2.5530  decode.loss_mask: 0.6699  decode.loss_dice: 0.9060  decode.d0.loss_cls: 6.8951  decode.d0.loss_mask: 0.7707  decode.d0.loss_dice: 1.2566  decode.d1.loss_cls: 2.2706  decode.d1.loss_mask: 0.7307  decode.d1.loss_dice: 1.0400  decode.d2.loss_cls: 2.2823  decode.d2.loss_mask: 0.7280  decode.d2.loss_dice: 0.9051  decode.d3.loss_cls: 2.2815  decode.d3.loss_mask: 0.6778  decode.d3.loss_dice: 0.8909  decode.d4.loss_cls: 2.2974  decode.d4.loss_mask: 0.6684  decode.d4.loss_dice: 0.8856  decode.d5.loss_cls: 2.3571  decode.d5.loss_mask: 0.6423  decode.d5.loss_dice: 0.8809  decode.d6.loss_cls: 2.4441  decode.d6.loss_mask: 0.6558  decode.d6.loss_dice: 0.8729  decode.d7.loss_cls: 2.4548  decode.d7.loss_mask: 0.6698  decode.d7.loss_dice: 0.8970  decode.d8.loss_cls: 2.5243  decode.d8.loss_mask: 0.6924  decode.d8.loss_dice: 0.9084
08/06 02:20:09 - mmengine - INFO - Iter(train) [   450/320000]  base_lr: 9.9874e-05 lr: 9.9874e-06  eta: 1 day, 16:06:49  time: 0.4351  data_time: 0.0091  memory: 5242  grad_norm: 246.9214  loss: 42.9736  decode.loss_cls: 2.3538  decode.loss_mask: 0.6646  decode.loss_dice: 0.7871  decode.d0.loss_cls: 6.7473  decode.d0.loss_mask: 0.7802  decode.d0.loss_dice: 1.1377  decode.d1.loss_cls: 2.3159  decode.d1.loss_mask: 0.6882  decode.d1.loss_dice: 0.8548  decode.d2.loss_cls: 2.3369  decode.d2.loss_mask: 0.7009  decode.d2.loss_dice: 0.7915  decode.d3.loss_cls: 2.3135  decode.d3.loss_mask: 0.6891  decode.d3.loss_dice: 0.7914  decode.d4.loss_cls: 2.3250  decode.d4.loss_mask: 0.6652  decode.d4.loss_dice: 0.7368  decode.d5.loss_cls: 2.3030  decode.d5.loss_mask: 0.6752  decode.d5.loss_dice: 0.8018  decode.d6.loss_cls: 2.3544  decode.d6.loss_mask: 0.6397  decode.d6.loss_dice: 0.7861  decode.d7.loss_cls: 2.3319  decode.d7.loss_mask: 0.7142  decode.d7.loss_dice: 0.8269  decode.d8.loss_cls: 2.3822  decode.d8.loss_mask: 0.6403  decode.d8.loss_dice: 0.8381
08/06 02:20:31 - mmengine - INFO - Iter(train) [   500/320000]  base_lr: 9.9860e-05 lr: 9.9860e-06  eta: 1 day, 15:58:00  time: 0.4338  data_time: 0.0091  memory: 5240  grad_norm: 222.3095  loss: 38.5922  decode.loss_cls: 2.0201  decode.loss_mask: 0.6720  decode.loss_dice: 0.6671  decode.d0.loss_cls: 6.5202  decode.d0.loss_mask: 0.6838  decode.d0.loss_dice: 0.9248  decode.d1.loss_cls: 2.1407  decode.d1.loss_mask: 0.6789  decode.d1.loss_dice: 0.7193  decode.d2.loss_cls: 1.9956  decode.d2.loss_mask: 0.6994  decode.d2.loss_dice: 0.6898  decode.d3.loss_cls: 2.0305  decode.d3.loss_mask: 0.6568  decode.d3.loss_dice: 0.6559  decode.d4.loss_cls: 2.0103  decode.d4.loss_mask: 0.6386  decode.d4.loss_dice: 0.6696  decode.d5.loss_cls: 2.0159  decode.d5.loss_mask: 0.6390  decode.d5.loss_dice: 0.6930  decode.d6.loss_cls: 2.0705  decode.d6.loss_mask: 0.6859  decode.d6.loss_dice: 0.6870  decode.d7.loss_cls: 2.0223  decode.d7.loss_mask: 0.6527  decode.d7.loss_dice: 0.6625  decode.d8.loss_cls: 2.0794  decode.d8.loss_mask: 0.6687  decode.d8.loss_dice: 0.6418
08/06 02:20:53 - mmengine - INFO - Iter(train) [   550/320000]  base_lr: 9.9846e-05 lr: 9.9846e-06  eta: 1 day, 15:50:15  time: 0.4351  data_time: 0.0092  memory: 5242  grad_norm: 196.3281  loss: 39.6669  decode.loss_cls: 2.1541  decode.loss_mask: 0.5553  decode.loss_dice: 0.7230  decode.d0.loss_cls: 6.4388  decode.d0.loss_mask: 0.6153  decode.d0.loss_dice: 0.9351  decode.d1.loss_cls: 2.3242  decode.d1.loss_mask: 0.5643  decode.d1.loss_dice: 0.7141  decode.d2.loss_cls: 2.2910  decode.d2.loss_mask: 0.5139  decode.d2.loss_dice: 0.6422  decode.d3.loss_cls: 2.2927  decode.d3.loss_mask: 0.4884  decode.d3.loss_dice: 0.5899  decode.d4.loss_cls: 2.2336  decode.d4.loss_mask: 0.5251  decode.d4.loss_dice: 0.6612  decode.d5.loss_cls: 2.2904  decode.d5.loss_mask: 0.5835  decode.d5.loss_dice: 0.7987  decode.d6.loss_cls: 2.2725  decode.d6.loss_mask: 0.5874  decode.d6.loss_dice: 0.7690  decode.d7.loss_cls: 2.2722  decode.d7.loss_mask: 0.5686  decode.d7.loss_dice: 0.7431  decode.d8.loss_cls: 2.2186  decode.d8.loss_mask: 0.5655  decode.d8.loss_dice: 0.7353
08/06 02:21:14 - mmengine - INFO - Iter(train) [   600/320000]  base_lr: 9.9832e-05 lr: 9.9832e-06  eta: 1 day, 15:43:53  time: 0.4350  data_time: 0.0090  memory: 5224  grad_norm: 154.8336  loss: 37.7889  decode.loss_cls: 2.1964  decode.loss_mask: 0.5243  decode.loss_dice: 0.5378  decode.d0.loss_cls: 6.3210  decode.d0.loss_mask: 0.6327  decode.d0.loss_dice: 0.7749  decode.d1.loss_cls: 2.3479  decode.d1.loss_mask: 0.5788  decode.d1.loss_dice: 0.6060  decode.d2.loss_cls: 2.2972  decode.d2.loss_mask: 0.5216  decode.d2.loss_dice: 0.5272  decode.d3.loss_cls: 2.2872  decode.d3.loss_mask: 0.5118  decode.d3.loss_dice: 0.4823  decode.d4.loss_cls: 2.3233  decode.d4.loss_mask: 0.5270  decode.d4.loss_dice: 0.5098  decode.d5.loss_cls: 2.3088  decode.d5.loss_mask: 0.5222  decode.d5.loss_dice: 0.5318  decode.d6.loss_cls: 2.2552  decode.d6.loss_mask: 0.5093  decode.d6.loss_dice: 0.5225  decode.d7.loss_cls: 2.2471  decode.d7.loss_mask: 0.5275  decode.d7.loss_dice: 0.5420  decode.d8.loss_cls: 2.2374  decode.d8.loss_mask: 0.5351  decode.d8.loss_dice: 0.5427
08/06 02:21:36 - mmengine - INFO - Iter(train) [   650/320000]  base_lr: 9.9817e-05 lr: 9.9817e-06  eta: 1 day, 15:38:27  time: 0.4362  data_time: 0.0090  memory: 5260  grad_norm: 171.3614  loss: 36.2365  decode.loss_cls: 2.1068  decode.loss_mask: 0.4905  decode.loss_dice: 0.6159  decode.d0.loss_cls: 6.1561  decode.d0.loss_mask: 0.5338  decode.d0.loss_dice: 0.8613  decode.d1.loss_cls: 2.0378  decode.d1.loss_mask: 0.4779  decode.d1.loss_dice: 0.6196  decode.d2.loss_cls: 2.0800  decode.d2.loss_mask: 0.4896  decode.d2.loss_dice: 0.6053  decode.d3.loss_cls: 2.0044  decode.d3.loss_mask: 0.4594  decode.d3.loss_dice: 0.6096  decode.d4.loss_cls: 2.0427  decode.d4.loss_mask: 0.4647  decode.d4.loss_dice: 0.5934  decode.d5.loss_cls: 2.0410  decode.d5.loss_mask: 0.4676  decode.d5.loss_dice: 0.6611  decode.d6.loss_cls: 2.0875  decode.d6.loss_mask: 0.4867  decode.d6.loss_dice: 0.6736  decode.d7.loss_cls: 2.1637  decode.d7.loss_mask: 0.4674  decode.d7.loss_dice: 0.6399  decode.d8.loss_cls: 2.1439  decode.d8.loss_mask: 0.5034  decode.d8.loss_dice: 0.6519
08/06 02:21:58 - mmengine - INFO - Iter(train) [   700/320000]  base_lr: 9.9803e-05 lr: 9.9803e-06  eta: 1 day, 15:33:39  time: 0.4356  data_time: 0.0089  memory: 5258  grad_norm: 220.4612  loss: 35.8167  decode.loss_cls: 1.8655  decode.loss_mask: 0.6378  decode.loss_dice: 0.6827  decode.d0.loss_cls: 6.1555  decode.d0.loss_mask: 0.7346  decode.d0.loss_dice: 0.9269  decode.d1.loss_cls: 2.1187  decode.d1.loss_mask: 0.5765  decode.d1.loss_dice: 0.6335  decode.d2.loss_cls: 1.9362  decode.d2.loss_mask: 0.5492  decode.d2.loss_dice: 0.6070  decode.d3.loss_cls: 1.8908  decode.d3.loss_mask: 0.5246  decode.d3.loss_dice: 0.5723  decode.d4.loss_cls: 1.8860  decode.d4.loss_mask: 0.5429  decode.d4.loss_dice: 0.5334  decode.d5.loss_cls: 1.8957  decode.d5.loss_mask: 0.5776  decode.d5.loss_dice: 0.5956  decode.d6.loss_cls: 1.9038  decode.d6.loss_mask: 0.6375  decode.d6.loss_dice: 0.6371  decode.d7.loss_cls: 1.9430  decode.d7.loss_mask: 0.5751  decode.d7.loss_dice: 0.5928  decode.d8.loss_cls: 1.9016  decode.d8.loss_mask: 0.5727  decode.d8.loss_dice: 0.6103
08/06 02:22:20 - mmengine - INFO - Iter(train) [   750/320000]  base_lr: 9.9789e-05 lr: 9.9789e-06  eta: 1 day, 15:29:41  time: 0.4365  data_time: 0.0091  memory: 5242  grad_norm: 199.2333  loss: 33.4879  decode.loss_cls: 1.9464  decode.loss_mask: 0.4901  decode.loss_dice: 0.5218  decode.d0.loss_cls: 5.8628  decode.d0.loss_mask: 0.5100  decode.d0.loss_dice: 0.7506  decode.d1.loss_cls: 2.0548  decode.d1.loss_mask: 0.4669  decode.d1.loss_dice: 0.5365  decode.d2.loss_cls: 1.9260  decode.d2.loss_mask: 0.4388  decode.d2.loss_dice: 0.4891  decode.d3.loss_cls: 1.9664  decode.d3.loss_mask: 0.4372  decode.d3.loss_dice: 0.4801  decode.d4.loss_cls: 1.9393  decode.d4.loss_mask: 0.4624  decode.d4.loss_dice: 0.5106  decode.d5.loss_cls: 2.0047  decode.d5.loss_mask: 0.4340  decode.d5.loss_dice: 0.4895  decode.d6.loss_cls: 1.9154  decode.d6.loss_mask: 0.4460  decode.d6.loss_dice: 0.4907  decode.d7.loss_cls: 1.9479  decode.d7.loss_mask: 0.5013  decode.d7.loss_dice: 0.5157  decode.d8.loss_cls: 1.9570  decode.d8.loss_mask: 0.4821  decode.d8.loss_dice: 0.5140
08/06 02:22:42 - mmengine - INFO - Iter(train) [   800/320000]  base_lr: 9.9775e-05 lr: 9.9775e-06  eta: 1 day, 15:26:20  time: 0.4366  data_time: 0.0089  memory: 5260  grad_norm: 157.6750  loss: 33.6729  decode.loss_cls: 1.8129  decode.loss_mask: 0.5412  decode.loss_dice: 0.5793  decode.d0.loss_cls: 5.6713  decode.d0.loss_mask: 0.6259  decode.d0.loss_dice: 0.7500  decode.d1.loss_cls: 1.9753  decode.d1.loss_mask: 0.5318  decode.d1.loss_dice: 0.5759  decode.d2.loss_cls: 1.7821  decode.d2.loss_mask: 0.5452  decode.d2.loss_dice: 0.6023  decode.d3.loss_cls: 1.8763  decode.d3.loss_mask: 0.5200  decode.d3.loss_dice: 0.5469  decode.d4.loss_cls: 1.8226  decode.d4.loss_mask: 0.5197  decode.d4.loss_dice: 0.5447  decode.d5.loss_cls: 1.8687  decode.d5.loss_mask: 0.5432  decode.d5.loss_dice: 0.5756  decode.d6.loss_cls: 1.9098  decode.d6.loss_mask: 0.5257  decode.d6.loss_dice: 0.5371  decode.d7.loss_cls: 1.8946  decode.d7.loss_mask: 0.5229  decode.d7.loss_dice: 0.5211  decode.d8.loss_cls: 1.8474  decode.d8.loss_mask: 0.5142  decode.d8.loss_dice: 0.5893
08/06 02:23:03 - mmengine - INFO - Iter(train) [   850/320000]  base_lr: 9.9761e-05 lr: 9.9761e-06  eta: 1 day, 15:23:10  time: 0.4355  data_time: 0.0090  memory: 5260  grad_norm: 176.2705  loss: 31.6168  decode.loss_cls: 1.6664  decode.loss_mask: 0.4749  decode.loss_dice: 0.5830  decode.d0.loss_cls: 5.4938  decode.d0.loss_mask: 0.5204  decode.d0.loss_dice: 0.7616  decode.d1.loss_cls: 1.7838  decode.d1.loss_mask: 0.5042  decode.d1.loss_dice: 0.5813  decode.d2.loss_cls: 1.7463  decode.d2.loss_mask: 0.4895  decode.d2.loss_dice: 0.5435  decode.d3.loss_cls: 1.7290  decode.d3.loss_mask: 0.4516  decode.d3.loss_dice: 0.5358  decode.d4.loss_cls: 1.7122  decode.d4.loss_mask: 0.4794  decode.d4.loss_dice: 0.5845  decode.d5.loss_cls: 1.6645  decode.d5.loss_mask: 0.4946  decode.d5.loss_dice: 0.5825  decode.d6.loss_cls: 1.6647  decode.d6.loss_mask: 0.4955  decode.d6.loss_dice: 0.5874  decode.d7.loss_cls: 1.7110  decode.d7.loss_mask: 0.4673  decode.d7.loss_dice: 0.5783  decode.d8.loss_cls: 1.6461  decode.d8.loss_mask: 0.4964  decode.d8.loss_dice: 0.5874
08/06 02:23:25 - mmengine - INFO - Iter(train) [   900/320000]  base_lr: 9.9747e-05 lr: 9.9747e-06  eta: 1 day, 15:20:17  time: 0.4350  data_time: 0.0088  memory: 5240  grad_norm: 229.7566  loss: 32.8998  decode.loss_cls: 1.6971  decode.loss_mask: 0.7154  decode.loss_dice: 0.5291  decode.d0.loss_cls: 5.3676  decode.d0.loss_mask: 0.6906  decode.d0.loss_dice: 0.7350  decode.d1.loss_cls: 1.9383  decode.d1.loss_mask: 0.5797  decode.d1.loss_dice: 0.5253  decode.d2.loss_cls: 1.8863  decode.d2.loss_mask: 0.4981  decode.d2.loss_dice: 0.4705  decode.d3.loss_cls: 1.8174  decode.d3.loss_mask: 0.5272  decode.d3.loss_dice: 0.4822  decode.d4.loss_cls: 1.8181  decode.d4.loss_mask: 0.5995  decode.d4.loss_dice: 0.5098  decode.d5.loss_cls: 1.7552  decode.d5.loss_mask: 0.6276  decode.d5.loss_dice: 0.4987  decode.d6.loss_cls: 1.7604  decode.d6.loss_mask: 0.6279  decode.d6.loss_dice: 0.5147  decode.d7.loss_cls: 1.6744  decode.d7.loss_mask: 0.6228  decode.d7.loss_dice: 0.5270  decode.d8.loss_cls: 1.7283  decode.d8.loss_mask: 0.6548  decode.d8.loss_dice: 0.5208
08/06 02:23:47 - mmengine - INFO - Iter(train) [   950/320000]  base_lr: 9.9733e-05 lr: 9.9733e-06  eta: 1 day, 15:17:43  time: 0.4362  data_time: 0.0089  memory: 5242  grad_norm: 116.2299  loss: 30.3387  decode.loss_cls: 1.7791  decode.loss_mask: 0.3373  decode.loss_dice: 0.5043  decode.d0.loss_cls: 5.2613  decode.d0.loss_mask: 0.3342  decode.d0.loss_dice: 0.5967  decode.d1.loss_cls: 2.0679  decode.d1.loss_mask: 0.3202  decode.d1.loss_dice: 0.4491  decode.d2.loss_cls: 1.9195  decode.d2.loss_mask: 0.3351  decode.d2.loss_dice: 0.4389  decode.d3.loss_cls: 1.9761  decode.d3.loss_mask: 0.3212  decode.d3.loss_dice: 0.4375  decode.d4.loss_cls: 1.8736  decode.d4.loss_mask: 0.3215  decode.d4.loss_dice: 0.4606  decode.d5.loss_cls: 1.8577  decode.d5.loss_mask: 0.3263  decode.d5.loss_dice: 0.4710  decode.d6.loss_cls: 1.8399  decode.d6.loss_mask: 0.3077  decode.d6.loss_dice: 0.4480  decode.d7.loss_cls: 1.9110  decode.d7.loss_mask: 0.3163  decode.d7.loss_dice: 0.4503  decode.d8.loss_cls: 1.8511  decode.d8.loss_mask: 0.3393  decode.d8.loss_dice: 0.4862
08/06 02:24:09 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 02:24:09 - mmengine - INFO - Iter(train) [  1000/320000]  base_lr: 9.9719e-05 lr: 9.9719e-06  eta: 1 day, 15:15:23  time: 0.4358  data_time: 0.0090  memory: 5236  grad_norm: 167.0411  loss: 29.2251  decode.loss_cls: 1.4432  decode.loss_mask: 0.6000  decode.loss_dice: 0.5273  decode.d0.loss_cls: 5.0145  decode.d0.loss_mask: 0.6056  decode.d0.loss_dice: 0.6232  decode.d1.loss_cls: 1.7209  decode.d1.loss_mask: 0.5714  decode.d1.loss_dice: 0.5051  decode.d2.loss_cls: 1.4502  decode.d2.loss_mask: 0.5395  decode.d2.loss_dice: 0.4837  decode.d3.loss_cls: 1.4857  decode.d3.loss_mask: 0.5297  decode.d3.loss_dice: 0.4849  decode.d4.loss_cls: 1.4642  decode.d4.loss_mask: 0.5458  decode.d4.loss_dice: 0.4713  decode.d5.loss_cls: 1.4655  decode.d5.loss_mask: 0.5311  decode.d5.loss_dice: 0.4570  decode.d6.loss_cls: 1.4598  decode.d6.loss_mask: 0.5715  decode.d6.loss_dice: 0.4987  decode.d7.loss_cls: 1.5165  decode.d7.loss_mask: 0.5876  decode.d7.loss_dice: 0.5199  decode.d8.loss_cls: 1.5005  decode.d8.loss_mask: 0.5675  decode.d8.loss_dice: 0.4834
08/06 02:24:31 - mmengine - INFO - Iter(train) [  1050/320000]  base_lr: 9.9705e-05 lr: 9.9705e-06  eta: 1 day, 15:13:15  time: 0.4367  data_time: 0.0088  memory: 5224  grad_norm: 127.5374  loss: 31.5408  decode.loss_cls: 1.8635  decode.loss_mask: 0.3817  decode.loss_dice: 0.5060  decode.d0.loss_cls: 5.0408  decode.d0.loss_mask: 0.5431  decode.d0.loss_dice: 0.7186  decode.d1.loss_cls: 2.1286  decode.d1.loss_mask: 0.3931  decode.d1.loss_dice: 0.4546  decode.d2.loss_cls: 1.9662  decode.d2.loss_mask: 0.3769  decode.d2.loss_dice: 0.4090  decode.d3.loss_cls: 2.0088  decode.d3.loss_mask: 0.3811  decode.d3.loss_dice: 0.4317  decode.d4.loss_cls: 2.0267  decode.d4.loss_mask: 0.3782  decode.d4.loss_dice: 0.4237  decode.d5.loss_cls: 2.0097  decode.d5.loss_mask: 0.3842  decode.d5.loss_dice: 0.4335  decode.d6.loss_cls: 1.9694  decode.d6.loss_mask: 0.3695  decode.d6.loss_dice: 0.4207  decode.d7.loss_cls: 1.9894  decode.d7.loss_mask: 0.3790  decode.d7.loss_dice: 0.4366  decode.d8.loss_cls: 1.9371  decode.d8.loss_mask: 0.3701  decode.d8.loss_dice: 0.4095
08/06 02:24:52 - mmengine - INFO - Iter(train) [  1100/320000]  base_lr: 9.9691e-05 lr: 9.9691e-06  eta: 1 day, 15:11:24  time: 0.4365  data_time: 0.0087  memory: 5260  grad_norm: 146.5344  loss: 27.8397  decode.loss_cls: 1.5463  decode.loss_mask: 0.3721  decode.loss_dice: 0.4440  decode.d0.loss_cls: 4.8055  decode.d0.loss_mask: 0.4289  decode.d0.loss_dice: 0.6357  decode.d1.loss_cls: 1.8014  decode.d1.loss_mask: 0.3768  decode.d1.loss_dice: 0.4584  decode.d2.loss_cls: 1.5961  decode.d2.loss_mask: 0.3493  decode.d2.loss_dice: 0.4242  decode.d3.loss_cls: 1.6817  decode.d3.loss_mask: 0.3325  decode.d3.loss_dice: 0.4236  decode.d4.loss_cls: 1.6432  decode.d4.loss_mask: 0.3455  decode.d4.loss_dice: 0.4543  decode.d5.loss_cls: 1.5992  decode.d5.loss_mask: 0.3625  decode.d5.loss_dice: 0.4626  decode.d6.loss_cls: 1.6635  decode.d6.loss_mask: 0.3721  decode.d6.loss_dice: 0.4580  decode.d7.loss_cls: 1.5800  decode.d7.loss_mask: 0.3678  decode.d7.loss_dice: 0.4449  decode.d8.loss_cls: 1.5770  decode.d8.loss_mask: 0.3781  decode.d8.loss_dice: 0.4547
08/06 02:25:14 - mmengine - INFO - Iter(train) [  1150/320000]  base_lr: 9.9677e-05 lr: 9.9677e-06  eta: 1 day, 15:09:41  time: 0.4366  data_time: 0.0090  memory: 5260  grad_norm: 169.7483  loss: 31.5674  decode.loss_cls: 1.8668  decode.loss_mask: 0.4314  decode.loss_dice: 0.5279  decode.d0.loss_cls: 4.7312  decode.d0.loss_mask: 0.4745  decode.d0.loss_dice: 0.7377  decode.d1.loss_cls: 1.9972  decode.d1.loss_mask: 0.4685  decode.d1.loss_dice: 0.5413  decode.d2.loss_cls: 1.8208  decode.d2.loss_mask: 0.4747  decode.d2.loss_dice: 0.5596  decode.d3.loss_cls: 1.8695  decode.d3.loss_mask: 0.4707  decode.d3.loss_dice: 0.5126  decode.d4.loss_cls: 1.8878  decode.d4.loss_mask: 0.4597  decode.d4.loss_dice: 0.5129  decode.d5.loss_cls: 1.8502  decode.d5.loss_mask: 0.4445  decode.d5.loss_dice: 0.5410  decode.d6.loss_cls: 1.8091  decode.d6.loss_mask: 0.4502  decode.d6.loss_dice: 0.5369  decode.d7.loss_cls: 1.7726  decode.d7.loss_mask: 0.4293  decode.d7.loss_dice: 0.5341  decode.d8.loss_cls: 1.8566  decode.d8.loss_mask: 0.4480  decode.d8.loss_dice: 0.5502
08/06 02:25:36 - mmengine - INFO - Iter(train) [  1200/320000]  base_lr: 9.9663e-05 lr: 9.9663e-06  eta: 1 day, 15:08:07  time: 0.4373  data_time: 0.0088  memory: 5316  grad_norm: 158.2509  loss: 28.4689  decode.loss_cls: 1.6149  decode.loss_mask: 0.3636  decode.loss_dice: 0.4496  decode.d0.loss_cls: 4.5328  decode.d0.loss_mask: 0.4196  decode.d0.loss_dice: 0.6568  decode.d1.loss_cls: 1.7978  decode.d1.loss_mask: 0.4025  decode.d1.loss_dice: 0.4926  decode.d2.loss_cls: 1.7363  decode.d2.loss_mask: 0.3876  decode.d2.loss_dice: 0.4864  decode.d3.loss_cls: 1.7011  decode.d3.loss_mask: 0.3705  decode.d3.loss_dice: 0.4599  decode.d4.loss_cls: 1.6488  decode.d4.loss_mask: 0.3817  decode.d4.loss_dice: 0.4497  decode.d5.loss_cls: 1.6787  decode.d5.loss_mask: 0.3723  decode.d5.loss_dice: 0.4633  decode.d6.loss_cls: 1.6922  decode.d6.loss_mask: 0.3659  decode.d6.loss_dice: 0.4538  decode.d7.loss_cls: 1.7026  decode.d7.loss_mask: 0.3781  decode.d7.loss_dice: 0.4703  decode.d8.loss_cls: 1.6655  decode.d8.loss_mask: 0.3815  decode.d8.loss_dice: 0.4927
08/06 02:25:58 - mmengine - INFO - Iter(train) [  1250/320000]  base_lr: 9.9649e-05 lr: 9.9649e-06  eta: 1 day, 15:07:20  time: 0.4353  data_time: 0.0088  memory: 5240  grad_norm: 169.8421  loss: 27.5978  decode.loss_cls: 1.6697  decode.loss_mask: 0.3094  decode.loss_dice: 0.4725  decode.d0.loss_cls: 4.3859  decode.d0.loss_mask: 0.4568  decode.d0.loss_dice: 0.7481  decode.d1.loss_cls: 1.7971  decode.d1.loss_mask: 0.3762  decode.d1.loss_dice: 0.4879  decode.d2.loss_cls: 1.6435  decode.d2.loss_mask: 0.3431  decode.d2.loss_dice: 0.4496  decode.d3.loss_cls: 1.5690  decode.d3.loss_mask: 0.3260  decode.d3.loss_dice: 0.4324  decode.d4.loss_cls: 1.5690  decode.d4.loss_mask: 0.3169  decode.d4.loss_dice: 0.4366  decode.d5.loss_cls: 1.6190  decode.d5.loss_mask: 0.3131  decode.d5.loss_dice: 0.4456  decode.d6.loss_cls: 1.6865  decode.d6.loss_mask: 0.3351  decode.d6.loss_dice: 0.4495  decode.d7.loss_cls: 1.6599  decode.d7.loss_mask: 0.3207  decode.d7.loss_dice: 0.4483  decode.d8.loss_cls: 1.7456  decode.d8.loss_mask: 0.3264  decode.d8.loss_dice: 0.4585
08/06 02:26:20 - mmengine - INFO - Iter(train) [  1300/320000]  base_lr: 9.9635e-05 lr: 9.9635e-06  eta: 1 day, 15:05:44  time: 0.4362  data_time: 0.0090  memory: 5275  grad_norm: 177.7142  loss: 28.6257  decode.loss_cls: 1.5140  decode.loss_mask: 0.5069  decode.loss_dice: 0.5346  decode.d0.loss_cls: 4.2466  decode.d0.loss_mask: 0.5593  decode.d0.loss_dice: 0.7211  decode.d1.loss_cls: 1.5737  decode.d1.loss_mask: 0.5088  decode.d1.loss_dice: 0.5033  decode.d2.loss_cls: 1.5089  decode.d2.loss_mask: 0.5189  decode.d2.loss_dice: 0.5675  decode.d3.loss_cls: 1.5140  decode.d3.loss_mask: 0.5001  decode.d3.loss_dice: 0.4949  decode.d4.loss_cls: 1.5415  decode.d4.loss_mask: 0.5059  decode.d4.loss_dice: 0.4870  decode.d5.loss_cls: 1.5415  decode.d5.loss_mask: 0.4818  decode.d5.loss_dice: 0.5371  decode.d6.loss_cls: 1.5507  decode.d6.loss_mask: 0.4970  decode.d6.loss_dice: 0.5298  decode.d7.loss_cls: 1.6048  decode.d7.loss_mask: 0.4998  decode.d7.loss_dice: 0.5220  decode.d8.loss_cls: 1.4987  decode.d8.loss_mask: 0.5138  decode.d8.loss_dice: 0.5418
08/06 02:26:42 - mmengine - INFO - Iter(train) [  1350/320000]  base_lr: 9.9621e-05 lr: 9.9621e-06  eta: 1 day, 15:04:32  time: 0.4380  data_time: 0.0088  memory: 5299  grad_norm: 115.0178  loss: 25.4547  decode.loss_cls: 1.4613  decode.loss_mask: 0.3626  decode.loss_dice: 0.4029  decode.d0.loss_cls: 3.9720  decode.d0.loss_mask: 0.4062  decode.d0.loss_dice: 0.5531  decode.d1.loss_cls: 1.6826  decode.d1.loss_mask: 0.3644  decode.d1.loss_dice: 0.4094  decode.d2.loss_cls: 1.5312  decode.d2.loss_mask: 0.3566  decode.d2.loss_dice: 0.3698  decode.d3.loss_cls: 1.4403  decode.d3.loss_mask: 0.3473  decode.d3.loss_dice: 0.3594  decode.d4.loss_cls: 1.4919  decode.d4.loss_mask: 0.3483  decode.d4.loss_dice: 0.4110  decode.d5.loss_cls: 1.5066  decode.d5.loss_mask: 0.3768  decode.d5.loss_dice: 0.4221  decode.d6.loss_cls: 1.5283  decode.d6.loss_mask: 0.3744  decode.d6.loss_dice: 0.4349  decode.d7.loss_cls: 1.5223  decode.d7.loss_mask: 0.3781  decode.d7.loss_dice: 0.4108  decode.d8.loss_cls: 1.4328  decode.d8.loss_mask: 0.3767  decode.d8.loss_dice: 0.4205
08/06 02:27:04 - mmengine - INFO - Iter(train) [  1400/320000]  base_lr: 9.9606e-05 lr: 9.9606e-06  eta: 1 day, 15:03:21  time: 0.4357  data_time: 0.0090  memory: 5240  grad_norm: 142.0854  loss: 28.3856  decode.loss_cls: 1.5589  decode.loss_mask: 0.4697  decode.loss_dice: 0.4885  decode.d0.loss_cls: 3.9978  decode.d0.loss_mask: 0.5722  decode.d0.loss_dice: 0.7204  decode.d1.loss_cls: 1.9226  decode.d1.loss_mask: 0.4280  decode.d1.loss_dice: 0.4921  decode.d2.loss_cls: 1.6584  decode.d2.loss_mask: 0.4398  decode.d2.loss_dice: 0.4813  decode.d3.loss_cls: 1.6272  decode.d3.loss_mask: 0.4348  decode.d3.loss_dice: 0.4662  decode.d4.loss_cls: 1.5623  decode.d4.loss_mask: 0.4426  decode.d4.loss_dice: 0.4923  decode.d5.loss_cls: 1.5827  decode.d5.loss_mask: 0.4430  decode.d5.loss_dice: 0.4822  decode.d6.loss_cls: 1.5576  decode.d6.loss_mask: 0.4593  decode.d6.loss_dice: 0.5040  decode.d7.loss_cls: 1.6013  decode.d7.loss_mask: 0.4666  decode.d7.loss_dice: 0.4879  decode.d8.loss_cls: 1.5983  decode.d8.loss_mask: 0.4591  decode.d8.loss_dice: 0.4884
08/06 02:27:25 - mmengine - INFO - Iter(train) [  1450/320000]  base_lr: 9.9592e-05 lr: 9.9592e-06  eta: 1 day, 15:01:55  time: 0.4353  data_time: 0.0088  memory: 5242  grad_norm: 165.6602  loss: 27.5646  decode.loss_cls: 1.5693  decode.loss_mask: 0.3573  decode.loss_dice: 0.4856  decode.d0.loss_cls: 3.8352  decode.d0.loss_mask: 0.4422  decode.d0.loss_dice: 0.6512  decode.d1.loss_cls: 1.7629  decode.d1.loss_mask: 0.3796  decode.d1.loss_dice: 0.5269  decode.d2.loss_cls: 1.8075  decode.d2.loss_mask: 0.3651  decode.d2.loss_dice: 0.5008  decode.d3.loss_cls: 1.6551  decode.d3.loss_mask: 0.3840  decode.d3.loss_dice: 0.4682  decode.d4.loss_cls: 1.6104  decode.d4.loss_mask: 0.4010  decode.d4.loss_dice: 0.4759  decode.d5.loss_cls: 1.5868  decode.d5.loss_mask: 0.3773  decode.d5.loss_dice: 0.4739  decode.d6.loss_cls: 1.5721  decode.d6.loss_mask: 0.4005  decode.d6.loss_dice: 0.4962  decode.d7.loss_cls: 1.5818  decode.d7.loss_mask: 0.3982  decode.d7.loss_dice: 0.5030  decode.d8.loss_cls: 1.6158  decode.d8.loss_mask: 0.3710  decode.d8.loss_dice: 0.5100
08/06 02:27:47 - mmengine - INFO - Iter(train) [  1500/320000]  base_lr: 9.9578e-05 lr: 9.9578e-06  eta: 1 day, 15:00:32  time: 0.4351  data_time: 0.0088  memory: 5242  grad_norm: 204.4505  loss: 27.8359  decode.loss_cls: 1.4179  decode.loss_mask: 0.4431  decode.loss_dice: 0.5593  decode.d0.loss_cls: 3.5996  decode.d0.loss_mask: 0.5709  decode.d0.loss_dice: 0.7350  decode.d1.loss_cls: 1.6538  decode.d1.loss_mask: 0.4585  decode.d1.loss_dice: 0.5760  decode.d2.loss_cls: 1.4796  decode.d2.loss_mask: 0.4541  decode.d2.loss_dice: 0.5418  decode.d3.loss_cls: 1.4444  decode.d3.loss_mask: 0.4419  decode.d3.loss_dice: 0.5794  decode.d4.loss_cls: 1.4673  decode.d4.loss_mask: 0.5175  decode.d4.loss_dice: 0.6159  decode.d5.loss_cls: 1.4530  decode.d5.loss_mask: 0.4759  decode.d5.loss_dice: 0.5856  decode.d6.loss_cls: 1.4976  decode.d6.loss_mask: 0.4822  decode.d6.loss_dice: 0.5963  decode.d7.loss_cls: 1.5548  decode.d7.loss_mask: 0.5422  decode.d7.loss_dice: 0.5556  decode.d8.loss_cls: 1.5001  decode.d8.loss_mask: 0.4459  decode.d8.loss_dice: 0.5906
08/06 02:28:09 - mmengine - INFO - Iter(train) [  1550/320000]  base_lr: 9.9564e-05 lr: 9.9564e-06  eta: 1 day, 14:59:16  time: 0.4354  data_time: 0.0089  memory: 5275  grad_norm: 137.8878  loss: 23.9968  decode.loss_cls: 1.2390  decode.loss_mask: 0.4545  decode.loss_dice: 0.4298  decode.d0.loss_cls: 3.3951  decode.d0.loss_mask: 0.4885  decode.d0.loss_dice: 0.6004  decode.d1.loss_cls: 1.3940  decode.d1.loss_mask: 0.4574  decode.d1.loss_dice: 0.4426  decode.d2.loss_cls: 1.3397  decode.d2.loss_mask: 0.4383  decode.d2.loss_dice: 0.4150  decode.d3.loss_cls: 1.2684  decode.d3.loss_mask: 0.4456  decode.d3.loss_dice: 0.4200  decode.d4.loss_cls: 1.2518  decode.d4.loss_mask: 0.4316  decode.d4.loss_dice: 0.4294  decode.d5.loss_cls: 1.3029  decode.d5.loss_mask: 0.4448  decode.d5.loss_dice: 0.4548  decode.d6.loss_cls: 1.2442  decode.d6.loss_mask: 0.4459  decode.d6.loss_dice: 0.4278  decode.d7.loss_cls: 1.2785  decode.d7.loss_mask: 0.4357  decode.d7.loss_dice: 0.4490  decode.d8.loss_cls: 1.2543  decode.d8.loss_mask: 0.4591  decode.d8.loss_dice: 0.4587
08/06 02:28:31 - mmengine - INFO - Iter(train) [  1600/320000]  base_lr: 9.9550e-05 lr: 9.9550e-06  eta: 1 day, 14:58:05  time: 0.4357  data_time: 0.0089  memory: 5260  grad_norm: 128.9029  loss: 27.9528  decode.loss_cls: 1.6304  decode.loss_mask: 0.4361  decode.loss_dice: 0.5238  decode.d0.loss_cls: 3.4428  decode.d0.loss_mask: 0.4743  decode.d0.loss_dice: 0.6274  decode.d1.loss_cls: 1.8204  decode.d1.loss_mask: 0.4548  decode.d1.loss_dice: 0.5107  decode.d2.loss_cls: 1.6976  decode.d2.loss_mask: 0.4408  decode.d2.loss_dice: 0.4884  decode.d3.loss_cls: 1.6567  decode.d3.loss_mask: 0.4220  decode.d3.loss_dice: 0.5062  decode.d4.loss_cls: 1.6757  decode.d4.loss_mask: 0.4120  decode.d4.loss_dice: 0.5097  decode.d5.loss_cls: 1.6688  decode.d5.loss_mask: 0.4129  decode.d5.loss_dice: 0.4546  decode.d6.loss_cls: 1.6583  decode.d6.loss_mask: 0.4177  decode.d6.loss_dice: 0.4956  decode.d7.loss_cls: 1.7145  decode.d7.loss_mask: 0.4046  decode.d7.loss_dice: 0.5019  decode.d8.loss_cls: 1.6072  decode.d8.loss_mask: 0.4160  decode.d8.loss_dice: 0.4710
08/06 02:28:52 - mmengine - INFO - Iter(train) [  1650/320000]  base_lr: 9.9536e-05 lr: 9.9536e-06  eta: 1 day, 14:56:52  time: 0.4353  data_time: 0.0089  memory: 5260  grad_norm: 197.9220  loss: 28.0375  decode.loss_cls: 1.6239  decode.loss_mask: 0.4907  decode.loss_dice: 0.5255  decode.d0.loss_cls: 3.4233  decode.d0.loss_mask: 0.4835  decode.d0.loss_dice: 0.6982  decode.d1.loss_cls: 1.6134  decode.d1.loss_mask: 0.5069  decode.d1.loss_dice: 0.5345  decode.d2.loss_cls: 1.4792  decode.d2.loss_mask: 0.4958  decode.d2.loss_dice: 0.5297  decode.d3.loss_cls: 1.6632  decode.d3.loss_mask: 0.4728  decode.d3.loss_dice: 0.5316  decode.d4.loss_cls: 1.5934  decode.d4.loss_mask: 0.4751  decode.d4.loss_dice: 0.5163  decode.d5.loss_cls: 1.5163  decode.d5.loss_mask: 0.5131  decode.d5.loss_dice: 0.5427  decode.d6.loss_cls: 1.5703  decode.d6.loss_mask: 0.4992  decode.d6.loss_dice: 0.5086  decode.d7.loss_cls: 1.5809  decode.d7.loss_mask: 0.5200  decode.d7.loss_dice: 0.5447  decode.d8.loss_cls: 1.5153  decode.d8.loss_mask: 0.5131  decode.d8.loss_dice: 0.5564
08/06 02:29:14 - mmengine - INFO - Iter(train) [  1700/320000]  base_lr: 9.9522e-05 lr: 9.9522e-06  eta: 1 day, 14:55:43  time: 0.4357  data_time: 0.0089  memory: 5260  grad_norm: 193.7012  loss: 23.5595  decode.loss_cls: 1.2197  decode.loss_mask: 0.3325  decode.loss_dice: 0.4654  decode.d0.loss_cls: 3.2405  decode.d0.loss_mask: 0.4361  decode.d0.loss_dice: 0.7363  decode.d1.loss_cls: 1.4334  decode.d1.loss_mask: 0.3146  decode.d1.loss_dice: 0.4661  decode.d2.loss_cls: 1.3268  decode.d2.loss_mask: 0.3398  decode.d2.loss_dice: 0.4576  decode.d3.loss_cls: 1.1630  decode.d3.loss_mask: 0.3915  decode.d3.loss_dice: 0.4668  decode.d4.loss_cls: 1.1955  decode.d4.loss_mask: 0.3848  decode.d4.loss_dice: 0.5046  decode.d5.loss_cls: 1.2646  decode.d5.loss_mask: 0.4008  decode.d5.loss_dice: 0.5186  decode.d6.loss_cls: 1.3257  decode.d6.loss_mask: 0.3272  decode.d6.loss_dice: 0.5141  decode.d7.loss_cls: 1.3627  decode.d7.loss_mask: 0.3571  decode.d7.loss_dice: 0.4929  decode.d8.loss_cls: 1.3489  decode.d8.loss_mask: 0.3121  decode.d8.loss_dice: 0.4597
08/06 02:29:36 - mmengine - INFO - Iter(train) [  1750/320000]  base_lr: 9.9508e-05 lr: 9.9508e-06  eta: 1 day, 14:54:39  time: 0.4358  data_time: 0.0090  memory: 5242  grad_norm: 156.8919  loss: 27.2584  decode.loss_cls: 1.6264  decode.loss_mask: 0.4367  decode.loss_dice: 0.5396  decode.d0.loss_cls: 3.1577  decode.d0.loss_mask: 0.4643  decode.d0.loss_dice: 0.6698  decode.d1.loss_cls: 1.6328  decode.d1.loss_mask: 0.4396  decode.d1.loss_dice: 0.5095  decode.d2.loss_cls: 1.5660  decode.d2.loss_mask: 0.4231  decode.d2.loss_dice: 0.5212  decode.d3.loss_cls: 1.5460  decode.d3.loss_mask: 0.4035  decode.d3.loss_dice: 0.4886  decode.d4.loss_cls: 1.5846  decode.d4.loss_mask: 0.4243  decode.d4.loss_dice: 0.5068  decode.d5.loss_cls: 1.6429  decode.d5.loss_mask: 0.4141  decode.d5.loss_dice: 0.5223  decode.d6.loss_cls: 1.6752  decode.d6.loss_mask: 0.4232  decode.d6.loss_dice: 0.5112  decode.d7.loss_cls: 1.6419  decode.d7.loss_mask: 0.4512  decode.d7.loss_dice: 0.4972  decode.d8.loss_cls: 1.5933  decode.d8.loss_mask: 0.4201  decode.d8.loss_dice: 0.5251
08/06 02:29:58 - mmengine - INFO - Iter(train) [  1800/320000]  base_lr: 9.9494e-05 lr: 9.9494e-06  eta: 1 day, 14:53:38  time: 0.4358  data_time: 0.0089  memory: 5260  grad_norm: 152.9022  loss: 23.6255  decode.loss_cls: 1.4974  decode.loss_mask: 0.3267  decode.loss_dice: 0.4011  decode.d0.loss_cls: 3.1079  decode.d0.loss_mask: 0.3855  decode.d0.loss_dice: 0.6084  decode.d1.loss_cls: 1.6089  decode.d1.loss_mask: 0.3401  decode.d1.loss_dice: 0.4528  decode.d2.loss_cls: 1.4784  decode.d2.loss_mask: 0.2843  decode.d2.loss_dice: 0.3833  decode.d3.loss_cls: 1.4296  decode.d3.loss_mask: 0.2996  decode.d3.loss_dice: 0.3931  decode.d4.loss_cls: 1.3129  decode.d4.loss_mask: 0.3022  decode.d4.loss_dice: 0.3826  decode.d5.loss_cls: 1.3752  decode.d5.loss_mask: 0.2993  decode.d5.loss_dice: 0.4111  decode.d6.loss_cls: 1.4460  decode.d6.loss_mask: 0.3194  decode.d6.loss_dice: 0.4068  decode.d7.loss_cls: 1.4473  decode.d7.loss_mask: 0.3282  decode.d7.loss_dice: 0.4236  decode.d8.loss_cls: 1.4021  decode.d8.loss_mask: 0.3535  decode.d8.loss_dice: 0.4183
08/06 02:30:20 - mmengine - INFO - Iter(train) [  1850/320000]  base_lr: 9.9480e-05 lr: 9.9480e-06  eta: 1 day, 14:52:37  time: 0.4353  data_time: 0.0089  memory: 5224  grad_norm: 219.3753  loss: 24.2578  decode.loss_cls: 1.3966  decode.loss_mask: 0.4035  decode.loss_dice: 0.4503  decode.d0.loss_cls: 3.0592  decode.d0.loss_mask: 0.4544  decode.d0.loss_dice: 0.5761  decode.d1.loss_cls: 1.6927  decode.d1.loss_mask: 0.3836  decode.d1.loss_dice: 0.4105  decode.d2.loss_cls: 1.4532  decode.d2.loss_mask: 0.3783  decode.d2.loss_dice: 0.3921  decode.d3.loss_cls: 1.3925  decode.d3.loss_mask: 0.3681  decode.d3.loss_dice: 0.3905  decode.d4.loss_cls: 1.4512  decode.d4.loss_mask: 0.3820  decode.d4.loss_dice: 0.3778  decode.d5.loss_cls: 1.3509  decode.d5.loss_mask: 0.4026  decode.d5.loss_dice: 0.4059  decode.d6.loss_cls: 1.4474  decode.d6.loss_mask: 0.3925  decode.d6.loss_dice: 0.4110  decode.d7.loss_cls: 1.4388  decode.d7.loss_mask: 0.3687  decode.d7.loss_dice: 0.3804  decode.d8.loss_cls: 1.4634  decode.d8.loss_mask: 0.3824  decode.d8.loss_dice: 0.4010
08/06 02:30:41 - mmengine - INFO - Iter(train) [  1900/320000]  base_lr: 9.9466e-05 lr: 9.9466e-06  eta: 1 day, 14:51:41  time: 0.4360  data_time: 0.0090  memory: 5260  grad_norm: 173.0859  loss: 23.1160  decode.loss_cls: 1.1683  decode.loss_mask: 0.4693  decode.loss_dice: 0.4571  decode.d0.loss_cls: 2.6841  decode.d0.loss_mask: 0.4419  decode.d0.loss_dice: 0.5593  decode.d1.loss_cls: 1.4930  decode.d1.loss_mask: 0.4377  decode.d1.loss_dice: 0.4197  decode.d2.loss_cls: 1.3663  decode.d2.loss_mask: 0.4490  decode.d2.loss_dice: 0.4369  decode.d3.loss_cls: 1.3343  decode.d3.loss_mask: 0.4377  decode.d3.loss_dice: 0.4369  decode.d4.loss_cls: 1.3283  decode.d4.loss_mask: 0.4253  decode.d4.loss_dice: 0.3932  decode.d5.loss_cls: 1.3218  decode.d5.loss_mask: 0.4395  decode.d5.loss_dice: 0.4187  decode.d6.loss_cls: 1.2114  decode.d6.loss_mask: 0.4159  decode.d6.loss_dice: 0.4419  decode.d7.loss_cls: 1.1946  decode.d7.loss_mask: 0.4357  decode.d7.loss_dice: 0.4525  decode.d8.loss_cls: 1.2010  decode.d8.loss_mask: 0.4235  decode.d8.loss_dice: 0.4213
08/06 02:31:03 - mmengine - INFO - Iter(train) [  1950/320000]  base_lr: 9.9452e-05 lr: 9.9452e-06  eta: 1 day, 14:50:44  time: 0.4356  data_time: 0.0090  memory: 5240  grad_norm: 172.7655  loss: 26.1480  decode.loss_cls: 1.4890  decode.loss_mask: 0.4568  decode.loss_dice: 0.4700  decode.d0.loss_cls: 2.8524  decode.d0.loss_mask: 0.5422  decode.d0.loss_dice: 0.6099  decode.d1.loss_cls: 1.7148  decode.d1.loss_mask: 0.4238  decode.d1.loss_dice: 0.4772  decode.d2.loss_cls: 1.5659  decode.d2.loss_mask: 0.4517  decode.d2.loss_dice: 0.4688  decode.d3.loss_cls: 1.4949  decode.d3.loss_mask: 0.4631  decode.d3.loss_dice: 0.4715  decode.d4.loss_cls: 1.4560  decode.d4.loss_mask: 0.4400  decode.d4.loss_dice: 0.4744  decode.d5.loss_cls: 1.5675  decode.d5.loss_mask: 0.4692  decode.d5.loss_dice: 0.4481  decode.d6.loss_cls: 1.5785  decode.d6.loss_mask: 0.4230  decode.d6.loss_dice: 0.4514  decode.d7.loss_cls: 1.4829  decode.d7.loss_mask: 0.4907  decode.d7.loss_dice: 0.4718  decode.d8.loss_cls: 1.5318  decode.d8.loss_mask: 0.4530  decode.d8.loss_dice: 0.4576
08/06 02:31:25 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 02:31:25 - mmengine - INFO - Iter(train) [  2000/320000]  base_lr: 9.9438e-05 lr: 9.9438e-06  eta: 1 day, 14:49:46  time: 0.4349  data_time: 0.0088  memory: 5223  grad_norm: 126.0393  loss: 23.5812  decode.loss_cls: 1.4557  decode.loss_mask: 0.3434  decode.loss_dice: 0.4272  decode.d0.loss_cls: 2.7079  decode.d0.loss_mask: 0.3815  decode.d0.loss_dice: 0.5613  decode.d1.loss_cls: 1.5905  decode.d1.loss_mask: 0.3517  decode.d1.loss_dice: 0.4153  decode.d2.loss_cls: 1.4138  decode.d2.loss_mask: 0.3479  decode.d2.loss_dice: 0.4244  decode.d3.loss_cls: 1.3318  decode.d3.loss_mask: 0.3708  decode.d3.loss_dice: 0.4222  decode.d4.loss_cls: 1.3563  decode.d4.loss_mask: 0.3673  decode.d4.loss_dice: 0.4412  decode.d5.loss_cls: 1.4282  decode.d5.loss_mask: 0.3511  decode.d5.loss_dice: 0.4356  decode.d6.loss_cls: 1.4085  decode.d6.loss_mask: 0.3544  decode.d6.loss_dice: 0.4340  decode.d7.loss_cls: 1.4660  decode.d7.loss_mask: 0.3686  decode.d7.loss_dice: 0.4610  decode.d8.loss_cls: 1.3713  decode.d8.loss_mask: 0.3514  decode.d8.loss_dice: 0.4408
08/06 02:31:47 - mmengine - INFO - Iter(train) [  2050/320000]  base_lr: 9.9424e-05 lr: 9.9424e-06  eta: 1 day, 14:48:57  time: 0.4361  data_time: 0.0091  memory: 5258  grad_norm: 210.9326  loss: 21.1251  decode.loss_cls: 1.0822  decode.loss_mask: 0.4054  decode.loss_dice: 0.4070  decode.d0.loss_cls: 2.4422  decode.d0.loss_mask: 0.4512  decode.d0.loss_dice: 0.5226  decode.d1.loss_cls: 1.3593  decode.d1.loss_mask: 0.3877  decode.d1.loss_dice: 0.4335  decode.d2.loss_cls: 1.1684  decode.d2.loss_mask: 0.3890  decode.d2.loss_dice: 0.4093  decode.d3.loss_cls: 1.1507  decode.d3.loss_mask: 0.3709  decode.d3.loss_dice: 0.3776  decode.d4.loss_cls: 1.1850  decode.d4.loss_mask: 0.3715  decode.d4.loss_dice: 0.3738  decode.d5.loss_cls: 1.1379  decode.d5.loss_mask: 0.3873  decode.d5.loss_dice: 0.3585  decode.d6.loss_cls: 1.2057  decode.d6.loss_mask: 0.3999  decode.d6.loss_dice: 0.3887  decode.d7.loss_cls: 1.2124  decode.d7.loss_mask: 0.4214  decode.d7.loss_dice: 0.3936  decode.d8.loss_cls: 1.1166  decode.d8.loss_mask: 0.4144  decode.d8.loss_dice: 0.4013
08/06 02:32:09 - mmengine - INFO - Iter(train) [  2100/320000]  base_lr: 9.9409e-05 lr: 9.9409e-06  eta: 1 day, 14:48:14  time: 0.4356  data_time: 0.0089  memory: 5240  grad_norm: 182.1534  loss: 22.5305  decode.loss_cls: 1.1525  decode.loss_mask: 0.4184  decode.loss_dice: 0.4258  decode.d0.loss_cls: 2.5105  decode.d0.loss_mask: 0.4944  decode.d0.loss_dice: 0.5792  decode.d1.loss_cls: 1.4849  decode.d1.loss_mask: 0.4561  decode.d1.loss_dice: 0.4465  decode.d2.loss_cls: 1.2349  decode.d2.loss_mask: 0.4698  decode.d2.loss_dice: 0.4182  decode.d3.loss_cls: 1.2712  decode.d3.loss_mask: 0.4176  decode.d3.loss_dice: 0.3970  decode.d4.loss_cls: 1.2123  decode.d4.loss_mask: 0.4746  decode.d4.loss_dice: 0.3893  decode.d5.loss_cls: 1.2637  decode.d5.loss_mask: 0.4209  decode.d5.loss_dice: 0.4173  decode.d6.loss_cls: 1.2311  decode.d6.loss_mask: 0.4295  decode.d6.loss_dice: 0.4168  decode.d7.loss_cls: 1.2005  decode.d7.loss_mask: 0.4327  decode.d7.loss_dice: 0.4131  decode.d8.loss_cls: 1.2067  decode.d8.loss_mask: 0.4363  decode.d8.loss_dice: 0.4090
08/06 02:32:30 - mmengine - INFO - Iter(train) [  2150/320000]  base_lr: 9.9395e-05 lr: 9.9395e-06  eta: 1 day, 14:47:23  time: 0.4355  data_time: 0.0090  memory: 5299  grad_norm: 166.4910  loss: 22.3042  decode.loss_cls: 1.0569  decode.loss_mask: 0.5080  decode.loss_dice: 0.4419  decode.d0.loss_cls: 2.4363  decode.d0.loss_mask: 0.5082  decode.d0.loss_dice: 0.5825  decode.d1.loss_cls: 1.2966  decode.d1.loss_mask: 0.4715  decode.d1.loss_dice: 0.5008  decode.d2.loss_cls: 1.1783  decode.d2.loss_mask: 0.5017  decode.d2.loss_dice: 0.4793  decode.d3.loss_cls: 1.1673  decode.d3.loss_mask: 0.5115  decode.d3.loss_dice: 0.4619  decode.d4.loss_cls: 1.0772  decode.d4.loss_mask: 0.4896  decode.d4.loss_dice: 0.4805  decode.d5.loss_cls: 1.0382  decode.d5.loss_mask: 0.5036  decode.d5.loss_dice: 0.4502  decode.d6.loss_cls: 1.0231  decode.d6.loss_mask: 0.5051  decode.d6.loss_dice: 0.5034  decode.d7.loss_cls: 1.0368  decode.d7.loss_mask: 0.5376  decode.d7.loss_dice: 0.5093  decode.d8.loss_cls: 1.0160  decode.d8.loss_mask: 0.5423  decode.d8.loss_dice: 0.4887
08/06 02:32:52 - mmengine - INFO - Iter(train) [  2200/320000]  base_lr: 9.9381e-05 lr: 9.9381e-06  eta: 1 day, 14:46:31  time: 0.4357  data_time: 0.0090  memory: 5275  grad_norm: 140.1581  loss: 25.4138  decode.loss_cls: 1.4729  decode.loss_mask: 0.3682  decode.loss_dice: 0.5064  decode.d0.loss_cls: 2.6254  decode.d0.loss_mask: 0.4327  decode.d0.loss_dice: 0.6544  decode.d1.loss_cls: 1.7489  decode.d1.loss_mask: 0.3615  decode.d1.loss_dice: 0.5311  decode.d2.loss_cls: 1.5602  decode.d2.loss_mask: 0.3693  decode.d2.loss_dice: 0.4963  decode.d3.loss_cls: 1.5015  decode.d3.loss_mask: 0.3425  decode.d3.loss_dice: 0.4777  decode.d4.loss_cls: 1.5239  decode.d4.loss_mask: 0.3376  decode.d4.loss_dice: 0.4867  decode.d5.loss_cls: 1.5090  decode.d5.loss_mask: 0.3497  decode.d5.loss_dice: 0.5055  decode.d6.loss_cls: 1.5219  decode.d6.loss_mask: 0.3613  decode.d6.loss_dice: 0.4905  decode.d7.loss_cls: 1.6030  decode.d7.loss_mask: 0.3766  decode.d7.loss_dice: 0.5062  decode.d8.loss_cls: 1.5198  decode.d8.loss_mask: 0.3655  decode.d8.loss_dice: 0.5077
08/06 02:33:14 - mmengine - INFO - Iter(train) [  2250/320000]  base_lr: 9.9367e-05 lr: 9.9367e-06  eta: 1 day, 14:45:42  time: 0.4354  data_time: 0.0089  memory: 5260  grad_norm: 122.7294  loss: 19.0695  decode.loss_cls: 1.1461  decode.loss_mask: 0.2784  decode.loss_dice: 0.3121  decode.d0.loss_cls: 2.3268  decode.d0.loss_mask: 0.3377  decode.d0.loss_dice: 0.4608  decode.d1.loss_cls: 1.3620  decode.d1.loss_mask: 0.2909  decode.d1.loss_dice: 0.3332  decode.d2.loss_cls: 1.2111  decode.d2.loss_mask: 0.2812  decode.d2.loss_dice: 0.3283  decode.d3.loss_cls: 1.1344  decode.d3.loss_mask: 0.2869  decode.d3.loss_dice: 0.3218  decode.d4.loss_cls: 1.0734  decode.d4.loss_mask: 0.2849  decode.d4.loss_dice: 0.3288  decode.d5.loss_cls: 1.0932  decode.d5.loss_mask: 0.2855  decode.d5.loss_dice: 0.3395  decode.d6.loss_cls: 1.1346  decode.d6.loss_mask: 0.2866  decode.d6.loss_dice: 0.3241  decode.d7.loss_cls: 1.1648  decode.d7.loss_mask: 0.2835  decode.d7.loss_dice: 0.3274  decode.d8.loss_cls: 1.1257  decode.d8.loss_mask: 0.2840  decode.d8.loss_dice: 0.3214
08/06 02:33:36 - mmengine - INFO - Iter(train) [  2300/320000]  base_lr: 9.9353e-05 lr: 9.9353e-06  eta: 1 day, 14:44:53  time: 0.4354  data_time: 0.0088  memory: 5240  grad_norm: 155.3742  loss: 21.1746  decode.loss_cls: 1.2444  decode.loss_mask: 0.3390  decode.loss_dice: 0.4658  decode.d0.loss_cls: 2.2086  decode.d0.loss_mask: 0.3200  decode.d0.loss_dice: 0.5299  decode.d1.loss_cls: 1.2963  decode.d1.loss_mask: 0.3407  decode.d1.loss_dice: 0.4728  decode.d2.loss_cls: 1.1280  decode.d2.loss_mask: 0.3428  decode.d2.loss_dice: 0.4726  decode.d3.loss_cls: 1.1544  decode.d3.loss_mask: 0.3385  decode.d3.loss_dice: 0.4349  decode.d4.loss_cls: 1.2153  decode.d4.loss_mask: 0.3261  decode.d4.loss_dice: 0.4359  decode.d5.loss_cls: 1.1637  decode.d5.loss_mask: 0.3304  decode.d5.loss_dice: 0.4652  decode.d6.loss_cls: 1.2543  decode.d6.loss_mask: 0.3298  decode.d6.loss_dice: 0.4712  decode.d7.loss_cls: 1.2487  decode.d7.loss_mask: 0.3372  decode.d7.loss_dice: 0.5234  decode.d8.loss_cls: 1.1656  decode.d8.loss_mask: 0.3396  decode.d8.loss_dice: 0.4793
08/06 02:33:57 - mmengine - INFO - Iter(train) [  2350/320000]  base_lr: 9.9339e-05 lr: 9.9339e-06  eta: 1 day, 14:44:08  time: 0.4360  data_time: 0.0092  memory: 5297  grad_norm: 183.5764  loss: 21.9009  decode.loss_cls: 1.2410  decode.loss_mask: 0.4047  decode.loss_dice: 0.4130  decode.d0.loss_cls: 2.2616  decode.d0.loss_mask: 0.4579  decode.d0.loss_dice: 0.5037  decode.d1.loss_cls: 1.3717  decode.d1.loss_mask: 0.3900  decode.d1.loss_dice: 0.4081  decode.d2.loss_cls: 1.2041  decode.d2.loss_mask: 0.3718  decode.d2.loss_dice: 0.3901  decode.d3.loss_cls: 1.2220  decode.d3.loss_mask: 0.3618  decode.d3.loss_dice: 0.3976  decode.d4.loss_cls: 1.2511  decode.d4.loss_mask: 0.3746  decode.d4.loss_dice: 0.4188  decode.d5.loss_cls: 1.2659  decode.d5.loss_mask: 0.3886  decode.d5.loss_dice: 0.4090  decode.d6.loss_cls: 1.3054  decode.d6.loss_mask: 0.3924  decode.d6.loss_dice: 0.4277  decode.d7.loss_cls: 1.3520  decode.d7.loss_mask: 0.3838  decode.d7.loss_dice: 0.4345  decode.d8.loss_cls: 1.2682  decode.d8.loss_mask: 0.3915  decode.d8.loss_dice: 0.4386
08/06 02:34:19 - mmengine - INFO - Iter(train) [  2400/320000]  base_lr: 9.9325e-05 lr: 9.9325e-06  eta: 1 day, 14:43:21  time: 0.4357  data_time: 0.0089  memory: 5275  grad_norm: 260.2106  loss: 24.4188  decode.loss_cls: 1.3201  decode.loss_mask: 0.4644  decode.loss_dice: 0.4121  decode.d0.loss_cls: 2.3247  decode.d0.loss_mask: 0.3959  decode.d0.loss_dice: 0.5258  decode.d1.loss_cls: 1.4181  decode.d1.loss_mask: 0.5202  decode.d1.loss_dice: 0.4145  decode.d2.loss_cls: 1.3320  decode.d2.loss_mask: 0.5001  decode.d2.loss_dice: 0.4422  decode.d3.loss_cls: 1.4642  decode.d3.loss_mask: 0.4998  decode.d3.loss_dice: 0.4058  decode.d4.loss_cls: 1.4369  decode.d4.loss_mask: 0.5351  decode.d4.loss_dice: 0.4488  decode.d5.loss_cls: 1.4270  decode.d5.loss_mask: 0.4899  decode.d5.loss_dice: 0.4250  decode.d6.loss_cls: 1.4145  decode.d6.loss_mask: 0.5490  decode.d6.loss_dice: 0.4447  decode.d7.loss_cls: 1.3592  decode.d7.loss_mask: 0.5599  decode.d7.loss_dice: 0.4458  decode.d8.loss_cls: 1.3556  decode.d8.loss_mask: 0.6074  decode.d8.loss_dice: 0.4801
08/06 02:34:41 - mmengine - INFO - Iter(train) [  2450/320000]  base_lr: 9.9311e-05 lr: 9.9311e-06  eta: 1 day, 14:42:37  time: 0.4362  data_time: 0.0089  memory: 5242  grad_norm: 158.0795  loss: 24.6269  decode.loss_cls: 1.4720  decode.loss_mask: 0.3563  decode.loss_dice: 0.4939  decode.d0.loss_cls: 2.3517  decode.d0.loss_mask: 0.3390  decode.d0.loss_dice: 0.6079  decode.d1.loss_cls: 1.5763  decode.d1.loss_mask: 0.3365  decode.d1.loss_dice: 0.5172  decode.d2.loss_cls: 1.4425  decode.d2.loss_mask: 0.3457  decode.d2.loss_dice: 0.5001  decode.d3.loss_cls: 1.4627  decode.d3.loss_mask: 0.3546  decode.d3.loss_dice: 0.5066  decode.d4.loss_cls: 1.4762  decode.d4.loss_mask: 0.3684  decode.d4.loss_dice: 0.5321  decode.d5.loss_cls: 1.5305  decode.d5.loss_mask: 0.3790  decode.d5.loss_dice: 0.5462  decode.d6.loss_cls: 1.4951  decode.d6.loss_mask: 0.3522  decode.d6.loss_dice: 0.5330  decode.d7.loss_cls: 1.5280  decode.d7.loss_mask: 0.3542  decode.d7.loss_dice: 0.5415  decode.d8.loss_cls: 1.4610  decode.d8.loss_mask: 0.3589  decode.d8.loss_dice: 0.5077
08/06 02:35:03 - mmengine - INFO - Iter(train) [  2500/320000]  base_lr: 9.9297e-05 lr: 9.9297e-06  eta: 1 day, 14:41:55  time: 0.4356  data_time: 0.0089  memory: 5260  grad_norm: 281.0882  loss: 21.9669  decode.loss_cls: 1.2384  decode.loss_mask: 0.4346  decode.loss_dice: 0.4393  decode.d0.loss_cls: 2.1506  decode.d0.loss_mask: 0.4667  decode.d0.loss_dice: 0.5169  decode.d1.loss_cls: 1.4068  decode.d1.loss_mask: 0.4251  decode.d1.loss_dice: 0.3957  decode.d2.loss_cls: 1.2361  decode.d2.loss_mask: 0.3786  decode.d2.loss_dice: 0.4001  decode.d3.loss_cls: 1.2841  decode.d3.loss_mask: 0.3796  decode.d3.loss_dice: 0.3928  decode.d4.loss_cls: 1.2384  decode.d4.loss_mask: 0.3814  decode.d4.loss_dice: 0.3874  decode.d5.loss_cls: 1.2425  decode.d5.loss_mask: 0.3921  decode.d5.loss_dice: 0.4154  decode.d6.loss_cls: 1.2695  decode.d6.loss_mask: 0.3806  decode.d6.loss_dice: 0.4165  decode.d7.loss_cls: 1.3035  decode.d7.loss_mask: 0.4062  decode.d7.loss_dice: 0.4539  decode.d8.loss_cls: 1.2979  decode.d8.loss_mask: 0.3858  decode.d8.loss_dice: 0.4502
08/06 02:35:25 - mmengine - INFO - Iter(train) [  2550/320000]  base_lr: 9.9283e-05 lr: 9.9283e-06  eta: 1 day, 14:41:14  time: 0.4362  data_time: 0.0089  memory: 5242  grad_norm: 157.4707  loss: 21.2082  decode.loss_cls: 1.1525  decode.loss_mask: 0.2923  decode.loss_dice: 0.4752  decode.d0.loss_cls: 2.1429  decode.d0.loss_mask: 0.2847  decode.d0.loss_dice: 0.5587  decode.d1.loss_cls: 1.3764  decode.d1.loss_mask: 0.2872  decode.d1.loss_dice: 0.4898  decode.d2.loss_cls: 1.2304  decode.d2.loss_mask: 0.2905  decode.d2.loss_dice: 0.5036  decode.d3.loss_cls: 1.2096  decode.d3.loss_mask: 0.3036  decode.d3.loss_dice: 0.5081  decode.d4.loss_cls: 1.1765  decode.d4.loss_mask: 0.3122  decode.d4.loss_dice: 0.5277  decode.d5.loss_cls: 1.2033  decode.d5.loss_mask: 0.3071  decode.d5.loss_dice: 0.5127  decode.d6.loss_cls: 1.2030  decode.d6.loss_mask: 0.3139  decode.d6.loss_dice: 0.5169  decode.d7.loss_cls: 1.1849  decode.d7.loss_mask: 0.3032  decode.d7.loss_dice: 0.5228  decode.d8.loss_cls: 1.1489  decode.d8.loss_mask: 0.3120  decode.d8.loss_dice: 0.5577
08/06 02:35:46 - mmengine - INFO - Iter(train) [  2600/320000]  base_lr: 9.9269e-05 lr: 9.9269e-06  eta: 1 day, 14:40:33  time: 0.4366  data_time: 0.0089  memory: 5242  grad_norm: 125.4297  loss: 18.9443  decode.loss_cls: 1.1110  decode.loss_mask: 0.2927  decode.loss_dice: 0.3555  decode.d0.loss_cls: 2.1152  decode.d0.loss_mask: 0.3541  decode.d0.loss_dice: 0.5275  decode.d1.loss_cls: 1.2355  decode.d1.loss_mask: 0.3399  decode.d1.loss_dice: 0.4114  decode.d2.loss_cls: 1.0570  decode.d2.loss_mask: 0.3210  decode.d2.loss_dice: 0.4088  decode.d3.loss_cls: 0.9708  decode.d3.loss_mask: 0.2955  decode.d3.loss_dice: 0.4008  decode.d4.loss_cls: 1.0488  decode.d4.loss_mask: 0.3064  decode.d4.loss_dice: 0.3902  decode.d5.loss_cls: 1.0891  decode.d5.loss_mask: 0.3118  decode.d5.loss_dice: 0.3789  decode.d6.loss_cls: 1.0222  decode.d6.loss_mask: 0.3118  decode.d6.loss_dice: 0.3640  decode.d7.loss_cls: 1.1297  decode.d7.loss_mask: 0.3045  decode.d7.loss_dice: 0.3518  decode.d8.loss_cls: 1.0651  decode.d8.loss_mask: 0.3148  decode.d8.loss_dice: 0.3584
08/06 02:36:08 - mmengine - INFO - Iter(train) [  2650/320000]  base_lr: 9.9255e-05 lr: 9.9255e-06  eta: 1 day, 14:39:54  time: 0.4353  data_time: 0.0089  memory: 5260  grad_norm: 151.8859  loss: 22.0297  decode.loss_cls: 1.2915  decode.loss_mask: 0.3573  decode.loss_dice: 0.3747  decode.d0.loss_cls: 2.2519  decode.d0.loss_mask: 0.4395  decode.d0.loss_dice: 0.4968  decode.d1.loss_cls: 1.5447  decode.d1.loss_mask: 0.3786  decode.d1.loss_dice: 0.4233  decode.d2.loss_cls: 1.2962  decode.d2.loss_mask: 0.3871  decode.d2.loss_dice: 0.4292  decode.d3.loss_cls: 1.2463  decode.d3.loss_mask: 0.3975  decode.d3.loss_dice: 0.4222  decode.d4.loss_cls: 1.2441  decode.d4.loss_mask: 0.3911  decode.d4.loss_dice: 0.4147  decode.d5.loss_cls: 1.2919  decode.d5.loss_mask: 0.3894  decode.d5.loss_dice: 0.4203  decode.d6.loss_cls: 1.2768  decode.d6.loss_mask: 0.3538  decode.d6.loss_dice: 0.3893  decode.d7.loss_cls: 1.2610  decode.d7.loss_mask: 0.3709  decode.d7.loss_dice: 0.3820  decode.d8.loss_cls: 1.2900  decode.d8.loss_mask: 0.3962  decode.d8.loss_dice: 0.4214
08/06 02:36:30 - mmengine - INFO - Iter(train) [  2700/320000]  base_lr: 9.9241e-05 lr: 9.9241e-06  eta: 1 day, 14:39:17  time: 0.4366  data_time: 0.0090  memory: 5275  grad_norm: 189.0981  loss: 18.1749  decode.loss_cls: 1.0457  decode.loss_mask: 0.3248  decode.loss_dice: 0.3329  decode.d0.loss_cls: 1.9282  decode.d0.loss_mask: 0.3579  decode.d0.loss_dice: 0.4176  decode.d1.loss_cls: 1.1914  decode.d1.loss_mask: 0.3450  decode.d1.loss_dice: 0.3578  decode.d2.loss_cls: 1.0182  decode.d2.loss_mask: 0.3323  decode.d2.loss_dice: 0.3299  decode.d3.loss_cls: 1.0435  decode.d3.loss_mask: 0.3449  decode.d3.loss_dice: 0.3328  decode.d4.loss_cls: 1.0351  decode.d4.loss_mask: 0.3698  decode.d4.loss_dice: 0.3473  decode.d5.loss_cls: 1.0136  decode.d5.loss_mask: 0.3648  decode.d5.loss_dice: 0.3316  decode.d6.loss_cls: 0.9805  decode.d6.loss_mask: 0.3388  decode.d6.loss_dice: 0.3419  decode.d7.loss_cls: 1.0014  decode.d7.loss_mask: 0.3343  decode.d7.loss_dice: 0.3579  decode.d8.loss_cls: 1.0057  decode.d8.loss_mask: 0.3274  decode.d8.loss_dice: 0.3219
08/06 02:36:52 - mmengine - INFO - Iter(train) [  2750/320000]  base_lr: 9.9227e-05 lr: 9.9227e-06  eta: 1 day, 14:38:42  time: 0.4363  data_time: 0.0092  memory: 5223  grad_norm: 147.9829  loss: 18.5319  decode.loss_cls: 0.9088  decode.loss_mask: 0.3926  decode.loss_dice: 0.4299  decode.d0.loss_cls: 1.8522  decode.d0.loss_mask: 0.4159  decode.d0.loss_dice: 0.5510  decode.d1.loss_cls: 1.0959  decode.d1.loss_mask: 0.3824  decode.d1.loss_dice: 0.4378  decode.d2.loss_cls: 0.8918  decode.d2.loss_mask: 0.3907  decode.d2.loss_dice: 0.4323  decode.d3.loss_cls: 0.9601  decode.d3.loss_mask: 0.3645  decode.d3.loss_dice: 0.3854  decode.d4.loss_cls: 0.9520  decode.d4.loss_mask: 0.3783  decode.d4.loss_dice: 0.3956  decode.d5.loss_cls: 0.9695  decode.d5.loss_mask: 0.3727  decode.d5.loss_dice: 0.4073  decode.d6.loss_cls: 0.8987  decode.d6.loss_mask: 0.3904  decode.d6.loss_dice: 0.4184  decode.d7.loss_cls: 0.9055  decode.d7.loss_mask: 0.3927  decode.d7.loss_dice: 0.4469  decode.d8.loss_cls: 0.9150  decode.d8.loss_mask: 0.3839  decode.d8.loss_dice: 0.4137
08/06 02:37:14 - mmengine - INFO - Iter(train) [  2800/320000]  base_lr: 9.9212e-05 lr: 9.9212e-06  eta: 1 day, 14:38:08  time: 0.4377  data_time: 0.0089  memory: 5236  grad_norm: 122.5971  loss: 17.8250  decode.loss_cls: 0.8552  decode.loss_mask: 0.3644  decode.loss_dice: 0.4114  decode.d0.loss_cls: 1.6876  decode.d0.loss_mask: 0.3743  decode.d0.loss_dice: 0.5150  decode.d1.loss_cls: 1.1714  decode.d1.loss_mask: 0.3492  decode.d1.loss_dice: 0.4529  decode.d2.loss_cls: 0.9354  decode.d2.loss_mask: 0.3619  decode.d2.loss_dice: 0.4634  decode.d3.loss_cls: 0.9527  decode.d3.loss_mask: 0.3414  decode.d3.loss_dice: 0.4113  decode.d4.loss_cls: 0.8303  decode.d4.loss_mask: 0.3518  decode.d4.loss_dice: 0.4243  decode.d5.loss_cls: 0.8469  decode.d5.loss_mask: 0.3529  decode.d5.loss_dice: 0.4233  decode.d6.loss_cls: 0.8865  decode.d6.loss_mask: 0.3419  decode.d6.loss_dice: 0.4004  decode.d7.loss_cls: 0.8998  decode.d7.loss_mask: 0.3508  decode.d7.loss_dice: 0.4107  decode.d8.loss_cls: 0.8898  decode.d8.loss_mask: 0.3576  decode.d8.loss_dice: 0.4103
08/06 02:37:35 - mmengine - INFO - Iter(train) [  2850/320000]  base_lr: 9.9198e-05 lr: 9.9198e-06  eta: 1 day, 14:37:36  time: 0.4369  data_time: 0.0090  memory: 5242  grad_norm: 209.3316  loss: 20.9000  decode.loss_cls: 1.0973  decode.loss_mask: 0.3984  decode.loss_dice: 0.4507  decode.d0.loss_cls: 2.0622  decode.d0.loss_mask: 0.4006  decode.d0.loss_dice: 0.5682  decode.d1.loss_cls: 1.3783  decode.d1.loss_mask: 0.3869  decode.d1.loss_dice: 0.4348  decode.d2.loss_cls: 1.1332  decode.d2.loss_mask: 0.3761  decode.d2.loss_dice: 0.4525  decode.d3.loss_cls: 1.1664  decode.d3.loss_mask: 0.3876  decode.d3.loss_dice: 0.4585  decode.d4.loss_cls: 1.1214  decode.d4.loss_mask: 0.3684  decode.d4.loss_dice: 0.4369  decode.d5.loss_cls: 1.1168  decode.d5.loss_mask: 0.3861  decode.d5.loss_dice: 0.4594  decode.d6.loss_cls: 1.0811  decode.d6.loss_mask: 0.4108  decode.d6.loss_dice: 0.4638  decode.d7.loss_cls: 1.0888  decode.d7.loss_mask: 0.3875  decode.d7.loss_dice: 0.4501  decode.d8.loss_cls: 1.1461  decode.d8.loss_mask: 0.3854  decode.d8.loss_dice: 0.4455
08/06 02:37:57 - mmengine - INFO - Iter(train) [  2900/320000]  base_lr: 9.9184e-05 lr: 9.9184e-06  eta: 1 day, 14:37:27  time: 0.4362  data_time: 0.0090  memory: 5242  grad_norm: 266.7872  loss: 20.7397  decode.loss_cls: 1.0174  decode.loss_mask: 0.4962  decode.loss_dice: 0.4798  decode.d0.loss_cls: 1.9344  decode.d0.loss_mask: 0.5107  decode.d0.loss_dice: 0.5749  decode.d1.loss_cls: 1.2550  decode.d1.loss_mask: 0.4615  decode.d1.loss_dice: 0.4824  decode.d2.loss_cls: 1.0421  decode.d2.loss_mask: 0.4850  decode.d2.loss_dice: 0.4584  decode.d3.loss_cls: 1.0251  decode.d3.loss_mask: 0.4773  decode.d3.loss_dice: 0.4563  decode.d4.loss_cls: 0.9793  decode.d4.loss_mask: 0.4862  decode.d4.loss_dice: 0.4662  decode.d5.loss_cls: 1.0014  decode.d5.loss_mask: 0.4524  decode.d5.loss_dice: 0.4689  decode.d6.loss_cls: 0.9406  decode.d6.loss_mask: 0.4530  decode.d6.loss_dice: 0.4870  decode.d7.loss_cls: 1.0229  decode.d7.loss_mask: 0.4412  decode.d7.loss_dice: 0.4706  decode.d8.loss_cls: 0.9597  decode.d8.loss_mask: 0.4907  decode.d8.loss_dice: 0.4634
08/06 02:38:19 - mmengine - INFO - Iter(train) [  2950/320000]  base_lr: 9.9170e-05 lr: 9.9170e-06  eta: 1 day, 14:36:57  time: 0.4365  data_time: 0.0090  memory: 5260  grad_norm: 160.3413  loss: 19.7996  decode.loss_cls: 1.0362  decode.loss_mask: 0.3749  decode.loss_dice: 0.4227  decode.d0.loss_cls: 2.0292  decode.d0.loss_mask: 0.3342  decode.d0.loss_dice: 0.5694  decode.d1.loss_cls: 1.2541  decode.d1.loss_mask: 0.3164  decode.d1.loss_dice: 0.4681  decode.d2.loss_cls: 1.0160  decode.d2.loss_mask: 0.2956  decode.d2.loss_dice: 0.4040  decode.d3.loss_cls: 1.0498  decode.d3.loss_mask: 0.3525  decode.d3.loss_dice: 0.4234  decode.d4.loss_cls: 1.0599  decode.d4.loss_mask: 0.3448  decode.d4.loss_dice: 0.4442  decode.d5.loss_cls: 1.0230  decode.d5.loss_mask: 0.4262  decode.d5.loss_dice: 0.4342  decode.d6.loss_cls: 1.0207  decode.d6.loss_mask: 0.4400  decode.d6.loss_dice: 0.4312  decode.d7.loss_cls: 1.0740  decode.d7.loss_mask: 0.4181  decode.d7.loss_dice: 0.4857  decode.d8.loss_cls: 1.0636  decode.d8.loss_mask: 0.3708  decode.d8.loss_dice: 0.4167
08/06 02:38:41 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250806_021635
08/06 02:38:41 - mmengine - INFO - Iter(train) [  3000/320000]  base_lr: 9.9156e-05 lr: 9.9156e-06  eta: 1 day, 14:36:24  time: 0.4365  data_time: 0.0087  memory: 5275  grad_norm: 153.5276  loss: 19.0304  decode.loss_cls: 1.2189  decode.loss_mask: 0.2568  decode.loss_dice: 0.3640  decode.d0.loss_cls: 2.0645  decode.d0.loss_mask: 0.3146  decode.d0.loss_dice: 0.4696  decode.d1.loss_cls: 1.4382  decode.d1.loss_mask: 0.2588  decode.d1.loss_dice: 0.3552  decode.d2.loss_cls: 1.2708  decode.d2.loss_mask: 0.2444  decode.d2.loss_dice: 0.3303  decode.d3.loss_cls: 1.1784  decode.d3.loss_mask: 0.2402  decode.d3.loss_dice: 0.3303  decode.d4.loss_cls: 1.1493  decode.d4.loss_mask: 0.2415  decode.d4.loss_dice: 0.3396  decode.d5.loss_cls: 1.1183  decode.d5.loss_mask: 0.2483  decode.d5.loss_dice: 0.3528  decode.d6.loss_cls: 1.1088  decode.d6.loss_mask: 0.2503  decode.d6.loss_dice: 0.3639  decode.d7.loss_cls: 1.1854  decode.d7.loss_mask: 0.2599  decode.d7.loss_dice: 0.3640  decode.d8.loss_cls: 1.1256  decode.d8.loss_mask: 0.2447  decode.d8.loss_dice: 0.3431
08/06 02:39:03 - mmengine - INFO - Iter(train) [  3050/320000]  base_lr: 9.9142e-05 lr: 9.9142e-06  eta: 1 day, 14:35:54  time: 0.4377  data_time: 0.0091  memory: 5260  grad_norm: 122.1484  loss: 17.0549  decode.loss_cls: 0.9014  decode.loss_mask: 0.2679  decode.loss_dice: 0.3070  decode.d0.loss_cls: 1.9468  decode.d0.loss_mask: 0.3299  decode.d0.loss_dice: 0.4435  decode.d1.loss_cls: 1.2507  decode.d1.loss_mask: 0.2733  decode.d1.loss_dice: 0.3297  decode.d2.loss_cls: 0.9934  decode.d2.loss_mask: 0.2794  decode.d2.loss_dice: 0.3397  decode.d3.loss_cls: 0.9700  decode.d3.loss_mask: 0.3122  decode.d3.loss_dice: 0.3301  decode.d4.loss_cls: 0.9773  decode.d4.loss_mask: 0.2738  decode.d4.loss_dice: 0.3163  decode.d5.loss_cls: 1.0022  decode.d5.loss_mask: 0.2707  decode.d5.loss_dice: 0.3212  decode.d6.loss_cls: 0.9059  decode.d6.loss_mask: 0.3103  decode.d6.loss_dice: 0.3404  decode.d7.loss_cls: 0.9537  decode.d7.loss_mask: 0.2768  decode.d7.loss_dice: 0.3033  decode.d8.loss_cls: 0.9371  decode.d8.loss_mask: 0.2710  decode.d8.loss_dice: 0.3200
08/06 02:39:25 - mmengine - INFO - Iter(train) [  3100/320000]  base_lr: 9.9128e-05 lr: 9.9128e-06  eta: 1 day, 14:35:23  time: 0.4365  data_time: 0.0089  memory: 5236  grad_norm: 188.8790  loss: 18.8788  decode.loss_cls: 0.9175  decode.loss_mask: 0.4236  decode.loss_dice: 0.5123  decode.d0.loss_cls: 1.7378  decode.d0.loss_mask: 0.3978  decode.d0.loss_dice: 0.5343  decode.d1.loss_cls: 1.0478  decode.d1.loss_mask: 0.4220  decode.d1.loss_dice: 0.4710  decode.d2.loss_cls: 0.8593  decode.d2.loss_mask: 0.4029  decode.d2.loss_dice: 0.4592  decode.d3.loss_cls: 0.8696  decode.d3.loss_mask: 0.4236  decode.d3.loss_dice: 0.4678  decode.d4.loss_cls: 0.8609  decode.d4.loss_mask: 0.4137  decode.d4.loss_dice: 0.4583  decode.d5.loss_cls: 0.8541  decode.d5.loss_mask: 0.4271  decode.d5.loss_dice: 0.4874  decode.d6.loss_cls: 0.8951  decode.d6.loss_mask: 0.4192  decode.d6.loss_dice: 0.4974  decode.d7.loss_cls: 0.8787  decode.d7.loss_mask: 0.4253  decode.d7.loss_dice: 0.5069  decode.d8.loss_cls: 0.8883  decode.d8.loss_mask: 0.4170  decode.d8.loss_dice: 0.5028
