==========================================
SLURM_JOB_ID = 2467971
SLURM_NODELIST = gnode070
SLURM_JOB_GPUS = 0
==========================================
07/29 22:57:09 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.23 (main, Jun  5 2025, 13:40:20) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 28830020
    GPU 0: NVIDIA GeForce RTX 2080 Ti
    CUDA_HOME: /opt/cuda-12.1/
    NVCC: Cuda compilation tools, release 12.1, V12.1.66
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.1.2
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.2
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: True
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 28830020
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

07/29 22:57:09 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=16, enable=False)
crop_size = (
    512,
    1024,
)
data_preprocessor = dict(
    bgr_to_rgb=True,
    mean=[
        123.675,
        116.28,
        103.53,
    ],
    pad_val=0,
    seg_pad_val=255,
    size=(
        512,
        1024,
    ),
    std=[
        58.395,
        57.12,
        57.375,
    ],
    test_cfg=dict(size_divisor=32),
    type='SegDataPreProcessor')
data_root = '/scratch/seg_benchmark/seg_full_redone_FINAL/'
dataset_type = 'YourDataset_BIG'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False, interval=5000, save_best='mIoU',
        type='CheckpointHook'),
    logger=dict(interval=50, log_metric_by_epoch=False, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'))
default_scope = 'mmseg'
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=True,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
img_ratios = [
    0.5,
    0.75,
    1.0,
    1.25,
    1.5,
    1.75,
]
img_suffix = '.jpg'
launcher = 'none'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=False)
model = dict(
    backbone=dict(
        deep_stem=False,
        depth=50,
        frozen_stages=-1,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=False, type='SyncBN'),
        num_stages=4,
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        style='pytorch',
        type='ResNet'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_val=0,
        seg_pad_val=255,
        size=(
            512,
            1024,
        ),
        std=[
            58.395,
            57.12,
            57.375,
        ],
        test_cfg=dict(size_divisor=32),
        type='SegDataPreProcessor'),
    decode_head=dict(
        align_corners=False,
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                0.1,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='mmdet.DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=True),
        num_classes=57,
        num_queries=100,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                init_cfg=None,
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=1024,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        im2col_step=64,
                        init_cfg=None,
                        norm_cfg=None,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            init_cfg=None,
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='mmdet.MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        train_cfg=dict(
            assigner=dict(
                match_costs=[
                    dict(type='mmdet.ClassificationCost', weight=2.0),
                    dict(
                        type='mmdet.CrossEntropyLossCost',
                        use_sigmoid=True,
                        weight=5.0),
                    dict(
                        eps=1.0,
                        pred_act=True,
                        type='mmdet.DiceCost',
                        weight=5.0),
                ],
                type='mmdet.HungarianAssigner'),
            importance_sample_ratio=0.75,
            num_points=12544,
            oversample_ratio=3.0,
            sampler=dict(type='mmdet.MaskPseudoSampler')),
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    attn_drop=0.0,
                    batch_first=True,
                    dropout_layer=None,
                    embed_dims=256,
                    num_heads=8,
                    proj_drop=0.0),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    add_identity=True,
                    dropout_layer=None,
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    attn_drop=0.0,
                    batch_first=True,
                    dropout_layer=None,
                    embed_dims=256,
                    num_heads=8,
                    proj_drop=0.0)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(mode='whole'),
    train_cfg=dict(),
    type='EncoderDecoder')
num_classes = 57
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(decay_mult=1.0, lr_mult=0.1),
            level_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_embed=dict(decay_mult=0.0, lr_mult=1.0),
            query_feat=dict(decay_mult=0.0, lr_mult=1.0)),
        norm_decay_mult=0.0),
    type='OptimWrapper')
optimizer = dict(
    betas=(
        0.9,
        0.999,
    ),
    eps=1e-08,
    lr=0.0001,
    type='AdamW',
    weight_decay=0.05)
param_scheduler = [
    dict(
        begin=0,
        by_epoch=False,
        end=80000,
        eta_min=0,
        power=0.9,
        type='PolyLR'),
]
resume = False
seg_map_suffix = '.png'
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_prefix=dict(img_path='test/images', seg_map_path='test/masks'),
        data_root='/scratch/seg_benchmark/seg_full_redone_FINAL/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                2048,
                1024,
            ), type='Resize'),
            dict(type='LoadAnnotations'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=8,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    classwise=True, iou_metrics=[
        'mIoU',
    ], type='IoUNanAbsent')
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        2048,
        1024,
    ), type='Resize'),
    dict(type='LoadAnnotations'),
    dict(type='PackSegInputs'),
]
train_cfg = dict(
    max_iters=80000, type='IterBasedTrainLoop', val_interval=80000)
train_dataloader = dict(
    batch_size=2,
    dataset=dict(
        data_prefix=dict(img_path='train/images', seg_map_path='train/masks'),
        data_root='/scratch/seg_benchmark/seg_full_redone_FINAL/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(
                max_size=4096,
                resize_type='ResizeShortestEdge',
                scales=[
                    512,
                    614,
                    716,
                    819,
                    921,
                    1024,
                    1126,
                    1228,
                    1331,
                    1433,
                    1536,
                    1638,
                    1740,
                    1843,
                    1945,
                    2048,
                ],
                type='RandomChoiceResize'),
            dict(
                cat_max_ratio=0.75, crop_size=(
                    512,
                    1024,
                ), type='RandomCrop'),
            dict(prob=0.5, type='RandomFlip'),
            dict(type='PhotoMetricDistortion'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='InfiniteSampler'))
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(
        max_size=4096,
        resize_type='ResizeShortestEdge',
        scales=[
            512,
            614,
            716,
            819,
            921,
            1024,
            1126,
            1228,
            1331,
            1433,
            1536,
            1638,
            1740,
            1843,
            1945,
            2048,
        ],
        type='RandomChoiceResize'),
    dict(cat_max_ratio=0.75, crop_size=(
        512,
        1024,
    ), type='RandomCrop'),
    dict(prob=0.5, type='RandomFlip'),
    dict(type='PhotoMetricDistortion'),
    dict(type='PackSegInputs'),
]
tta_model = dict(type='SegTTAModel')
tta_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        transforms=[
            [
                dict(keep_ratio=True, scale_factor=0.5, type='Resize'),
                dict(keep_ratio=True, scale_factor=0.75, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.0, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.25, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.5, type='Resize'),
                dict(keep_ratio=True, scale_factor=1.75, type='Resize'),
            ],
            [
                dict(direction='horizontal', prob=0.0, type='RandomFlip'),
                dict(direction='horizontal', prob=1.0, type='RandomFlip'),
            ],
            [
                dict(type='LoadAnnotations'),
            ],
            [
                dict(type='PackSegInputs'),
            ],
        ],
        type='TestTimeAug'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_prefix=dict(img_path='test/images', seg_map_path='test/masks'),
        data_root='/scratch/seg_benchmark/seg_full_redone_FINAL/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                2048,
                1024,
            ), type='Resize'),
            dict(type='LoadAnnotations'),
            dict(type='PackSegInputs'),
        ],
        type='YourDataset_BIG'),
    num_workers=8,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    iou_metrics=[
        'mIoU',
    ], type='IoUMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='SegLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = '/scratch/seg_benchmark/FINAL_seg_full_redone_redo/mask2former_R50'

07/29 22:57:14 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
07/29 22:57:14 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.0.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.0.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.0.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.0.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.0.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.0.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.0.downsample.1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.0.downsample.1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.1.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.1.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.1.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.1.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.1.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.1.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.2.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.2.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.2.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.2.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.2.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer1.2.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.0.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.0.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.0.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.0.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.0.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.0.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.0.downsample.1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.0.downsample.1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.1.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.1.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.1.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.1.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.1.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.1.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.2.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.2.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.2.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.2.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.2.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.2.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.3.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.3.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.3.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.3.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.3.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer2.3.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.0.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.0.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.0.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.0.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.0.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.0.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.0.downsample.1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.0.downsample.1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.1.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.1.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.1.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.1.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.1.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.1.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.2.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.2.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.2.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.2.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.2.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.2.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.3.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.3.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.3.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.3.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.3.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.3.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.4.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.4.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.4.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.4.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.4.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.4.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.5.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.5.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.5.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.5.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.5.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer3.5.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.0.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.0.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.0.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.0.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.0.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.0.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.0.downsample.1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.0.downsample.1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.1.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.1.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.1.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.1.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.1.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.1.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.2.bn1.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.2.bn1.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.2.bn2.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.2.bn2.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr=1e-05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:weight_decay=0.05
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr_mult=0.1
07/29 22:57:14 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:decay_mult=1.0
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.2.bn3.weight is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - WARNING - backbone.layer4.2.bn3.bias is skipped since its requires_grad=False
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.bias:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr=0.0001
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:weight_decay=0.0
07/29 22:57:14 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr_mult=1.0
07/29 22:57:15 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:decay_mult=0.0
07/29 22:57:15 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr=0.0001
07/29 22:57:15 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:weight_decay=0.0
07/29 22:57:15 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr_mult=1.0
07/29 22:57:15 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:decay_mult=0.0
07/29 22:57:15 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr=0.0001
07/29 22:57:15 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:weight_decay=0.0
07/29 22:57:15 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr_mult=1.0
07/29 22:57:15 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:decay_mult=0.0
07/29 22:57:15 - mmengine - WARNING - The prefix is not set in metric class IoUMetric.
07/29 22:57:15 - mmengine - INFO - load model from: torchvision://resnet50
07/29 22:57:15 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
07/29 22:57:16 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

07/29 22:57:16 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
07/29 22:57:16 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
07/29 22:57:16 - mmengine - INFO - Checkpoints will be saved to /scratch/seg_benchmark/FINAL_seg_full_redone_redo/mask2former_R50.
/home2/yasharora120/miniconda3/envs/mmseg/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400441250/work/aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
07/29 22:57:46 - mmengine - INFO - Iter(train) [   50/80000]  base_lr: 9.9945e-05 lr: 9.9945e-06  eta: 13:20:21  time: 0.4516  data_time: 0.0100  memory: 10587  grad_norm: 130.8985  loss: 97.5249  decode.loss_cls: 4.3242  decode.loss_mask: 1.9968  decode.loss_dice: 4.2007  decode.d0.loss_cls: 8.1870  decode.d0.loss_mask: 1.5815  decode.d0.loss_dice: 3.5716  decode.d1.loss_cls: 3.7045  decode.d1.loss_mask: 1.5688  decode.d1.loss_dice: 3.5596  decode.d2.loss_cls: 3.6209  decode.d2.loss_mask: 1.6125  decode.d2.loss_dice: 3.5686  decode.d3.loss_cls: 3.5740  decode.d3.loss_mask: 1.5598  decode.d3.loss_dice: 3.6096  decode.d4.loss_cls: 3.7815  decode.d4.loss_mask: 1.5555  decode.d4.loss_dice: 3.6607  decode.d5.loss_cls: 3.8423  decode.d5.loss_mask: 1.5584  decode.d5.loss_dice: 3.6747  decode.d6.loss_cls: 3.9854  decode.d6.loss_mask: 1.6528  decode.d6.loss_dice: 3.7649  decode.d7.loss_cls: 4.1229  decode.d7.loss_mask: 1.7154  decode.d7.loss_dice: 3.8774  decode.d8.loss_cls: 4.2462  decode.d8.loss_mask: 1.8756  decode.d8.loss_dice: 3.9711
07/29 22:58:08 - mmengine - INFO - Iter(train) [  100/80000]  base_lr: 9.9889e-05 lr: 9.9889e-06  eta: 11:41:24  time: 0.4526  data_time: 0.0106  memory: 5264  grad_norm: 218.3776  loss: 72.1210  decode.loss_cls: 3.0435  decode.loss_mask: 1.5477  decode.loss_dice: 2.6397  decode.d0.loss_cls: 8.0732  decode.d0.loss_mask: 1.3600  decode.d0.loss_dice: 2.5723  decode.d1.loss_cls: 2.8316  decode.d1.loss_mask: 1.3786  decode.d1.loss_dice: 2.4139  decode.d2.loss_cls: 2.5852  decode.d2.loss_mask: 1.4151  decode.d2.loss_dice: 2.4054  decode.d3.loss_cls: 2.5189  decode.d3.loss_mask: 1.4084  decode.d3.loss_dice: 2.4907  decode.d4.loss_cls: 2.5660  decode.d4.loss_mask: 1.4072  decode.d4.loss_dice: 2.4268  decode.d5.loss_cls: 2.6162  decode.d5.loss_mask: 1.4292  decode.d5.loss_dice: 2.4278  decode.d6.loss_cls: 2.7502  decode.d6.loss_mask: 1.4395  decode.d6.loss_dice: 2.4789  decode.d7.loss_cls: 2.8478  decode.d7.loss_mask: 1.5400  decode.d7.loss_dice: 2.5088  decode.d8.loss_cls: 2.9285  decode.d8.loss_mask: 1.5006  decode.d8.loss_dice: 2.5693
07/29 22:58:31 - mmengine - INFO - Iter(train) [  150/80000]  base_lr: 9.9832e-05 lr: 9.9832e-06  eta: 11:07:14  time: 0.4495  data_time: 0.0102  memory: 5281  grad_norm: 225.4554  loss: 66.7000  decode.loss_cls: 3.2167  decode.loss_mask: 1.3247  decode.loss_dice: 2.2596  decode.d0.loss_cls: 7.9274  decode.d0.loss_mask: 1.0549  decode.d0.loss_dice: 2.2410  decode.d1.loss_cls: 2.8525  decode.d1.loss_mask: 1.0262  decode.d1.loss_dice: 1.9930  decode.d2.loss_cls: 2.7683  decode.d2.loss_mask: 1.1051  decode.d2.loss_dice: 1.9851  decode.d3.loss_cls: 2.8670  decode.d3.loss_mask: 1.1042  decode.d3.loss_dice: 1.9178  decode.d4.loss_cls: 3.0556  decode.d4.loss_mask: 1.0404  decode.d4.loss_dice: 1.9248  decode.d5.loss_cls: 3.0499  decode.d5.loss_mask: 1.0462  decode.d5.loss_dice: 1.9330  decode.d6.loss_cls: 3.0914  decode.d6.loss_mask: 1.1013  decode.d6.loss_dice: 1.9665  decode.d7.loss_cls: 3.1545  decode.d7.loss_mask: 1.1721  decode.d7.loss_dice: 2.0564  decode.d8.loss_cls: 3.1549  decode.d8.loss_mask: 1.2018  decode.d8.loss_dice: 2.1076
07/29 22:58:53 - mmengine - INFO - Iter(train) [  200/80000]  base_lr: 9.9776e-05 lr: 9.9776e-06  eta: 10:48:48  time: 0.4465  data_time: 0.0098  memory: 5244  grad_norm: 251.9306  loss: 52.3131  decode.loss_cls: 2.6617  decode.loss_mask: 0.8914  decode.loss_dice: 1.2105  decode.d0.loss_cls: 7.8921  decode.d0.loss_mask: 0.9377  decode.d0.loss_dice: 1.5819  decode.d1.loss_cls: 2.5380  decode.d1.loss_mask: 0.9506  decode.d1.loss_dice: 1.3449  decode.d2.loss_cls: 2.3891  decode.d2.loss_mask: 0.9329  decode.d2.loss_dice: 1.2198  decode.d3.loss_cls: 2.5551  decode.d3.loss_mask: 0.8469  decode.d3.loss_dice: 1.1757  decode.d4.loss_cls: 2.5393  decode.d4.loss_mask: 0.8723  decode.d4.loss_dice: 1.1888  decode.d5.loss_cls: 2.6074  decode.d5.loss_mask: 0.8711  decode.d5.loss_dice: 1.1839  decode.d6.loss_cls: 2.6303  decode.d6.loss_mask: 0.8401  decode.d6.loss_dice: 1.1420  decode.d7.loss_cls: 2.6416  decode.d7.loss_mask: 0.8633  decode.d7.loss_dice: 1.1410  decode.d8.loss_cls: 2.6292  decode.d8.loss_mask: 0.8973  decode.d8.loss_dice: 1.1372
07/29 22:59:15 - mmengine - INFO - Iter(train) [  250/80000]  base_lr: 9.9720e-05 lr: 9.9720e-06  eta: 10:36:55  time: 0.4422  data_time: 0.0096  memory: 5338  grad_norm: 286.3852  loss: 53.4432  decode.loss_cls: 2.5301  decode.loss_mask: 0.9483  decode.loss_dice: 1.3503  decode.d0.loss_cls: 7.7005  decode.d0.loss_mask: 0.9062  decode.d0.loss_dice: 1.7012  decode.d1.loss_cls: 2.5763  decode.d1.loss_mask: 0.9008  decode.d1.loss_dice: 1.4087  decode.d2.loss_cls: 2.5289  decode.d2.loss_mask: 0.8941  decode.d2.loss_dice: 1.3726  decode.d3.loss_cls: 2.4584  decode.d3.loss_mask: 0.8876  decode.d3.loss_dice: 1.3207  decode.d4.loss_cls: 2.4730  decode.d4.loss_mask: 0.9013  decode.d4.loss_dice: 1.3203  decode.d5.loss_cls: 2.4908  decode.d5.loss_mask: 0.9007  decode.d5.loss_dice: 1.3739  decode.d6.loss_cls: 2.4759  decode.d6.loss_mask: 0.9106  decode.d6.loss_dice: 1.3956  decode.d7.loss_cls: 2.5456  decode.d7.loss_mask: 0.9317  decode.d7.loss_dice: 1.3787  decode.d8.loss_cls: 2.5546  decode.d8.loss_mask: 0.9303  decode.d8.loss_dice: 1.3753
07/29 22:59:38 - mmengine - INFO - Iter(train) [  300/80000]  base_lr: 9.9664e-05 lr: 9.9664e-06  eta: 10:29:10  time: 0.4486  data_time: 0.0099  memory: 5266  grad_norm: 338.4005  loss: 51.2104  decode.loss_cls: 2.4832  decode.loss_mask: 1.0107  decode.loss_dice: 1.2344  decode.d0.loss_cls: 7.5174  decode.d0.loss_mask: 1.0616  decode.d0.loss_dice: 1.5806  decode.d1.loss_cls: 2.3620  decode.d1.loss_mask: 1.0730  decode.d1.loss_dice: 1.3552  decode.d2.loss_cls: 2.2960  decode.d2.loss_mask: 1.0001  decode.d2.loss_dice: 1.2216  decode.d3.loss_cls: 2.2058  decode.d3.loss_mask: 0.9902  decode.d3.loss_dice: 1.1503  decode.d4.loss_cls: 2.3051  decode.d4.loss_mask: 0.9516  decode.d4.loss_dice: 1.1867  decode.d5.loss_cls: 2.3776  decode.d5.loss_mask: 0.9236  decode.d5.loss_dice: 1.1507  decode.d6.loss_cls: 2.4329  decode.d6.loss_mask: 0.9468  decode.d6.loss_dice: 1.1452  decode.d7.loss_cls: 2.4693  decode.d7.loss_mask: 0.9610  decode.d7.loss_dice: 1.1823  decode.d8.loss_cls: 2.5120  decode.d8.loss_mask: 0.9601  decode.d8.loss_dice: 1.1635
07/29 23:00:00 - mmengine - INFO - Iter(train) [  350/80000]  base_lr: 9.9607e-05 lr: 9.9607e-06  eta: 10:23:52  time: 0.4462  data_time: 0.0096  memory: 5244  grad_norm: 268.4163  loss: 47.0405  decode.loss_cls: 2.5410  decode.loss_mask: 0.7675  decode.loss_dice: 0.9865  decode.d0.loss_cls: 7.4423  decode.d0.loss_mask: 0.8121  decode.d0.loss_dice: 1.2825  decode.d1.loss_cls: 2.4079  decode.d1.loss_mask: 0.7929  decode.d1.loss_dice: 1.0632  decode.d2.loss_cls: 2.3472  decode.d2.loss_mask: 0.8043  decode.d2.loss_dice: 1.0034  decode.d3.loss_cls: 2.3157  decode.d3.loss_mask: 0.7658  decode.d3.loss_dice: 0.9379  decode.d4.loss_cls: 2.3476  decode.d4.loss_mask: 0.7708  decode.d4.loss_dice: 0.9593  decode.d5.loss_cls: 2.4492  decode.d5.loss_mask: 0.7430  decode.d5.loss_dice: 0.9459  decode.d6.loss_cls: 2.4447  decode.d6.loss_mask: 0.7279  decode.d6.loss_dice: 0.9494  decode.d7.loss_cls: 2.4731  decode.d7.loss_mask: 0.7454  decode.d7.loss_dice: 0.9475  decode.d8.loss_cls: 2.5182  decode.d8.loss_mask: 0.7744  decode.d8.loss_dice: 0.9738
07/29 23:00:23 - mmengine - INFO - Iter(train) [  400/80000]  base_lr: 9.9551e-05 lr: 9.9551e-06  eta: 10:20:07  time: 0.4587  data_time: 0.0105  memory: 5264  grad_norm: 228.8453  loss: 45.5877  decode.loss_cls: 2.4909  decode.loss_mask: 0.6228  decode.loss_dice: 0.9521  decode.d0.loss_cls: 7.2964  decode.d0.loss_mask: 0.6838  decode.d0.loss_dice: 1.3303  decode.d1.loss_cls: 2.5294  decode.d1.loss_mask: 0.6275  decode.d1.loss_dice: 1.0106  decode.d2.loss_cls: 2.4381  decode.d2.loss_mask: 0.6038  decode.d2.loss_dice: 0.9136  decode.d3.loss_cls: 2.3587  decode.d3.loss_mask: 0.6545  decode.d3.loss_dice: 0.9572  decode.d4.loss_cls: 2.3512  decode.d4.loss_mask: 0.6290  decode.d4.loss_dice: 0.9631  decode.d5.loss_cls: 2.4163  decode.d5.loss_mask: 0.6208  decode.d5.loss_dice: 0.9647  decode.d6.loss_cls: 2.4568  decode.d6.loss_mask: 0.6173  decode.d6.loss_dice: 0.9585  decode.d7.loss_cls: 2.5330  decode.d7.loss_mask: 0.5960  decode.d7.loss_dice: 0.9238  decode.d8.loss_cls: 2.4884  decode.d8.loss_mask: 0.6456  decode.d8.loss_dice: 0.9537
07/29 23:00:45 - mmengine - INFO - Iter(train) [  450/80000]  base_lr: 9.9495e-05 lr: 9.9495e-06  eta: 10:18:07  time: 0.4547  data_time: 0.0103  memory: 5244  grad_norm: 304.0554  loss: 42.4217  decode.loss_cls: 2.1287  decode.loss_mask: 0.8113  decode.loss_dice: 0.8847  decode.d0.loss_cls: 7.0472  decode.d0.loss_mask: 0.7233  decode.d0.loss_dice: 1.0839  decode.d1.loss_cls: 2.0045  decode.d1.loss_mask: 0.7681  decode.d1.loss_dice: 0.9322  decode.d2.loss_cls: 2.0247  decode.d2.loss_mask: 0.7487  decode.d2.loss_dice: 0.8830  decode.d3.loss_cls: 2.0373  decode.d3.loss_mask: 0.7440  decode.d3.loss_dice: 0.8884  decode.d4.loss_cls: 2.0092  decode.d4.loss_mask: 0.7766  decode.d4.loss_dice: 0.8611  decode.d5.loss_cls: 2.0867  decode.d5.loss_mask: 0.7730  decode.d5.loss_dice: 0.8787  decode.d6.loss_cls: 2.1388  decode.d6.loss_mask: 0.7705  decode.d6.loss_dice: 0.8229  decode.d7.loss_cls: 2.1301  decode.d7.loss_mask: 0.7747  decode.d7.loss_dice: 0.8795  decode.d8.loss_cls: 2.1541  decode.d8.loss_mask: 0.7737  decode.d8.loss_dice: 0.8820
07/29 23:01:08 - mmengine - INFO - Iter(train) [  500/80000]  base_lr: 9.9438e-05 lr: 9.9438e-06  eta: 10:15:56  time: 0.4555  data_time: 0.0105  memory: 5245  grad_norm: 224.5015  loss: 45.7318  decode.loss_cls: 2.6567  decode.loss_mask: 0.6026  decode.loss_dice: 0.8373  decode.d0.loss_cls: 7.1728  decode.d0.loss_mask: 0.6811  decode.d0.loss_dice: 1.1708  decode.d1.loss_cls: 2.6974  decode.d1.loss_mask: 0.5968  decode.d1.loss_dice: 0.9551  decode.d2.loss_cls: 2.6403  decode.d2.loss_mask: 0.5755  decode.d2.loss_dice: 0.8790  decode.d3.loss_cls: 2.5605  decode.d3.loss_mask: 0.5688  decode.d3.loss_dice: 0.8333  decode.d4.loss_cls: 2.5008  decode.d4.loss_mask: 0.6551  decode.d4.loss_dice: 0.8736  decode.d5.loss_cls: 2.5235  decode.d5.loss_mask: 0.6161  decode.d5.loss_dice: 0.8714  decode.d6.loss_cls: 2.6344  decode.d6.loss_mask: 0.6151  decode.d6.loss_dice: 0.8379  decode.d7.loss_cls: 2.6895  decode.d7.loss_mask: 0.5619  decode.d7.loss_dice: 0.8416  decode.d8.loss_cls: 2.6599  decode.d8.loss_mask: 0.5753  decode.d8.loss_dice: 0.8477
07/29 23:01:31 - mmengine - INFO - Iter(train) [  550/80000]  base_lr: 9.9382e-05 lr: 9.9382e-06  eta: 10:14:19  time: 0.4537  data_time: 0.0105  memory: 5281  grad_norm: 299.9016  loss: 43.4889  decode.loss_cls: 2.2802  decode.loss_mask: 0.7289  decode.loss_dice: 0.8870  decode.d0.loss_cls: 6.9018  decode.d0.loss_mask: 0.6701  decode.d0.loss_dice: 1.1750  decode.d1.loss_cls: 2.3448  decode.d1.loss_mask: 0.6580  decode.d1.loss_dice: 0.8768  decode.d2.loss_cls: 2.2366  decode.d2.loss_mask: 0.6883  decode.d2.loss_dice: 0.8961  decode.d3.loss_cls: 2.1831  decode.d3.loss_mask: 0.6350  decode.d3.loss_dice: 0.8472  decode.d4.loss_cls: 2.2120  decode.d4.loss_mask: 0.6643  decode.d4.loss_dice: 0.8300  decode.d5.loss_cls: 2.2321  decode.d5.loss_mask: 0.6604  decode.d5.loss_dice: 0.8746  decode.d6.loss_cls: 2.2193  decode.d6.loss_mask: 0.7553  decode.d6.loss_dice: 0.9003  decode.d7.loss_cls: 2.3681  decode.d7.loss_mask: 0.7768  decode.d7.loss_dice: 0.8972  decode.d8.loss_cls: 2.3914  decode.d8.loss_mask: 0.7519  decode.d8.loss_dice: 0.9463
07/29 23:01:54 - mmengine - INFO - Iter(train) [  600/80000]  base_lr: 9.9326e-05 lr: 9.9326e-06  eta: 10:12:51  time: 0.4573  data_time: 0.0109  memory: 5245  grad_norm: 159.2028  loss: 36.6353  decode.loss_cls: 2.0747  decode.loss_mask: 0.5114  decode.loss_dice: 0.6406  decode.d0.loss_cls: 6.7737  decode.d0.loss_mask: 0.5041  decode.d0.loss_dice: 0.8427  decode.d1.loss_cls: 2.0251  decode.d1.loss_mask: 0.5127  decode.d1.loss_dice: 0.6685  decode.d2.loss_cls: 1.9462  decode.d2.loss_mask: 0.4854  decode.d2.loss_dice: 0.5945  decode.d3.loss_cls: 1.9516  decode.d3.loss_mask: 0.4981  decode.d3.loss_dice: 0.6115  decode.d4.loss_cls: 1.9681  decode.d4.loss_mask: 0.5117  decode.d4.loss_dice: 0.6599  decode.d5.loss_cls: 2.0076  decode.d5.loss_mask: 0.5067  decode.d5.loss_dice: 0.6532  decode.d6.loss_cls: 2.0099  decode.d6.loss_mask: 0.4938  decode.d6.loss_dice: 0.6505  decode.d7.loss_cls: 2.0801  decode.d7.loss_mask: 0.5238  decode.d7.loss_dice: 0.6798  decode.d8.loss_cls: 2.0917  decode.d8.loss_mask: 0.5107  decode.d8.loss_dice: 0.6471
07/29 23:02:16 - mmengine - INFO - Iter(train) [  650/80000]  base_lr: 9.9270e-05 lr: 9.9270e-06  eta: 10:11:44  time: 0.4545  data_time: 0.0104  memory: 5307  grad_norm: 298.4275  loss: 37.7119  decode.loss_cls: 2.0944  decode.loss_mask: 0.6103  decode.loss_dice: 0.6287  decode.d0.loss_cls: 6.4541  decode.d0.loss_mask: 0.6823  decode.d0.loss_dice: 0.9003  decode.d1.loss_cls: 1.9973  decode.d1.loss_mask: 0.6736  decode.d1.loss_dice: 0.6975  decode.d2.loss_cls: 1.9527  decode.d2.loss_mask: 0.6611  decode.d2.loss_dice: 0.7116  decode.d3.loss_cls: 1.8625  decode.d3.loss_mask: 0.6421  decode.d3.loss_dice: 0.6668  decode.d4.loss_cls: 1.9309  decode.d4.loss_mask: 0.6126  decode.d4.loss_dice: 0.6574  decode.d5.loss_cls: 2.0334  decode.d5.loss_mask: 0.5962  decode.d5.loss_dice: 0.6480  decode.d6.loss_cls: 2.0457  decode.d6.loss_mask: 0.6016  decode.d6.loss_dice: 0.7302  decode.d7.loss_cls: 2.1170  decode.d7.loss_mask: 0.5832  decode.d7.loss_dice: 0.6291  decode.d8.loss_cls: 2.1180  decode.d8.loss_mask: 0.5675  decode.d8.loss_dice: 0.6057
07/29 23:02:39 - mmengine - INFO - Iter(train) [  700/80000]  base_lr: 9.9213e-05 lr: 9.9213e-06  eta: 10:10:32  time: 0.4515  data_time: 0.0101  memory: 5266  grad_norm: 293.3412  loss: 39.5237  decode.loss_cls: 2.2706  decode.loss_mask: 0.5948  decode.loss_dice: 0.6667  decode.d0.loss_cls: 6.6323  decode.d0.loss_mask: 0.5233  decode.d0.loss_dice: 0.8903  decode.d1.loss_cls: 2.3722  decode.d1.loss_mask: 0.5211  decode.d1.loss_dice: 0.6468  decode.d2.loss_cls: 2.2017  decode.d2.loss_mask: 0.6258  decode.d2.loss_dice: 0.6347  decode.d3.loss_cls: 2.1035  decode.d3.loss_mask: 0.6639  decode.d3.loss_dice: 0.6614  decode.d4.loss_cls: 2.1326  decode.d4.loss_mask: 0.6524  decode.d4.loss_dice: 0.6485  decode.d5.loss_cls: 2.2282  decode.d5.loss_mask: 0.6103  decode.d5.loss_dice: 0.6000  decode.d6.loss_cls: 2.2621  decode.d6.loss_mask: 0.6213  decode.d6.loss_dice: 0.6245  decode.d7.loss_cls: 2.3572  decode.d7.loss_mask: 0.5906  decode.d7.loss_dice: 0.6085  decode.d8.loss_cls: 2.3240  decode.d8.loss_mask: 0.6088  decode.d8.loss_dice: 0.6458
07/29 23:03:02 - mmengine - INFO - Iter(train) [  750/80000]  base_lr: 9.9157e-05 lr: 9.9157e-06  eta: 10:09:21  time: 0.4528  data_time: 0.0105  memory: 5266  grad_norm: 232.7408  loss: 33.1126  decode.loss_cls: 1.8109  decode.loss_mask: 0.4991  decode.loss_dice: 0.6136  decode.d0.loss_cls: 6.3054  decode.d0.loss_mask: 0.5111  decode.d0.loss_dice: 0.7684  decode.d1.loss_cls: 1.8222  decode.d1.loss_mask: 0.4255  decode.d1.loss_dice: 0.6090  decode.d2.loss_cls: 1.6137  decode.d2.loss_mask: 0.4588  decode.d2.loss_dice: 0.5555  decode.d3.loss_cls: 1.7660  decode.d3.loss_mask: 0.4349  decode.d3.loss_dice: 0.5453  decode.d4.loss_cls: 1.8300  decode.d4.loss_mask: 0.3837  decode.d4.loss_dice: 0.4876  decode.d5.loss_cls: 1.7985  decode.d5.loss_mask: 0.4137  decode.d5.loss_dice: 0.5281  decode.d6.loss_cls: 1.7903  decode.d6.loss_mask: 0.4560  decode.d6.loss_dice: 0.5560  decode.d7.loss_cls: 1.9079  decode.d7.loss_mask: 0.4643  decode.d7.loss_dice: 0.6378  decode.d8.loss_cls: 1.9466  decode.d8.loss_mask: 0.5011  decode.d8.loss_dice: 0.6718
07/29 23:03:24 - mmengine - INFO - Iter(train) [  800/80000]  base_lr: 9.9101e-05 lr: 9.9101e-06  eta: 10:08:25  time: 0.4595  data_time: 0.0107  memory: 5338  grad_norm: 198.6432  loss: 36.8624  decode.loss_cls: 2.1813  decode.loss_mask: 0.5033  decode.loss_dice: 0.7039  decode.d0.loss_cls: 6.4228  decode.d0.loss_mask: 0.5082  decode.d0.loss_dice: 0.8122  decode.d1.loss_cls: 2.1953  decode.d1.loss_mask: 0.4862  decode.d1.loss_dice: 0.6387  decode.d2.loss_cls: 2.0586  decode.d2.loss_mask: 0.4714  decode.d2.loss_dice: 0.6196  decode.d3.loss_cls: 2.0394  decode.d3.loss_mask: 0.4742  decode.d3.loss_dice: 0.6166  decode.d4.loss_cls: 2.0464  decode.d4.loss_mask: 0.4395  decode.d4.loss_dice: 0.6271  decode.d5.loss_cls: 2.0591  decode.d5.loss_mask: 0.4688  decode.d5.loss_dice: 0.6304  decode.d6.loss_cls: 2.1254  decode.d6.loss_mask: 0.4750  decode.d6.loss_dice: 0.6390  decode.d7.loss_cls: 2.1181  decode.d7.loss_mask: 0.5049  decode.d7.loss_dice: 0.6663  decode.d8.loss_cls: 2.1741  decode.d8.loss_mask: 0.4666  decode.d8.loss_dice: 0.6899
07/29 23:03:47 - mmengine - INFO - Iter(train) [  850/80000]  base_lr: 9.9044e-05 lr: 9.9044e-06  eta: 10:07:42  time: 0.4556  data_time: 0.0103  memory: 5266  grad_norm: 226.2417  loss: 33.8034  decode.loss_cls: 2.0011  decode.loss_mask: 0.4479  decode.loss_dice: 0.5243  decode.d0.loss_cls: 6.2475  decode.d0.loss_mask: 0.4742  decode.d0.loss_dice: 0.8419  decode.d1.loss_cls: 2.0276  decode.d1.loss_mask: 0.4410  decode.d1.loss_dice: 0.5684  decode.d2.loss_cls: 1.8397  decode.d2.loss_mask: 0.4308  decode.d2.loss_dice: 0.5577  decode.d3.loss_cls: 1.9137  decode.d3.loss_mask: 0.4184  decode.d3.loss_dice: 0.5026  decode.d4.loss_cls: 1.9577  decode.d4.loss_mask: 0.4078  decode.d4.loss_dice: 0.5225  decode.d5.loss_cls: 1.9971  decode.d5.loss_mask: 0.4020  decode.d5.loss_dice: 0.5108  decode.d6.loss_cls: 1.9677  decode.d6.loss_mask: 0.4182  decode.d6.loss_dice: 0.4954  decode.d7.loss_cls: 1.9409  decode.d7.loss_mask: 0.4128  decode.d7.loss_dice: 0.5262  decode.d8.loss_cls: 2.0424  decode.d8.loss_mask: 0.4454  decode.d8.loss_dice: 0.5199
07/29 23:04:10 - mmengine - INFO - Iter(train) [  900/80000]  base_lr: 9.8988e-05 lr: 9.8988e-06  eta: 10:06:45  time: 0.4542  data_time: 0.0104  memory: 5245  grad_norm: 160.2054  loss: 34.6561  decode.loss_cls: 2.2956  decode.loss_mask: 0.3617  decode.loss_dice: 0.5021  decode.d0.loss_cls: 6.0046  decode.d0.loss_mask: 0.4156  decode.d0.loss_dice: 0.7009  decode.d1.loss_cls: 2.1522  decode.d1.loss_mask: 0.3709  decode.d1.loss_dice: 0.5281  decode.d2.loss_cls: 2.0867  decode.d2.loss_mask: 0.3374  decode.d2.loss_dice: 0.4913  decode.d3.loss_cls: 2.1784  decode.d3.loss_mask: 0.3418  decode.d3.loss_dice: 0.4753  decode.d4.loss_cls: 2.1687  decode.d4.loss_mask: 0.3510  decode.d4.loss_dice: 0.4863  decode.d5.loss_cls: 2.1963  decode.d5.loss_mask: 0.3573  decode.d5.loss_dice: 0.5046  decode.d6.loss_cls: 2.2393  decode.d6.loss_mask: 0.3636  decode.d6.loss_dice: 0.5078  decode.d7.loss_cls: 2.2292  decode.d7.loss_mask: 0.3604  decode.d7.loss_dice: 0.4899  decode.d8.loss_cls: 2.3207  decode.d8.loss_mask: 0.3619  decode.d8.loss_dice: 0.4762
07/29 23:04:33 - mmengine - INFO - Iter(train) [  950/80000]  base_lr: 9.8932e-05 lr: 9.8932e-06  eta: 10:06:07  time: 0.4561  data_time: 0.0107  memory: 5245  grad_norm: 283.0493  loss: 34.0617  decode.loss_cls: 1.7574  decode.loss_mask: 0.6228  decode.loss_dice: 0.6152  decode.d0.loss_cls: 5.7642  decode.d0.loss_mask: 0.6459  decode.d0.loss_dice: 0.8000  decode.d1.loss_cls: 1.7664  decode.d1.loss_mask: 0.5826  decode.d1.loss_dice: 0.6822  decode.d2.loss_cls: 1.7502  decode.d2.loss_mask: 0.5723  decode.d2.loss_dice: 0.5910  decode.d3.loss_cls: 1.6954  decode.d3.loss_mask: 0.5867  decode.d3.loss_dice: 0.6096  decode.d4.loss_cls: 1.7815  decode.d4.loss_mask: 0.5767  decode.d4.loss_dice: 0.6346  decode.d5.loss_cls: 1.7517  decode.d5.loss_mask: 0.5710  decode.d5.loss_dice: 0.6284  decode.d6.loss_cls: 1.7921  decode.d6.loss_mask: 0.5734  decode.d6.loss_dice: 0.5959  decode.d7.loss_cls: 1.8446  decode.d7.loss_mask: 0.6017  decode.d7.loss_dice: 0.5962  decode.d8.loss_cls: 1.8318  decode.d8.loss_mask: 0.6198  decode.d8.loss_dice: 0.6206
07/29 23:04:56 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250729_225707
07/29 23:04:56 - mmengine - INFO - Iter(train) [ 1000/80000]  base_lr: 9.8875e-05 lr: 9.8875e-06  eta: 10:05:41  time: 0.4575  data_time: 0.0105  memory: 5307  grad_norm: 250.2064  loss: 35.5588  decode.loss_cls: 1.9939  decode.loss_mask: 0.5195  decode.loss_dice: 0.6568  decode.d0.loss_cls: 5.7957  decode.d0.loss_mask: 0.5859  decode.d0.loss_dice: 0.9571  decode.d1.loss_cls: 2.0333  decode.d1.loss_mask: 0.5088  decode.d1.loss_dice: 0.6323  decode.d2.loss_cls: 1.8346  decode.d2.loss_mask: 0.5178  decode.d2.loss_dice: 0.6743  decode.d3.loss_cls: 1.9347  decode.d3.loss_mask: 0.5326  decode.d3.loss_dice: 0.6622  decode.d4.loss_cls: 1.9230  decode.d4.loss_mask: 0.5108  decode.d4.loss_dice: 0.6579  decode.d5.loss_cls: 1.9476  decode.d5.loss_mask: 0.5341  decode.d5.loss_dice: 0.6690  decode.d6.loss_cls: 1.9671  decode.d6.loss_mask: 0.5027  decode.d6.loss_dice: 0.6672  decode.d7.loss_cls: 1.9993  decode.d7.loss_mask: 0.4911  decode.d7.loss_dice: 0.6259  decode.d8.loss_cls: 2.0110  decode.d8.loss_mask: 0.5217  decode.d8.loss_dice: 0.6906
07/29 23:05:19 - mmengine - INFO - Iter(train) [ 1050/80000]  base_lr: 9.8819e-05 lr: 9.8819e-06  eta: 10:05:12  time: 0.4571  data_time: 0.0107  memory: 5281  grad_norm: 190.4667  loss: 29.7096  decode.loss_cls: 1.6812  decode.loss_mask: 0.4041  decode.loss_dice: 0.4657  decode.d0.loss_cls: 5.6097  decode.d0.loss_mask: 0.4932  decode.d0.loss_dice: 0.7172  decode.d1.loss_cls: 1.7176  decode.d1.loss_mask: 0.4590  decode.d1.loss_dice: 0.5521  decode.d2.loss_cls: 1.5423  decode.d2.loss_mask: 0.4520  decode.d2.loss_dice: 0.5318  decode.d3.loss_cls: 1.5251  decode.d3.loss_mask: 0.4630  decode.d3.loss_dice: 0.5360  decode.d4.loss_cls: 1.5758  decode.d4.loss_mask: 0.4385  decode.d4.loss_dice: 0.4953  decode.d5.loss_cls: 1.6984  decode.d5.loss_mask: 0.4008  decode.d5.loss_dice: 0.4557  decode.d6.loss_cls: 1.6835  decode.d6.loss_mask: 0.4073  decode.d6.loss_dice: 0.4527  decode.d7.loss_cls: 1.6432  decode.d7.loss_mask: 0.3909  decode.d7.loss_dice: 0.4289  decode.d8.loss_cls: 1.6495  decode.d8.loss_mask: 0.4054  decode.d8.loss_dice: 0.4336
07/29 23:05:42 - mmengine - INFO - Iter(train) [ 1100/80000]  base_lr: 9.8763e-05 lr: 9.8763e-06  eta: 10:04:50  time: 0.4608  data_time: 0.0107  memory: 5264  grad_norm: 173.4720  loss: 30.0653  decode.loss_cls: 1.6603  decode.loss_mask: 0.4030  decode.loss_dice: 0.5578  decode.d0.loss_cls: 5.4555  decode.d0.loss_mask: 0.3982  decode.d0.loss_dice: 0.7185  decode.d1.loss_cls: 1.7222  decode.d1.loss_mask: 0.3853  decode.d1.loss_dice: 0.5790  decode.d2.loss_cls: 1.6382  decode.d2.loss_mask: 0.3926  decode.d2.loss_dice: 0.5349  decode.d3.loss_cls: 1.6763  decode.d3.loss_mask: 0.3692  decode.d3.loss_dice: 0.5335  decode.d4.loss_cls: 1.6964  decode.d4.loss_mask: 0.3797  decode.d4.loss_dice: 0.5517  decode.d5.loss_cls: 1.6511  decode.d5.loss_mask: 0.4402  decode.d5.loss_dice: 0.5457  decode.d6.loss_cls: 1.6403  decode.d6.loss_mask: 0.4123  decode.d6.loss_dice: 0.5294  decode.d7.loss_cls: 1.6818  decode.d7.loss_mask: 0.3865  decode.d7.loss_dice: 0.5466  decode.d8.loss_cls: 1.6172  decode.d8.loss_mask: 0.4214  decode.d8.loss_dice: 0.5405
07/29 23:06:04 - mmengine - INFO - Iter(train) [ 1150/80000]  base_lr: 9.8706e-05 lr: 9.8706e-06  eta: 10:04:15  time: 0.4566  data_time: 0.0104  memory: 5230  grad_norm: 212.1894  loss: 37.3082  decode.loss_cls: 2.0488  decode.loss_mask: 0.5074  decode.loss_dice: 0.6672  decode.d0.loss_cls: 5.3942  decode.d0.loss_mask: 0.6095  decode.d0.loss_dice: 0.9527  decode.d1.loss_cls: 2.2236  decode.d1.loss_mask: 0.5059  decode.d1.loss_dice: 0.6721  decode.d2.loss_cls: 2.0656  decode.d2.loss_mask: 0.4802  decode.d2.loss_dice: 0.6572  decode.d3.loss_cls: 2.0904  decode.d3.loss_mask: 0.5231  decode.d3.loss_dice: 0.6916  decode.d4.loss_cls: 2.1771  decode.d4.loss_mask: 0.5682  decode.d4.loss_dice: 0.6788  decode.d5.loss_cls: 2.1828  decode.d5.loss_mask: 0.5451  decode.d5.loss_dice: 0.7207  decode.d6.loss_cls: 2.2335  decode.d6.loss_mask: 0.5440  decode.d6.loss_dice: 0.7215  decode.d7.loss_cls: 2.1696  decode.d7.loss_mask: 0.5770  decode.d7.loss_dice: 0.6904  decode.d8.loss_cls: 2.1213  decode.d8.loss_mask: 0.5760  decode.d8.loss_dice: 0.7129
07/29 23:06:27 - mmengine - INFO - Iter(train) [ 1200/80000]  base_lr: 9.8650e-05 lr: 9.8650e-06  eta: 10:03:30  time: 0.4518  data_time: 0.0100  memory: 5266  grad_norm: 134.7072  loss: 31.9267  decode.loss_cls: 1.7876  decode.loss_mask: 0.3963  decode.loss_dice: 0.5382  decode.d0.loss_cls: 5.2983  decode.d0.loss_mask: 0.4267  decode.d0.loss_dice: 0.7104  decode.d1.loss_cls: 2.0199  decode.d1.loss_mask: 0.4218  decode.d1.loss_dice: 0.5966  decode.d2.loss_cls: 1.9179  decode.d2.loss_mask: 0.4059  decode.d2.loss_dice: 0.5391  decode.d3.loss_cls: 1.8547  decode.d3.loss_mask: 0.3980  decode.d3.loss_dice: 0.5566  decode.d4.loss_cls: 1.8843  decode.d4.loss_mask: 0.3933  decode.d4.loss_dice: 0.5723  decode.d5.loss_cls: 1.8692  decode.d5.loss_mask: 0.3841  decode.d5.loss_dice: 0.5164  decode.d6.loss_cls: 1.9670  decode.d6.loss_mask: 0.3781  decode.d6.loss_dice: 0.5081  decode.d7.loss_cls: 1.9197  decode.d7.loss_mask: 0.3736  decode.d7.loss_dice: 0.5285  decode.d8.loss_cls: 1.8693  decode.d8.loss_mask: 0.3759  decode.d8.loss_dice: 0.5189
07/29 23:06:50 - mmengine - INFO - Iter(train) [ 1250/80000]  base_lr: 9.8594e-05 lr: 9.8594e-06  eta: 10:02:51  time: 0.4554  data_time: 0.0103  memory: 5283  grad_norm: 181.3734  loss: 29.5882  decode.loss_cls: 1.6499  decode.loss_mask: 0.4799  decode.loss_dice: 0.4462  decode.d0.loss_cls: 5.0628  decode.d0.loss_mask: 0.5159  decode.d0.loss_dice: 0.6202  decode.d1.loss_cls: 1.8812  decode.d1.loss_mask: 0.4555  decode.d1.loss_dice: 0.4562  decode.d2.loss_cls: 1.6342  decode.d2.loss_mask: 0.4340  decode.d2.loss_dice: 0.4364  decode.d3.loss_cls: 1.6654  decode.d3.loss_mask: 0.4238  decode.d3.loss_dice: 0.4265  decode.d4.loss_cls: 1.7621  decode.d4.loss_mask: 0.4203  decode.d4.loss_dice: 0.4077  decode.d5.loss_cls: 1.7238  decode.d5.loss_mask: 0.4235  decode.d5.loss_dice: 0.4225  decode.d6.loss_cls: 1.7725  decode.d6.loss_mask: 0.4190  decode.d6.loss_dice: 0.4070  decode.d7.loss_cls: 1.7566  decode.d7.loss_mask: 0.4170  decode.d7.loss_dice: 0.4368  decode.d8.loss_cls: 1.7456  decode.d8.loss_mask: 0.4436  decode.d8.loss_dice: 0.4420
07/29 23:07:13 - mmengine - INFO - Iter(train) [ 1300/80000]  base_lr: 9.8537e-05 lr: 9.8537e-06  eta: 10:02:25  time: 0.4524  data_time: 0.0101  memory: 5266  grad_norm: 134.8184  loss: 30.2270  decode.loss_cls: 1.8343  decode.loss_mask: 0.3431  decode.loss_dice: 0.4281  decode.d0.loss_cls: 4.9565  decode.d0.loss_mask: 0.4177  decode.d0.loss_dice: 0.6422  decode.d1.loss_cls: 1.9585  decode.d1.loss_mask: 0.3857  decode.d1.loss_dice: 0.4952  decode.d2.loss_cls: 1.8441  decode.d2.loss_mask: 0.3672  decode.d2.loss_dice: 0.4571  decode.d3.loss_cls: 1.8927  decode.d3.loss_mask: 0.3459  decode.d3.loss_dice: 0.4238  decode.d4.loss_cls: 1.8897  decode.d4.loss_mask: 0.3463  decode.d4.loss_dice: 0.4329  decode.d5.loss_cls: 1.9156  decode.d5.loss_mask: 0.3551  decode.d5.loss_dice: 0.4209  decode.d6.loss_cls: 1.8948  decode.d6.loss_mask: 0.3632  decode.d6.loss_dice: 0.4747  decode.d7.loss_cls: 1.8821  decode.d7.loss_mask: 0.3582  decode.d7.loss_dice: 0.4325  decode.d8.loss_cls: 1.8924  decode.d8.loss_mask: 0.3490  decode.d8.loss_dice: 0.4275
07/29 23:07:36 - mmengine - INFO - Iter(train) [ 1350/80000]  base_lr: 9.8481e-05 lr: 9.8481e-06  eta: 10:01:55  time: 0.4595  data_time: 0.0107  memory: 5283  grad_norm: 201.5738  loss: 26.9519  decode.loss_cls: 1.5319  decode.loss_mask: 0.3797  decode.loss_dice: 0.3870  decode.d0.loss_cls: 4.6367  decode.d0.loss_mask: 0.4934  decode.d0.loss_dice: 0.6314  decode.d1.loss_cls: 1.6065  decode.d1.loss_mask: 0.3888  decode.d1.loss_dice: 0.4002  decode.d2.loss_cls: 1.5436  decode.d2.loss_mask: 0.3725  decode.d2.loss_dice: 0.3956  decode.d3.loss_cls: 1.5301  decode.d3.loss_mask: 0.3730  decode.d3.loss_dice: 0.4039  decode.d4.loss_cls: 1.5068  decode.d4.loss_mask: 0.3826  decode.d4.loss_dice: 0.4208  decode.d5.loss_cls: 1.5593  decode.d5.loss_mask: 0.3990  decode.d5.loss_dice: 0.4012  decode.d6.loss_cls: 1.5621  decode.d6.loss_mask: 0.4091  decode.d6.loss_dice: 0.4180  decode.d7.loss_cls: 1.6188  decode.d7.loss_mask: 0.3888  decode.d7.loss_dice: 0.4200  decode.d8.loss_cls: 1.6332  decode.d8.loss_mask: 0.3694  decode.d8.loss_dice: 0.3883
07/29 23:07:58 - mmengine - INFO - Iter(train) [ 1400/80000]  base_lr: 9.8425e-05 lr: 9.8425e-06  eta: 10:01:19  time: 0.4607  data_time: 0.0109  memory: 5307  grad_norm: 139.3219  loss: 30.0821  decode.loss_cls: 1.8824  decode.loss_mask: 0.3015  decode.loss_dice: 0.4400  decode.d0.loss_cls: 4.7304  decode.d0.loss_mask: 0.3967  decode.d0.loss_dice: 0.6341  decode.d1.loss_cls: 2.2430  decode.d1.loss_mask: 0.2981  decode.d1.loss_dice: 0.4363  decode.d2.loss_cls: 1.9988  decode.d2.loss_mask: 0.2887  decode.d2.loss_dice: 0.3970  decode.d3.loss_cls: 1.9290  decode.d3.loss_mask: 0.2942  decode.d3.loss_dice: 0.3910  decode.d4.loss_cls: 2.0392  decode.d4.loss_mask: 0.2886  decode.d4.loss_dice: 0.3976  decode.d5.loss_cls: 1.9645  decode.d5.loss_mask: 0.2915  decode.d5.loss_dice: 0.3993  decode.d6.loss_cls: 1.9299  decode.d6.loss_mask: 0.3048  decode.d6.loss_dice: 0.4166  decode.d7.loss_cls: 1.9797  decode.d7.loss_mask: 0.3072  decode.d7.loss_dice: 0.4228  decode.d8.loss_cls: 1.9305  decode.d8.loss_mask: 0.2986  decode.d8.loss_dice: 0.4501
07/29 23:08:21 - mmengine - INFO - Iter(train) [ 1450/80000]  base_lr: 9.8368e-05 lr: 9.8368e-06  eta: 10:00:49  time: 0.4555  data_time: 0.0103  memory: 5245  grad_norm: 282.9243  loss: 31.4063  decode.loss_cls: 1.7638  decode.loss_mask: 0.4997  decode.loss_dice: 0.5178  decode.d0.loss_cls: 4.5844  decode.d0.loss_mask: 0.5249  decode.d0.loss_dice: 0.7278  decode.d1.loss_cls: 2.0822  decode.d1.loss_mask: 0.4828  decode.d1.loss_dice: 0.5318  decode.d2.loss_cls: 1.7659  decode.d2.loss_mask: 0.4400  decode.d2.loss_dice: 0.4941  decode.d3.loss_cls: 1.8667  decode.d3.loss_mask: 0.4481  decode.d3.loss_dice: 0.5280  decode.d4.loss_cls: 1.9331  decode.d4.loss_mask: 0.4444  decode.d4.loss_dice: 0.5070  decode.d5.loss_cls: 1.8423  decode.d5.loss_mask: 0.4521  decode.d5.loss_dice: 0.4978  decode.d6.loss_cls: 1.8938  decode.d6.loss_mask: 0.4845  decode.d6.loss_dice: 0.5057  decode.d7.loss_cls: 1.8328  decode.d7.loss_mask: 0.4807  decode.d7.loss_dice: 0.5030  decode.d8.loss_cls: 1.7627  decode.d8.loss_mask: 0.4904  decode.d8.loss_dice: 0.5179
07/29 23:08:44 - mmengine - INFO - Iter(train) [ 1500/80000]  base_lr: 9.8312e-05 lr: 9.8312e-06  eta: 10:00:24  time: 0.4621  data_time: 0.0108  memory: 5245  grad_norm: 153.2426  loss: 29.2900  decode.loss_cls: 1.8475  decode.loss_mask: 0.3939  decode.loss_dice: 0.5145  decode.d0.loss_cls: 4.4224  decode.d0.loss_mask: 0.4938  decode.d0.loss_dice: 0.6858  decode.d1.loss_cls: 1.8928  decode.d1.loss_mask: 0.3510  decode.d1.loss_dice: 0.4546  decode.d2.loss_cls: 1.7521  decode.d2.loss_mask: 0.3625  decode.d2.loss_dice: 0.4260  decode.d3.loss_cls: 1.7930  decode.d3.loss_mask: 0.3564  decode.d3.loss_dice: 0.4387  decode.d4.loss_cls: 1.8024  decode.d4.loss_mask: 0.3561  decode.d4.loss_dice: 0.4352  decode.d5.loss_cls: 1.7977  decode.d5.loss_mask: 0.3454  decode.d5.loss_dice: 0.4494  decode.d6.loss_cls: 1.7617  decode.d6.loss_mask: 0.3550  decode.d6.loss_dice: 0.4717  decode.d7.loss_cls: 1.8451  decode.d7.loss_mask: 0.3634  decode.d7.loss_dice: 0.5045  decode.d8.loss_cls: 1.7700  decode.d8.loss_mask: 0.3773  decode.d8.loss_dice: 0.4700
07/29 23:09:07 - mmengine - INFO - Iter(train) [ 1550/80000]  base_lr: 9.8256e-05 lr: 9.8256e-06  eta: 9:59:49  time: 0.4510  data_time: 0.0102  memory: 5245  grad_norm: 233.7298  loss: 25.7842  decode.loss_cls: 1.4361  decode.loss_mask: 0.3840  decode.loss_dice: 0.4202  decode.d0.loss_cls: 4.0105  decode.d0.loss_mask: 0.4757  decode.d0.loss_dice: 0.6174  decode.d1.loss_cls: 1.6398  decode.d1.loss_mask: 0.4250  decode.d1.loss_dice: 0.4590  decode.d2.loss_cls: 1.3898  decode.d2.loss_mask: 0.4547  decode.d2.loss_dice: 0.4574  decode.d3.loss_cls: 1.3848  decode.d3.loss_mask: 0.4152  decode.d3.loss_dice: 0.4307  decode.d4.loss_cls: 1.4219  decode.d4.loss_mask: 0.4482  decode.d4.loss_dice: 0.4495  decode.d5.loss_cls: 1.4429  decode.d5.loss_mask: 0.4068  decode.d5.loss_dice: 0.4221  decode.d6.loss_cls: 1.4252  decode.d6.loss_mask: 0.3974  decode.d6.loss_dice: 0.4118  decode.d7.loss_cls: 1.4200  decode.d7.loss_mask: 0.4056  decode.d7.loss_dice: 0.4479  decode.d8.loss_cls: 1.4270  decode.d8.loss_mask: 0.4096  decode.d8.loss_dice: 0.4481
07/29 23:09:30 - mmengine - INFO - Iter(train) [ 1600/80000]  base_lr: 9.8199e-05 lr: 9.8199e-06  eta: 9:59:23  time: 0.4604  data_time: 0.0108  memory: 5307  grad_norm: 178.3728  loss: 28.3463  decode.loss_cls: 1.6825  decode.loss_mask: 0.4068  decode.loss_dice: 0.4605  decode.d0.loss_cls: 4.0102  decode.d0.loss_mask: 0.5073  decode.d0.loss_dice: 0.6574  decode.d1.loss_cls: 1.7888  decode.d1.loss_mask: 0.4337  decode.d1.loss_dice: 0.5045  decode.d2.loss_cls: 1.5804  decode.d2.loss_mask: 0.4268  decode.d2.loss_dice: 0.4928  decode.d3.loss_cls: 1.5686  decode.d3.loss_mask: 0.4395  decode.d3.loss_dice: 0.4776  decode.d4.loss_cls: 1.6845  decode.d4.loss_mask: 0.4269  decode.d4.loss_dice: 0.4657  decode.d5.loss_cls: 1.6447  decode.d5.loss_mask: 0.4214  decode.d5.loss_dice: 0.4597  decode.d6.loss_cls: 1.7069  decode.d6.loss_mask: 0.4485  decode.d6.loss_dice: 0.4677  decode.d7.loss_cls: 1.6422  decode.d7.loss_mask: 0.4499  decode.d7.loss_dice: 0.5021  decode.d8.loss_cls: 1.6980  decode.d8.loss_mask: 0.4108  decode.d8.loss_dice: 0.4800
07/29 23:09:52 - mmengine - INFO - Iter(train) [ 1650/80000]  base_lr: 9.8143e-05 lr: 9.8143e-06  eta: 9:58:49  time: 0.4582  data_time: 0.0107  memory: 5266  grad_norm: 216.8196  loss: 29.4648  decode.loss_cls: 1.5957  decode.loss_mask: 0.5319  decode.loss_dice: 0.6302  decode.d0.loss_cls: 3.9450  decode.d0.loss_mask: 0.5055  decode.d0.loss_dice: 0.6848  decode.d1.loss_cls: 1.8413  decode.d1.loss_mask: 0.4303  decode.d1.loss_dice: 0.5422  decode.d2.loss_cls: 1.6540  decode.d2.loss_mask: 0.4452  decode.d2.loss_dice: 0.5547  decode.d3.loss_cls: 1.5028  decode.d3.loss_mask: 0.4792  decode.d3.loss_dice: 0.5767  decode.d4.loss_cls: 1.5607  decode.d4.loss_mask: 0.4812  decode.d4.loss_dice: 0.5975  decode.d5.loss_cls: 1.6529  decode.d5.loss_mask: 0.4893  decode.d5.loss_dice: 0.6485  decode.d6.loss_cls: 1.5619  decode.d6.loss_mask: 0.4951  decode.d6.loss_dice: 0.6212  decode.d7.loss_cls: 1.5677  decode.d7.loss_mask: 0.5338  decode.d7.loss_dice: 0.6369  decode.d8.loss_cls: 1.5967  decode.d8.loss_mask: 0.4894  decode.d8.loss_dice: 0.6125
07/29 23:10:15 - mmengine - INFO - Iter(train) [ 1700/80000]  base_lr: 9.8087e-05 lr: 9.8087e-06  eta: 9:58:20  time: 0.4554  data_time: 0.0103  memory: 5244  grad_norm: 118.2903  loss: 27.6961  decode.loss_cls: 1.6566  decode.loss_mask: 0.4214  decode.loss_dice: 0.4100  decode.d0.loss_cls: 3.8045  decode.d0.loss_mask: 0.4618  decode.d0.loss_dice: 0.5555  decode.d1.loss_cls: 1.8546  decode.d1.loss_mask: 0.4715  decode.d1.loss_dice: 0.4444  decode.d2.loss_cls: 1.5987  decode.d2.loss_mask: 0.4374  decode.d2.loss_dice: 0.4503  decode.d3.loss_cls: 1.6220  decode.d3.loss_mask: 0.4386  decode.d3.loss_dice: 0.4376  decode.d4.loss_cls: 1.6334  decode.d4.loss_mask: 0.4367  decode.d4.loss_dice: 0.4340  decode.d5.loss_cls: 1.6611  decode.d5.loss_mask: 0.4456  decode.d5.loss_dice: 0.4619  decode.d6.loss_cls: 1.6419  decode.d6.loss_mask: 0.4103  decode.d6.loss_dice: 0.4128  decode.d7.loss_cls: 1.6971  decode.d7.loss_mask: 0.4297  decode.d7.loss_dice: 0.4277  decode.d8.loss_cls: 1.7139  decode.d8.loss_mask: 0.4182  decode.d8.loss_dice: 0.4067
07/29 23:10:38 - mmengine - INFO - Iter(train) [ 1750/80000]  base_lr: 9.8030e-05 lr: 9.8030e-06  eta: 9:57:47  time: 0.4506  data_time: 0.0101  memory: 5244  grad_norm: 161.7921  loss: 25.6822  decode.loss_cls: 1.5846  decode.loss_mask: 0.3842  decode.loss_dice: 0.3926  decode.d0.loss_cls: 3.7323  decode.d0.loss_mask: 0.3646  decode.d0.loss_dice: 0.4909  decode.d1.loss_cls: 1.8871  decode.d1.loss_mask: 0.3618  decode.d1.loss_dice: 0.4114  decode.d2.loss_cls: 1.5893  decode.d2.loss_mask: 0.3497  decode.d2.loss_dice: 0.3840  decode.d3.loss_cls: 1.5577  decode.d3.loss_mask: 0.3368  decode.d3.loss_dice: 0.3720  decode.d4.loss_cls: 1.6049  decode.d4.loss_mask: 0.3179  decode.d4.loss_dice: 0.3586  decode.d5.loss_cls: 1.5979  decode.d5.loss_mask: 0.3225  decode.d5.loss_dice: 0.3284  decode.d6.loss_cls: 1.6298  decode.d6.loss_mask: 0.3174  decode.d6.loss_dice: 0.3310  decode.d7.loss_cls: 1.6477  decode.d7.loss_mask: 0.3291  decode.d7.loss_dice: 0.3491  decode.d8.loss_cls: 1.6441  decode.d8.loss_mask: 0.3342  decode.d8.loss_dice: 0.3705
07/29 23:11:00 - mmengine - INFO - Iter(train) [ 1800/80000]  base_lr: 9.7974e-05 lr: 9.7974e-06  eta: 9:57:12  time: 0.4520  data_time: 0.0104  memory: 5266  grad_norm: 235.6193  loss: 30.4397  decode.loss_cls: 1.8178  decode.loss_mask: 0.4806  decode.loss_dice: 0.5937  decode.d0.loss_cls: 3.5876  decode.d0.loss_mask: 0.5508  decode.d0.loss_dice: 0.6751  decode.d1.loss_cls: 1.8360  decode.d1.loss_mask: 0.5362  decode.d1.loss_dice: 0.6151  decode.d2.loss_cls: 1.7097  decode.d2.loss_mask: 0.5212  decode.d2.loss_dice: 0.5866  decode.d3.loss_cls: 1.7035  decode.d3.loss_mask: 0.5246  decode.d3.loss_dice: 0.5277  decode.d4.loss_cls: 1.7182  decode.d4.loss_mask: 0.5036  decode.d4.loss_dice: 0.5386  decode.d5.loss_cls: 1.7535  decode.d5.loss_mask: 0.5414  decode.d5.loss_dice: 0.5291  decode.d6.loss_cls: 1.7923  decode.d6.loss_mask: 0.5315  decode.d6.loss_dice: 0.5538  decode.d7.loss_cls: 1.8218  decode.d7.loss_mask: 0.4723  decode.d7.loss_dice: 0.5591  decode.d8.loss_cls: 1.8293  decode.d8.loss_mask: 0.4574  decode.d8.loss_dice: 0.5715
07/29 23:11:23 - mmengine - INFO - Iter(train) [ 1850/80000]  base_lr: 9.7917e-05 lr: 9.7917e-06  eta: 9:56:37  time: 0.4516  data_time: 0.0103  memory: 5245  grad_norm: 157.5939  loss: 25.2418  decode.loss_cls: 1.3430  decode.loss_mask: 0.4768  decode.loss_dice: 0.4141  decode.d0.loss_cls: 3.3807  decode.d0.loss_mask: 0.5413  decode.d0.loss_dice: 0.5598  decode.d1.loss_cls: 1.5774  decode.d1.loss_mask: 0.4923  decode.d1.loss_dice: 0.4359  decode.d2.loss_cls: 1.3313  decode.d2.loss_mask: 0.4898  decode.d2.loss_dice: 0.4008  decode.d3.loss_cls: 1.3424  decode.d3.loss_mask: 0.4776  decode.d3.loss_dice: 0.4057  decode.d4.loss_cls: 1.3967  decode.d4.loss_mask: 0.4817  decode.d4.loss_dice: 0.4054  decode.d5.loss_cls: 1.4164  decode.d5.loss_mask: 0.4924  decode.d5.loss_dice: 0.4283  decode.d6.loss_cls: 1.4240  decode.d6.loss_mask: 0.4820  decode.d6.loss_dice: 0.4188  decode.d7.loss_cls: 1.4339  decode.d7.loss_mask: 0.4799  decode.d7.loss_dice: 0.4223  decode.d8.loss_cls: 1.3832  decode.d8.loss_mask: 0.4798  decode.d8.loss_dice: 0.4281
07/29 23:11:46 - mmengine - INFO - Iter(train) [ 1900/80000]  base_lr: 9.7861e-05 lr: 9.7861e-06  eta: 9:56:02  time: 0.4496  data_time: 0.0102  memory: 5281  grad_norm: 101.3072  loss: 23.5761  decode.loss_cls: 1.4178  decode.loss_mask: 0.3000  decode.loss_dice: 0.3625  decode.d0.loss_cls: 3.4214  decode.d0.loss_mask: 0.2998  decode.d0.loss_dice: 0.4251  decode.d1.loss_cls: 1.7787  decode.d1.loss_mask: 0.2966  decode.d1.loss_dice: 0.3659  decode.d2.loss_cls: 1.5051  decode.d2.loss_mask: 0.2935  decode.d2.loss_dice: 0.3673  decode.d3.loss_cls: 1.4640  decode.d3.loss_mask: 0.3005  decode.d3.loss_dice: 0.3688  decode.d4.loss_cls: 1.4573  decode.d4.loss_mask: 0.2976  decode.d4.loss_dice: 0.3310  decode.d5.loss_cls: 1.4707  decode.d5.loss_mask: 0.3046  decode.d5.loss_dice: 0.3751  decode.d6.loss_cls: 1.5077  decode.d6.loss_mask: 0.2998  decode.d6.loss_dice: 0.3493  decode.d7.loss_cls: 1.4364  decode.d7.loss_mask: 0.2975  decode.d7.loss_dice: 0.3664  decode.d8.loss_cls: 1.4325  decode.d8.loss_mask: 0.3026  decode.d8.loss_dice: 0.3806
07/29 23:12:08 - mmengine - INFO - Iter(train) [ 1950/80000]  base_lr: 9.7805e-05 lr: 9.7805e-06  eta: 9:55:25  time: 0.4523  data_time: 0.0102  memory: 5266  grad_norm: 178.6178  loss: 27.2199  decode.loss_cls: 1.5778  decode.loss_mask: 0.4194  decode.loss_dice: 0.4067  decode.d0.loss_cls: 3.3079  decode.d0.loss_mask: 0.5342  decode.d0.loss_dice: 0.5671  decode.d1.loss_cls: 1.8093  decode.d1.loss_mask: 0.4147  decode.d1.loss_dice: 0.4283  decode.d2.loss_cls: 1.6862  decode.d2.loss_mask: 0.4374  decode.d2.loss_dice: 0.4181  decode.d3.loss_cls: 1.6604  decode.d3.loss_mask: 0.4052  decode.d3.loss_dice: 0.3920  decode.d4.loss_cls: 1.7629  decode.d4.loss_mask: 0.4052  decode.d4.loss_dice: 0.4117  decode.d5.loss_cls: 1.6864  decode.d5.loss_mask: 0.4156  decode.d5.loss_dice: 0.4111  decode.d6.loss_cls: 1.8079  decode.d6.loss_mask: 0.4169  decode.d6.loss_dice: 0.4015  decode.d7.loss_cls: 1.7540  decode.d7.loss_mask: 0.3919  decode.d7.loss_dice: 0.4027  decode.d8.loss_cls: 1.6905  decode.d8.loss_mask: 0.3855  decode.d8.loss_dice: 0.4114
07/29 23:12:31 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250729_225707
07/29 23:12:31 - mmengine - INFO - Iter(train) [ 2000/80000]  base_lr: 9.7748e-05 lr: 9.7748e-06  eta: 9:54:46  time: 0.4530  data_time: 0.0100  memory: 5266  grad_norm: 162.8570  loss: 26.0531  decode.loss_cls: 1.6343  decode.loss_mask: 0.3668  decode.loss_dice: 0.3995  decode.d0.loss_cls: 3.2290  decode.d0.loss_mask: 0.4044  decode.d0.loss_dice: 0.5270  decode.d1.loss_cls: 1.7550  decode.d1.loss_mask: 0.3819  decode.d1.loss_dice: 0.4027  decode.d2.loss_cls: 1.6647  decode.d2.loss_mask: 0.3669  decode.d2.loss_dice: 0.4193  decode.d3.loss_cls: 1.5457  decode.d3.loss_mask: 0.3661  decode.d3.loss_dice: 0.3785  decode.d4.loss_cls: 1.5916  decode.d4.loss_mask: 0.3777  decode.d4.loss_dice: 0.3823  decode.d5.loss_cls: 1.6082  decode.d5.loss_mask: 0.3829  decode.d5.loss_dice: 0.4327  decode.d6.loss_cls: 1.6850  decode.d6.loss_mask: 0.3597  decode.d6.loss_dice: 0.3820  decode.d7.loss_cls: 1.7621  decode.d7.loss_mask: 0.3644  decode.d7.loss_dice: 0.4030  decode.d8.loss_cls: 1.7045  decode.d8.loss_mask: 0.3723  decode.d8.loss_dice: 0.4027
07/29 23:12:54 - mmengine - INFO - Iter(train) [ 2050/80000]  base_lr: 9.7692e-05 lr: 9.7692e-06  eta: 9:54:27  time: 0.4565  data_time: 0.0104  memory: 5244  grad_norm: 163.8398  loss: 24.2275  decode.loss_cls: 1.3455  decode.loss_mask: 0.3729  decode.loss_dice: 0.4752  decode.d0.loss_cls: 2.9540  decode.d0.loss_mask: 0.3612  decode.d0.loss_dice: 0.5606  decode.d1.loss_cls: 1.6570  decode.d1.loss_mask: 0.3554  decode.d1.loss_dice: 0.4307  decode.d2.loss_cls: 1.4149  decode.d2.loss_mask: 0.3625  decode.d2.loss_dice: 0.4579  decode.d3.loss_cls: 1.3587  decode.d3.loss_mask: 0.3526  decode.d3.loss_dice: 0.4811  decode.d4.loss_cls: 1.4008  decode.d4.loss_mask: 0.3611  decode.d4.loss_dice: 0.4622  decode.d5.loss_cls: 1.3927  decode.d5.loss_mask: 0.3633  decode.d5.loss_dice: 0.4795  decode.d6.loss_cls: 1.3841  decode.d6.loss_mask: 0.3641  decode.d6.loss_dice: 0.4571  decode.d7.loss_cls: 1.4913  decode.d7.loss_mask: 0.3674  decode.d7.loss_dice: 0.4538  decode.d8.loss_cls: 1.5064  decode.d8.loss_mask: 0.3588  decode.d8.loss_dice: 0.4449
07/29 23:13:17 - mmengine - INFO - Iter(train) [ 2100/80000]  base_lr: 9.7635e-05 lr: 9.7635e-06  eta: 9:54:03  time: 0.4597  data_time: 0.0106  memory: 5227  grad_norm: 140.7623  loss: 22.9602  decode.loss_cls: 1.4090  decode.loss_mask: 0.3760  decode.loss_dice: 0.3648  decode.d0.loss_cls: 2.8857  decode.d0.loss_mask: 0.3488  decode.d0.loss_dice: 0.4882  decode.d1.loss_cls: 1.6106  decode.d1.loss_mask: 0.3337  decode.d1.loss_dice: 0.3529  decode.d2.loss_cls: 1.4137  decode.d2.loss_mask: 0.3080  decode.d2.loss_dice: 0.3587  decode.d3.loss_cls: 1.3877  decode.d3.loss_mask: 0.3479  decode.d3.loss_dice: 0.3630  decode.d4.loss_cls: 1.4655  decode.d4.loss_mask: 0.3109  decode.d4.loss_dice: 0.3672  decode.d5.loss_cls: 1.4069  decode.d5.loss_mask: 0.3470  decode.d5.loss_dice: 0.3555  decode.d6.loss_cls: 1.5224  decode.d6.loss_mask: 0.3116  decode.d6.loss_dice: 0.3663  decode.d7.loss_cls: 1.4578  decode.d7.loss_mask: 0.3007  decode.d7.loss_dice: 0.3657  decode.d8.loss_cls: 1.3329  decode.d8.loss_mask: 0.3401  decode.d8.loss_dice: 0.3610
07/29 23:13:39 - mmengine - INFO - Iter(train) [ 2150/80000]  base_lr: 9.7579e-05 lr: 9.7579e-06  eta: 9:53:39  time: 0.4540  data_time: 0.0104  memory: 5245  grad_norm: 237.8163  loss: 22.4908  decode.loss_cls: 1.2382  decode.loss_mask: 0.4592  decode.loss_dice: 0.3554  decode.d0.loss_cls: 2.7207  decode.d0.loss_mask: 0.5280  decode.d0.loss_dice: 0.4767  decode.d1.loss_cls: 1.4415  decode.d1.loss_mask: 0.4871  decode.d1.loss_dice: 0.3518  decode.d2.loss_cls: 1.2601  decode.d2.loss_mask: 0.4511  decode.d2.loss_dice: 0.3533  decode.d3.loss_cls: 1.2309  decode.d3.loss_mask: 0.4500  decode.d3.loss_dice: 0.3620  decode.d4.loss_cls: 1.2805  decode.d4.loss_mask: 0.4463  decode.d4.loss_dice: 0.3523  decode.d5.loss_cls: 1.1866  decode.d5.loss_mask: 0.4710  decode.d5.loss_dice: 0.3535  decode.d6.loss_cls: 1.2801  decode.d6.loss_mask: 0.4600  decode.d6.loss_dice: 0.3513  decode.d7.loss_cls: 1.3323  decode.d7.loss_mask: 0.4533  decode.d7.loss_dice: 0.3300  decode.d8.loss_cls: 1.2270  decode.d8.loss_mask: 0.4629  decode.d8.loss_dice: 0.3377
07/29 23:14:02 - mmengine - INFO - Iter(train) [ 2200/80000]  base_lr: 9.7523e-05 lr: 9.7523e-06  eta: 9:53:12  time: 0.4559  data_time: 0.0104  memory: 5266  grad_norm: 147.3076  loss: 24.4580  decode.loss_cls: 1.4632  decode.loss_mask: 0.3768  decode.loss_dice: 0.4309  decode.d0.loss_cls: 2.8191  decode.d0.loss_mask: 0.4184  decode.d0.loss_dice: 0.5595  decode.d1.loss_cls: 1.7239  decode.d1.loss_mask: 0.3563  decode.d1.loss_dice: 0.4471  decode.d2.loss_cls: 1.5390  decode.d2.loss_mask: 0.3538  decode.d2.loss_dice: 0.4018  decode.d3.loss_cls: 1.4199  decode.d3.loss_mask: 0.3512  decode.d3.loss_dice: 0.4232  decode.d4.loss_cls: 1.3953  decode.d4.loss_mask: 0.3741  decode.d4.loss_dice: 0.4406  decode.d5.loss_cls: 1.4783  decode.d5.loss_mask: 0.3945  decode.d5.loss_dice: 0.4301  decode.d6.loss_cls: 1.4694  decode.d6.loss_mask: 0.4121  decode.d6.loss_dice: 0.4356  decode.d7.loss_cls: 1.4479  decode.d7.loss_mask: 0.4232  decode.d7.loss_dice: 0.4290  decode.d8.loss_cls: 1.4447  decode.d8.loss_mask: 0.3920  decode.d8.loss_dice: 0.4070
07/29 23:14:25 - mmengine - INFO - Iter(train) [ 2250/80000]  base_lr: 9.7466e-05 lr: 9.7466e-06  eta: 9:52:47  time: 0.4539  data_time: 0.0103  memory: 5307  grad_norm: 121.6778  loss: 22.9048  decode.loss_cls: 1.2140  decode.loss_mask: 0.4168  decode.loss_dice: 0.4875  decode.d0.loss_cls: 2.5807  decode.d0.loss_mask: 0.4906  decode.d0.loss_dice: 0.5815  decode.d1.loss_cls: 1.3840  decode.d1.loss_mask: 0.4423  decode.d1.loss_dice: 0.4609  decode.d2.loss_cls: 1.1876  decode.d2.loss_mask: 0.4268  decode.d2.loss_dice: 0.4937  decode.d3.loss_cls: 1.2322  decode.d3.loss_mask: 0.4156  decode.d3.loss_dice: 0.4750  decode.d4.loss_cls: 1.1593  decode.d4.loss_mask: 0.4207  decode.d4.loss_dice: 0.4721  decode.d5.loss_cls: 1.2586  decode.d5.loss_mask: 0.4161  decode.d5.loss_dice: 0.4645  decode.d6.loss_cls: 1.2522  decode.d6.loss_mask: 0.4210  decode.d6.loss_dice: 0.4753  decode.d7.loss_cls: 1.2630  decode.d7.loss_mask: 0.4173  decode.d7.loss_dice: 0.4621  decode.d8.loss_cls: 1.2444  decode.d8.loss_mask: 0.4087  decode.d8.loss_dice: 0.4804
07/29 23:14:48 - mmengine - INFO - Iter(train) [ 2300/80000]  base_lr: 9.7410e-05 lr: 9.7410e-06  eta: 9:52:13  time: 0.4561  data_time: 0.0103  memory: 5266  grad_norm: 123.4051  loss: 24.0530  decode.loss_cls: 1.4116  decode.loss_mask: 0.3334  decode.loss_dice: 0.4312  decode.d0.loss_cls: 2.7073  decode.d0.loss_mask: 0.3403  decode.d0.loss_dice: 0.5426  decode.d1.loss_cls: 1.8562  decode.d1.loss_mask: 0.3070  decode.d1.loss_dice: 0.3798  decode.d2.loss_cls: 1.5589  decode.d2.loss_mask: 0.2933  decode.d2.loss_dice: 0.4052  decode.d3.loss_cls: 1.5447  decode.d3.loss_mask: 0.2989  decode.d3.loss_dice: 0.3863  decode.d4.loss_cls: 1.5543  decode.d4.loss_mask: 0.2937  decode.d4.loss_dice: 0.3866  decode.d5.loss_cls: 1.4886  decode.d5.loss_mask: 0.3122  decode.d5.loss_dice: 0.4074  decode.d6.loss_cls: 1.5035  decode.d6.loss_mask: 0.3406  decode.d6.loss_dice: 0.4285  decode.d7.loss_cls: 1.5101  decode.d7.loss_mask: 0.3519  decode.d7.loss_dice: 0.4265  decode.d8.loss_cls: 1.4892  decode.d8.loss_mask: 0.3418  decode.d8.loss_dice: 0.4213
07/29 23:15:10 - mmengine - INFO - Iter(train) [ 2350/80000]  base_lr: 9.7353e-05 lr: 9.7353e-06  eta: 9:51:50  time: 0.4600  data_time: 0.0109  memory: 5264  grad_norm: 198.5817  loss: 23.9405  decode.loss_cls: 1.2772  decode.loss_mask: 0.4455  decode.loss_dice: 0.5259  decode.d0.loss_cls: 2.6844  decode.d0.loss_mask: 0.4489  decode.d0.loss_dice: 0.6017  decode.d1.loss_cls: 1.4817  decode.d1.loss_mask: 0.4487  decode.d1.loss_dice: 0.5311  decode.d2.loss_cls: 1.3332  decode.d2.loss_mask: 0.4283  decode.d2.loss_dice: 0.5066  decode.d3.loss_cls: 1.2840  decode.d3.loss_mask: 0.4143  decode.d3.loss_dice: 0.4779  decode.d4.loss_cls: 1.3287  decode.d4.loss_mask: 0.3934  decode.d4.loss_dice: 0.4595  decode.d5.loss_cls: 1.2995  decode.d5.loss_mask: 0.3905  decode.d5.loss_dice: 0.4725  decode.d6.loss_cls: 1.3973  decode.d6.loss_mask: 0.3970  decode.d6.loss_dice: 0.4864  decode.d7.loss_cls: 1.3828  decode.d7.loss_mask: 0.3934  decode.d7.loss_dice: 0.4684  decode.d8.loss_cls: 1.2611  decode.d8.loss_mask: 0.4096  decode.d8.loss_dice: 0.5110
07/29 23:15:33 - mmengine - INFO - Iter(train) [ 2400/80000]  base_lr: 9.7297e-05 lr: 9.7297e-06  eta: 9:51:27  time: 0.4582  data_time: 0.0106  memory: 5244  grad_norm: 139.0168  loss: 20.9967  decode.loss_cls: 1.3037  decode.loss_mask: 0.2976  decode.loss_dice: 0.3562  decode.d0.loss_cls: 2.3432  decode.d0.loss_mask: 0.3738  decode.d0.loss_dice: 0.4496  decode.d1.loss_cls: 1.3923  decode.d1.loss_mask: 0.3189  decode.d1.loss_dice: 0.3749  decode.d2.loss_cls: 1.2506  decode.d2.loss_mask: 0.3477  decode.d2.loss_dice: 0.3922  decode.d3.loss_cls: 1.3039  decode.d3.loss_mask: 0.3342  decode.d3.loss_dice: 0.3671  decode.d4.loss_cls: 1.2969  decode.d4.loss_mask: 0.3267  decode.d4.loss_dice: 0.3594  decode.d5.loss_cls: 1.2882  decode.d5.loss_mask: 0.2868  decode.d5.loss_dice: 0.3438  decode.d6.loss_cls: 1.2356  decode.d6.loss_mask: 0.3114  decode.d6.loss_dice: 0.3580  decode.d7.loss_cls: 1.3149  decode.d7.loss_mask: 0.3295  decode.d7.loss_dice: 0.3548  decode.d8.loss_cls: 1.3256  decode.d8.loss_mask: 0.3016  decode.d8.loss_dice: 0.3577
07/29 23:15:56 - mmengine - INFO - Iter(train) [ 2450/80000]  base_lr: 9.7241e-05 lr: 9.7241e-06  eta: 9:51:05  time: 0.4569  data_time: 0.0106  memory: 5229  grad_norm: 201.8985  loss: 23.5198  decode.loss_cls: 1.1895  decode.loss_mask: 0.4605  decode.loss_dice: 0.4835  decode.d0.loss_cls: 2.5135  decode.d0.loss_mask: 0.4746  decode.d0.loss_dice: 0.5743  decode.d1.loss_cls: 1.5125  decode.d1.loss_mask: 0.4394  decode.d1.loss_dice: 0.4624  decode.d2.loss_cls: 1.2756  decode.d2.loss_mask: 0.4331  decode.d2.loss_dice: 0.4750  decode.d3.loss_cls: 1.2919  decode.d3.loss_mask: 0.4335  decode.d3.loss_dice: 0.4771  decode.d4.loss_cls: 1.3604  decode.d4.loss_mask: 0.4403  decode.d4.loss_dice: 0.4797  decode.d5.loss_cls: 1.2805  decode.d5.loss_mask: 0.4304  decode.d5.loss_dice: 0.4553  decode.d6.loss_cls: 1.2255  decode.d6.loss_mask: 0.4427  decode.d6.loss_dice: 0.5008  decode.d7.loss_cls: 1.2442  decode.d7.loss_mask: 0.4626  decode.d7.loss_dice: 0.5093  decode.d8.loss_cls: 1.2112  decode.d8.loss_mask: 0.4689  decode.d8.loss_dice: 0.5116
07/29 23:16:19 - mmengine - INFO - Iter(train) [ 2500/80000]  base_lr: 9.7184e-05 lr: 9.7184e-06  eta: 9:50:44  time: 0.4605  data_time: 0.0106  memory: 5281  grad_norm: 179.9398  loss: 24.7805  decode.loss_cls: 1.4633  decode.loss_mask: 0.3430  decode.loss_dice: 0.5488  decode.d0.loss_cls: 2.6774  decode.d0.loss_mask: 0.3533  decode.d0.loss_dice: 0.6509  decode.d1.loss_cls: 1.5406  decode.d1.loss_mask: 0.3321  decode.d1.loss_dice: 0.5807  decode.d2.loss_cls: 1.4557  decode.d2.loss_mask: 0.3194  decode.d2.loss_dice: 0.5376  decode.d3.loss_cls: 1.3888  decode.d3.loss_mask: 0.3320  decode.d3.loss_dice: 0.5310  decode.d4.loss_cls: 1.3763  decode.d4.loss_mask: 0.3293  decode.d4.loss_dice: 0.5454  decode.d5.loss_cls: 1.4406  decode.d5.loss_mask: 0.3267  decode.d5.loss_dice: 0.5439  decode.d6.loss_cls: 1.4487  decode.d6.loss_mask: 0.3228  decode.d6.loss_dice: 0.5352  decode.d7.loss_cls: 1.5510  decode.d7.loss_mask: 0.3346  decode.d7.loss_dice: 0.5285  decode.d8.loss_cls: 1.5407  decode.d8.loss_mask: 0.3307  decode.d8.loss_dice: 0.5714
07/29 23:16:42 - mmengine - INFO - Iter(train) [ 2550/80000]  base_lr: 9.7128e-05 lr: 9.7128e-06  eta: 9:50:24  time: 0.4581  data_time: 0.0108  memory: 5266  grad_norm: 149.8729  loss: 23.3804  decode.loss_cls: 1.3748  decode.loss_mask: 0.2935  decode.loss_dice: 0.4291  decode.d0.loss_cls: 2.5381  decode.d0.loss_mask: 0.3002  decode.d0.loss_dice: 0.5485  decode.d1.loss_cls: 1.7895  decode.d1.loss_mask: 0.2744  decode.d1.loss_dice: 0.4299  decode.d2.loss_cls: 1.5412  decode.d2.loss_mask: 0.2741  decode.d2.loss_dice: 0.4257  decode.d3.loss_cls: 1.4486  decode.d3.loss_mask: 0.2626  decode.d3.loss_dice: 0.4116  decode.d4.loss_cls: 1.5372  decode.d4.loss_mask: 0.2454  decode.d4.loss_dice: 0.4238  decode.d5.loss_cls: 1.4488  decode.d5.loss_mask: 0.2510  decode.d5.loss_dice: 0.4267  decode.d6.loss_cls: 1.3727  decode.d6.loss_mask: 0.2815  decode.d6.loss_dice: 0.4672  decode.d7.loss_cls: 1.5575  decode.d7.loss_mask: 0.3151  decode.d7.loss_dice: 0.4518  decode.d8.loss_cls: 1.5363  decode.d8.loss_mask: 0.2869  decode.d8.loss_dice: 0.4369
07/29 23:17:05 - mmengine - INFO - Iter(train) [ 2600/80000]  base_lr: 9.7071e-05 lr: 9.7071e-06  eta: 9:50:03  time: 0.4567  data_time: 0.0105  memory: 5283  grad_norm: 170.8938  loss: 24.8368  decode.loss_cls: 1.3201  decode.loss_mask: 0.5027  decode.loss_dice: 0.5594  decode.d0.loss_cls: 2.2375  decode.d0.loss_mask: 0.5383  decode.d0.loss_dice: 0.6659  decode.d1.loss_cls: 1.6121  decode.d1.loss_mask: 0.4782  decode.d1.loss_dice: 0.5339  decode.d2.loss_cls: 1.4458  decode.d2.loss_mask: 0.4481  decode.d2.loss_dice: 0.5335  decode.d3.loss_cls: 1.3390  decode.d3.loss_mask: 0.4466  decode.d3.loss_dice: 0.4826  decode.d4.loss_cls: 1.3272  decode.d4.loss_mask: 0.4670  decode.d4.loss_dice: 0.5028  decode.d5.loss_cls: 1.3180  decode.d5.loss_mask: 0.5054  decode.d5.loss_dice: 0.5004  decode.d6.loss_cls: 1.2894  decode.d6.loss_mask: 0.4910  decode.d6.loss_dice: 0.5093  decode.d7.loss_cls: 1.3599  decode.d7.loss_mask: 0.4916  decode.d7.loss_dice: 0.5040  decode.d8.loss_cls: 1.3259  decode.d8.loss_mask: 0.5325  decode.d8.loss_dice: 0.5687
07/29 23:17:28 - mmengine - INFO - Iter(train) [ 2650/80000]  base_lr: 9.7015e-05 lr: 9.7015e-06  eta: 9:49:41  time: 0.4597  data_time: 0.0105  memory: 5243  grad_norm: 118.9178  loss: 19.1894  decode.loss_cls: 1.0164  decode.loss_mask: 0.3002  decode.loss_dice: 0.3510  decode.d0.loss_cls: 2.3380  decode.d0.loss_mask: 0.3360  decode.d0.loss_dice: 0.4785  decode.d1.loss_cls: 1.3840  decode.d1.loss_mask: 0.2970  decode.d1.loss_dice: 0.3488  decode.d2.loss_cls: 1.1473  decode.d2.loss_mask: 0.2964  decode.d2.loss_dice: 0.3579  decode.d3.loss_cls: 1.0471  decode.d3.loss_mask: 0.3005  decode.d3.loss_dice: 0.3659  decode.d4.loss_cls: 1.1074  decode.d4.loss_mask: 0.3074  decode.d4.loss_dice: 0.3706  decode.d5.loss_cls: 1.1379  decode.d5.loss_mask: 0.2958  decode.d5.loss_dice: 0.3409  decode.d6.loss_cls: 1.0620  decode.d6.loss_mask: 0.3047  decode.d6.loss_dice: 0.3675  decode.d7.loss_cls: 1.0891  decode.d7.loss_mask: 0.3051  decode.d7.loss_dice: 0.3684  decode.d8.loss_cls: 1.1120  decode.d8.loss_mask: 0.3023  decode.d8.loss_dice: 0.3530
07/29 23:17:51 - mmengine - INFO - Iter(train) [ 2700/80000]  base_lr: 9.6958e-05 lr: 9.6958e-06  eta: 9:49:18  time: 0.4552  data_time: 0.0105  memory: 5245  grad_norm: 181.1866  loss: 23.6443  decode.loss_cls: 1.4332  decode.loss_mask: 0.3951  decode.loss_dice: 0.3912  decode.d0.loss_cls: 2.3829  decode.d0.loss_mask: 0.4608  decode.d0.loss_dice: 0.4854  decode.d1.loss_cls: 1.7109  decode.d1.loss_mask: 0.3965  decode.d1.loss_dice: 0.3751  decode.d2.loss_cls: 1.5390  decode.d2.loss_mask: 0.4136  decode.d2.loss_dice: 0.3678  decode.d3.loss_cls: 1.4137  decode.d3.loss_mask: 0.4150  decode.d3.loss_dice: 0.3907  decode.d4.loss_cls: 1.3616  decode.d4.loss_mask: 0.4118  decode.d4.loss_dice: 0.3894  decode.d5.loss_cls: 1.3972  decode.d5.loss_mask: 0.3990  decode.d5.loss_dice: 0.3916  decode.d6.loss_cls: 1.4959  decode.d6.loss_mask: 0.3900  decode.d6.loss_dice: 0.3706  decode.d7.loss_cls: 1.4200  decode.d7.loss_mask: 0.4047  decode.d7.loss_dice: 0.3743  decode.d8.loss_cls: 1.4557  decode.d8.loss_mask: 0.4153  decode.d8.loss_dice: 0.3964
07/29 23:18:13 - mmengine - INFO - Iter(train) [ 2750/80000]  base_lr: 9.6902e-05 lr: 9.6902e-06  eta: 9:48:48  time: 0.4537  data_time: 0.0105  memory: 5283  grad_norm: 130.9628  loss: 19.6313  decode.loss_cls: 1.1818  decode.loss_mask: 0.3046  decode.loss_dice: 0.3583  decode.d0.loss_cls: 2.2731  decode.d0.loss_mask: 0.3260  decode.d0.loss_dice: 0.4831  decode.d1.loss_cls: 1.3244  decode.d1.loss_mask: 0.3199  decode.d1.loss_dice: 0.4006  decode.d2.loss_cls: 1.1516  decode.d2.loss_mask: 0.3036  decode.d2.loss_dice: 0.3664  decode.d3.loss_cls: 1.1178  decode.d3.loss_mask: 0.2919  decode.d3.loss_dice: 0.3411  decode.d4.loss_cls: 1.1770  decode.d4.loss_mask: 0.2931  decode.d4.loss_dice: 0.3490  decode.d5.loss_cls: 1.2083  decode.d5.loss_mask: 0.2925  decode.d5.loss_dice: 0.3397  decode.d6.loss_cls: 1.2221  decode.d6.loss_mask: 0.2897  decode.d6.loss_dice: 0.3247  decode.d7.loss_cls: 1.1673  decode.d7.loss_mask: 0.2887  decode.d7.loss_dice: 0.3305  decode.d8.loss_cls: 1.1501  decode.d8.loss_mask: 0.2942  decode.d8.loss_dice: 0.3603
07/29 23:18:36 - mmengine - INFO - Iter(train) [ 2800/80000]  base_lr: 9.6846e-05 lr: 9.6846e-06  eta: 9:48:17  time: 0.4513  data_time: 0.0102  memory: 5266  grad_norm: 194.4316  loss: 21.5046  decode.loss_cls: 1.2840  decode.loss_mask: 0.3507  decode.loss_dice: 0.3391  decode.d0.loss_cls: 2.1884  decode.d0.loss_mask: 0.4943  decode.d0.loss_dice: 0.5495  decode.d1.loss_cls: 1.4543  decode.d1.loss_mask: 0.4017  decode.d1.loss_dice: 0.3983  decode.d2.loss_cls: 1.2369  decode.d2.loss_mask: 0.3690  decode.d2.loss_dice: 0.3718  decode.d3.loss_cls: 1.2287  decode.d3.loss_mask: 0.3570  decode.d3.loss_dice: 0.3646  decode.d4.loss_cls: 1.2977  decode.d4.loss_mask: 0.3576  decode.d4.loss_dice: 0.3742  decode.d5.loss_cls: 1.2699  decode.d5.loss_mask: 0.3591  decode.d5.loss_dice: 0.3667  decode.d6.loss_cls: 1.2843  decode.d6.loss_mask: 0.3643  decode.d6.loss_dice: 0.3715  decode.d7.loss_cls: 1.3155  decode.d7.loss_mask: 0.3592  decode.d7.loss_dice: 0.3537  decode.d8.loss_cls: 1.3178  decode.d8.loss_mask: 0.3608  decode.d8.loss_dice: 0.3638
07/29 23:18:59 - mmengine - INFO - Iter(train) [ 2850/80000]  base_lr: 9.6789e-05 lr: 9.6789e-06  eta: 9:47:47  time: 0.4502  data_time: 0.0098  memory: 5281  grad_norm: 235.2179  loss: 21.7733  decode.loss_cls: 1.2399  decode.loss_mask: 0.3619  decode.loss_dice: 0.4652  decode.d0.loss_cls: 2.1280  decode.d0.loss_mask: 0.3815  decode.d0.loss_dice: 0.5597  decode.d1.loss_cls: 1.6093  decode.d1.loss_mask: 0.3440  decode.d1.loss_dice: 0.4694  decode.d2.loss_cls: 1.2571  decode.d2.loss_mask: 0.3579  decode.d2.loss_dice: 0.4880  decode.d3.loss_cls: 1.2104  decode.d3.loss_mask: 0.3428  decode.d3.loss_dice: 0.4270  decode.d4.loss_cls: 1.2293  decode.d4.loss_mask: 0.3207  decode.d4.loss_dice: 0.4190  decode.d5.loss_cls: 1.2383  decode.d5.loss_mask: 0.3495  decode.d5.loss_dice: 0.4337  decode.d6.loss_cls: 1.3030  decode.d6.loss_mask: 0.3219  decode.d6.loss_dice: 0.4249  decode.d7.loss_cls: 1.3336  decode.d7.loss_mask: 0.3232  decode.d7.loss_dice: 0.4216  decode.d8.loss_cls: 1.2375  decode.d8.loss_mask: 0.3388  decode.d8.loss_dice: 0.4362
07/29 23:19:21 - mmengine - INFO - Iter(train) [ 2900/80000]  base_lr: 9.6733e-05 lr: 9.6733e-06  eta: 9:47:14  time: 0.4506  data_time: 0.0094  memory: 5266  grad_norm: 127.1710  loss: 20.7248  decode.loss_cls: 1.2242  decode.loss_mask: 0.3551  decode.loss_dice: 0.4678  decode.d0.loss_cls: 2.2418  decode.d0.loss_mask: 0.3664  decode.d0.loss_dice: 0.4786  decode.d1.loss_cls: 1.4508  decode.d1.loss_mask: 0.3132  decode.d1.loss_dice: 0.4429  decode.d2.loss_cls: 1.1554  decode.d2.loss_mask: 0.3147  decode.d2.loss_dice: 0.4461  decode.d3.loss_cls: 1.1882  decode.d3.loss_mask: 0.3514  decode.d3.loss_dice: 0.4098  decode.d4.loss_cls: 1.1591  decode.d4.loss_mask: 0.3457  decode.d4.loss_dice: 0.4458  decode.d5.loss_cls: 1.1129  decode.d5.loss_mask: 0.3164  decode.d5.loss_dice: 0.4124  decode.d6.loss_cls: 1.1465  decode.d6.loss_mask: 0.3154  decode.d6.loss_dice: 0.4069  decode.d7.loss_cls: 1.1549  decode.d7.loss_mask: 0.3309  decode.d7.loss_dice: 0.3814  decode.d8.loss_cls: 1.2178  decode.d8.loss_mask: 0.3265  decode.d8.loss_dice: 0.4457
07/29 23:19:44 - mmengine - INFO - Iter(train) [ 2950/80000]  base_lr: 9.6676e-05 lr: 9.6676e-06  eta: 9:46:45  time: 0.4485  data_time: 0.0097  memory: 5230  grad_norm: 300.6509  loss: 25.8912  decode.loss_cls: 1.3230  decode.loss_mask: 0.5078  decode.loss_dice: 0.5120  decode.d0.loss_cls: 2.2015  decode.d0.loss_mask: 0.5542  decode.d0.loss_dice: 0.6013  decode.d1.loss_cls: 1.5519  decode.d1.loss_mask: 0.5811  decode.d1.loss_dice: 0.5334  decode.d2.loss_cls: 1.3120  decode.d2.loss_mask: 0.5955  decode.d2.loss_dice: 0.5313  decode.d3.loss_cls: 1.3602  decode.d3.loss_mask: 0.5941  decode.d3.loss_dice: 0.5884  decode.d4.loss_cls: 1.4686  decode.d4.loss_mask: 0.5408  decode.d4.loss_dice: 0.5603  decode.d5.loss_cls: 1.3993  decode.d5.loss_mask: 0.5356  decode.d5.loss_dice: 0.5476  decode.d6.loss_cls: 1.4221  decode.d6.loss_mask: 0.5822  decode.d6.loss_dice: 0.5448  decode.d7.loss_cls: 1.4824  decode.d7.loss_mask: 0.5175  decode.d7.loss_dice: 0.5238  decode.d8.loss_cls: 1.4332  decode.d8.loss_mask: 0.5040  decode.d8.loss_dice: 0.4814
07/29 23:20:06 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250729_225707
07/29 23:20:06 - mmengine - INFO - Iter(train) [ 3000/80000]  base_lr: 9.6620e-05 lr: 9.6620e-06  eta: 9:46:11  time: 0.4502  data_time: 0.0097  memory: 5266  grad_norm: 123.2151  loss: 18.3374  decode.loss_cls: 0.9989  decode.loss_mask: 0.3039  decode.loss_dice: 0.3603  decode.d0.loss_cls: 2.0798  decode.d0.loss_mask: 0.3656  decode.d0.loss_dice: 0.5313  decode.d1.loss_cls: 1.2471  decode.d1.loss_mask: 0.3241  decode.d1.loss_dice: 0.4234  decode.d2.loss_cls: 0.9692  decode.d2.loss_mask: 0.3137  decode.d2.loss_dice: 0.3884  decode.d3.loss_cls: 0.8921  decode.d3.loss_mask: 0.3187  decode.d3.loss_dice: 0.3922  decode.d4.loss_cls: 0.9494  decode.d4.loss_mask: 0.3207  decode.d4.loss_dice: 0.3855  decode.d5.loss_cls: 0.9004  decode.d5.loss_mask: 0.3227  decode.d5.loss_dice: 0.3896  decode.d6.loss_cls: 1.0642  decode.d6.loss_mask: 0.3114  decode.d6.loss_dice: 0.3640  decode.d7.loss_cls: 1.0271  decode.d7.loss_mask: 0.3174  decode.d7.loss_dice: 0.3727  decode.d8.loss_cls: 1.0288  decode.d8.loss_mask: 0.3174  decode.d8.loss_dice: 0.3573
07/29 23:20:28 - mmengine - INFO - Iter(train) [ 3050/80000]  base_lr: 9.6563e-05 lr: 9.6563e-06  eta: 9:45:38  time: 0.4493  data_time: 0.0102  memory: 5229  grad_norm: 89.4108  loss: 14.6513  decode.loss_cls: 0.7562  decode.loss_mask: 0.2999  decode.loss_dice: 0.3117  decode.d0.loss_cls: 1.6045  decode.d0.loss_mask: 0.3268  decode.d0.loss_dice: 0.3766  decode.d1.loss_cls: 0.8725  decode.d1.loss_mask: 0.2957  decode.d1.loss_dice: 0.3174  decode.d2.loss_cls: 0.7306  decode.d2.loss_mask: 0.2967  decode.d2.loss_dice: 0.3046  decode.d3.loss_cls: 0.7681  decode.d3.loss_mask: 0.2971  decode.d3.loss_dice: 0.3165  decode.d4.loss_cls: 0.7260  decode.d4.loss_mask: 0.3014  decode.d4.loss_dice: 0.3059  decode.d5.loss_cls: 0.6983  decode.d5.loss_mask: 0.2986  decode.d5.loss_dice: 0.3146  decode.d6.loss_cls: 0.7396  decode.d6.loss_mask: 0.2997  decode.d6.loss_dice: 0.3103  decode.d7.loss_cls: 0.7938  decode.d7.loss_mask: 0.3022  decode.d7.loss_dice: 0.3219  decode.d8.loss_cls: 0.7471  decode.d8.loss_mask: 0.2983  decode.d8.loss_dice: 0.3184
07/29 23:20:51 - mmengine - INFO - Iter(train) [ 3100/80000]  base_lr: 9.6507e-05 lr: 9.6507e-06  eta: 9:45:07  time: 0.4518  data_time: 0.0104  memory: 5266  grad_norm: 149.8204  loss: 17.9640  decode.loss_cls: 0.8763  decode.loss_mask: 0.4430  decode.loss_dice: 0.4350  decode.d0.loss_cls: 1.8108  decode.d0.loss_mask: 0.4475  decode.d0.loss_dice: 0.4371  decode.d1.loss_cls: 1.0290  decode.d1.loss_mask: 0.3882  decode.d1.loss_dice: 0.4109  decode.d2.loss_cls: 0.8015  decode.d2.loss_mask: 0.3919  decode.d2.loss_dice: 0.3966  decode.d3.loss_cls: 0.8835  decode.d3.loss_mask: 0.3767  decode.d3.loss_dice: 0.4015  decode.d4.loss_cls: 0.8565  decode.d4.loss_mask: 0.3785  decode.d4.loss_dice: 0.3875  decode.d5.loss_cls: 0.8316  decode.d5.loss_mask: 0.3860  decode.d5.loss_dice: 0.4093  decode.d6.loss_cls: 0.9017  decode.d6.loss_mask: 0.3944  decode.d6.loss_dice: 0.3906  decode.d7.loss_cls: 0.9325  decode.d7.loss_mask: 0.4263  decode.d7.loss_dice: 0.4109  decode.d8.loss_cls: 0.9047  decode.d8.loss_mask: 0.4122  decode.d8.loss_dice: 0.4118
07/29 23:21:13 - mmengine - INFO - Iter(train) [ 3150/80000]  base_lr: 9.6450e-05 lr: 9.6450e-06  eta: 9:44:36  time: 0.4464  data_time: 0.0097  memory: 5281  grad_norm: 157.5818  loss: 16.4741  decode.loss_cls: 0.8730  decode.loss_mask: 0.3320  decode.loss_dice: 0.3870  decode.d0.loss_cls: 1.8093  decode.d0.loss_mask: 0.3104  decode.d0.loss_dice: 0.4541  decode.d1.loss_cls: 1.0333  decode.d1.loss_mask: 0.2968  decode.d1.loss_dice: 0.3740  decode.d2.loss_cls: 0.8016  decode.d2.loss_mask: 0.2852  decode.d2.loss_dice: 0.3684  decode.d3.loss_cls: 0.7553  decode.d3.loss_mask: 0.2790  decode.d3.loss_dice: 0.3958  decode.d4.loss_cls: 0.8261  decode.d4.loss_mask: 0.3062  decode.d4.loss_dice: 0.3864  decode.d5.loss_cls: 0.9106  decode.d5.loss_mask: 0.3105  decode.d5.loss_dice: 0.3928  decode.d6.loss_cls: 0.8985  decode.d6.loss_mask: 0.3037  decode.d6.loss_dice: 0.3591  decode.d7.loss_cls: 0.8531  decode.d7.loss_mask: 0.3041  decode.d7.loss_dice: 0.3671  decode.d8.loss_cls: 0.8617  decode.d8.loss_mask: 0.2802  decode.d8.loss_dice: 0.3590
07/29 23:21:36 - mmengine - INFO - Iter(train) [ 3200/80000]  base_lr: 9.6394e-05 lr: 9.6394e-06  eta: 9:44:05  time: 0.4505  data_time: 0.0104  memory: 5227  grad_norm: 117.0157  loss: 18.2332  decode.loss_cls: 0.9803  decode.loss_mask: 0.3677  decode.loss_dice: 0.4118  decode.d0.loss_cls: 1.7317  decode.d0.loss_mask: 0.3899  decode.d0.loss_dice: 0.5136  decode.d1.loss_cls: 1.0487  decode.d1.loss_mask: 0.3668  decode.d1.loss_dice: 0.4131  decode.d2.loss_cls: 0.9296  decode.d2.loss_mask: 0.3662  decode.d2.loss_dice: 0.4101  decode.d3.loss_cls: 0.9091  decode.d3.loss_mask: 0.3593  decode.d3.loss_dice: 0.3970  decode.d4.loss_cls: 0.8882  decode.d4.loss_mask: 0.3667  decode.d4.loss_dice: 0.4012  decode.d5.loss_cls: 0.9687  decode.d5.loss_mask: 0.3740  decode.d5.loss_dice: 0.4296  decode.d6.loss_cls: 0.9839  decode.d6.loss_mask: 0.3708  decode.d6.loss_dice: 0.4086  decode.d7.loss_cls: 0.9728  decode.d7.loss_mask: 0.3589  decode.d7.loss_dice: 0.4025  decode.d8.loss_cls: 0.9423  decode.d8.loss_mask: 0.3641  decode.d8.loss_dice: 0.4059
07/29 23:21:58 - mmengine - INFO - Iter(train) [ 3250/80000]  base_lr: 9.6337e-05 lr: 9.6337e-06  eta: 9:43:35  time: 0.4542  data_time: 0.0106  memory: 5244  grad_norm: 134.5244  loss: 17.4132  decode.loss_cls: 0.9053  decode.loss_mask: 0.3383  decode.loss_dice: 0.3936  decode.d0.loss_cls: 1.7577  decode.d0.loss_mask: 0.3527  decode.d0.loss_dice: 0.4580  decode.d1.loss_cls: 1.0981  decode.d1.loss_mask: 0.3477  decode.d1.loss_dice: 0.3898  decode.d2.loss_cls: 0.8882  decode.d2.loss_mask: 0.3385  decode.d2.loss_dice: 0.3895  decode.d3.loss_cls: 0.8762  decode.d3.loss_mask: 0.3420  decode.d3.loss_dice: 0.3826  decode.d4.loss_cls: 0.8467  decode.d4.loss_mask: 0.3392  decode.d4.loss_dice: 0.3568  decode.d5.loss_cls: 0.9557  decode.d5.loss_mask: 0.3346  decode.d5.loss_dice: 0.3886  decode.d6.loss_cls: 0.9399  decode.d6.loss_mask: 0.3321  decode.d6.loss_dice: 0.3698  decode.d7.loss_cls: 0.9074  decode.d7.loss_mask: 0.3482  decode.d7.loss_dice: 0.3906  decode.d8.loss_cls: 0.9135  decode.d8.loss_mask: 0.3433  decode.d8.loss_dice: 0.3887
07/29 23:22:21 - mmengine - INFO - Iter(train) [ 3300/80000]  base_lr: 9.6281e-05 lr: 9.6281e-06  eta: 9:43:08  time: 0.4507  data_time: 0.0103  memory: 5266  grad_norm: 119.9490  loss: 17.8225  decode.loss_cls: 0.9056  decode.loss_mask: 0.3932  decode.loss_dice: 0.3851  decode.d0.loss_cls: 1.8698  decode.d0.loss_mask: 0.4108  decode.d0.loss_dice: 0.4138  decode.d1.loss_cls: 1.0574  decode.d1.loss_mask: 0.4180  decode.d1.loss_dice: 0.3638  decode.d2.loss_cls: 0.9516  decode.d2.loss_mask: 0.3743  decode.d2.loss_dice: 0.3683  decode.d3.loss_cls: 0.9311  decode.d3.loss_mask: 0.3654  decode.d3.loss_dice: 0.3791  decode.d4.loss_cls: 0.8753  decode.d4.loss_mask: 0.3732  decode.d4.loss_dice: 0.3739  decode.d5.loss_cls: 0.8729  decode.d5.loss_mask: 0.3780  decode.d5.loss_dice: 0.3623  decode.d6.loss_cls: 0.8705  decode.d6.loss_mask: 0.3683  decode.d6.loss_dice: 0.3652  decode.d7.loss_cls: 0.9109  decode.d7.loss_mask: 0.3786  decode.d7.loss_dice: 0.3736  decode.d8.loss_cls: 0.9412  decode.d8.loss_mask: 0.3975  decode.d8.loss_dice: 0.3939
07/29 23:22:44 - mmengine - INFO - Iter(train) [ 3350/80000]  base_lr: 9.6224e-05 lr: 9.6224e-06  eta: 9:42:44  time: 0.4562  data_time: 0.0107  memory: 5266  grad_norm: 100.2722  loss: 13.8524  decode.loss_cls: 0.7405  decode.loss_mask: 0.2412  decode.loss_dice: 0.2945  decode.d0.loss_cls: 1.7237  decode.d0.loss_mask: 0.2628  decode.d0.loss_dice: 0.3952  decode.d1.loss_cls: 0.8582  decode.d1.loss_mask: 0.2407  decode.d1.loss_dice: 0.2757  decode.d2.loss_cls: 0.7535  decode.d2.loss_mask: 0.2340  decode.d2.loss_dice: 0.2803  decode.d3.loss_cls: 0.6908  decode.d3.loss_mask: 0.2354  decode.d3.loss_dice: 0.2892  decode.d4.loss_cls: 0.6998  decode.d4.loss_mask: 0.2335  decode.d4.loss_dice: 0.2856  decode.d5.loss_cls: 0.7359  decode.d5.loss_mask: 0.2331  decode.d5.loss_dice: 0.2952  decode.d6.loss_cls: 0.7451  decode.d6.loss_mask: 0.2437  decode.d6.loss_dice: 0.3090  decode.d7.loss_cls: 0.7332  decode.d7.loss_mask: 0.2327  decode.d7.loss_dice: 0.3105  decode.d8.loss_cls: 0.7510  decode.d8.loss_mask: 0.2404  decode.d8.loss_dice: 0.2884
07/29 23:23:07 - mmengine - INFO - Iter(train) [ 3400/80000]  base_lr: 9.6168e-05 lr: 9.6168e-06  eta: 9:42:22  time: 0.4548  data_time: 0.0102  memory: 5266  grad_norm: 168.0489  loss: 20.7123  decode.loss_cls: 1.1463  decode.loss_mask: 0.3180  decode.loss_dice: 0.4296  decode.d0.loss_cls: 2.1742  decode.d0.loss_mask: 0.2960  decode.d0.loss_dice: 0.4636  decode.d1.loss_cls: 1.5270  decode.d1.loss_mask: 0.2769  decode.d1.loss_dice: 0.4125  decode.d2.loss_cls: 1.2250  decode.d2.loss_mask: 0.2848  decode.d2.loss_dice: 0.4197  decode.d3.loss_cls: 1.1129  decode.d3.loss_mask: 0.3418  decode.d3.loss_dice: 0.4554  decode.d4.loss_cls: 1.2655  decode.d4.loss_mask: 0.3388  decode.d4.loss_dice: 0.4582  decode.d5.loss_cls: 1.2544  decode.d5.loss_mask: 0.3484  decode.d5.loss_dice: 0.4327  decode.d6.loss_cls: 1.1862  decode.d6.loss_mask: 0.3161  decode.d6.loss_dice: 0.4129  decode.d7.loss_cls: 1.1809  decode.d7.loss_mask: 0.2996  decode.d7.loss_dice: 0.4065  decode.d8.loss_cls: 1.2133  decode.d8.loss_mask: 0.3075  decode.d8.loss_dice: 0.4074
07/29 23:23:29 - mmengine - INFO - Iter(train) [ 3450/80000]  base_lr: 9.6111e-05 lr: 9.6111e-06  eta: 9:41:58  time: 0.4553  data_time: 0.0104  memory: 5229  grad_norm: 159.6710  loss: 18.4179  decode.loss_cls: 0.9939  decode.loss_mask: 0.3478  decode.loss_dice: 0.4053  decode.d0.loss_cls: 1.8962  decode.d0.loss_mask: 0.3468  decode.d0.loss_dice: 0.4570  decode.d1.loss_cls: 1.2567  decode.d1.loss_mask: 0.3587  decode.d1.loss_dice: 0.4218  decode.d2.loss_cls: 0.9072  decode.d2.loss_mask: 0.3321  decode.d2.loss_dice: 0.4248  decode.d3.loss_cls: 0.9021  decode.d3.loss_mask: 0.3252  decode.d3.loss_dice: 0.4075  decode.d4.loss_cls: 0.9381  decode.d4.loss_mask: 0.3345  decode.d4.loss_dice: 0.4027  decode.d5.loss_cls: 0.9468  decode.d5.loss_mask: 0.3512  decode.d5.loss_dice: 0.4136  decode.d6.loss_cls: 0.9221  decode.d6.loss_mask: 0.3542  decode.d6.loss_dice: 0.3953  decode.d7.loss_cls: 1.0240  decode.d7.loss_mask: 0.3452  decode.d7.loss_dice: 0.3978  decode.d8.loss_cls: 1.0548  decode.d8.loss_mask: 0.3488  decode.d8.loss_dice: 0.4058
07/29 23:23:52 - mmengine - INFO - Iter(train) [ 3500/80000]  base_lr: 9.6055e-05 lr: 9.6055e-06  eta: 9:41:37  time: 0.4550  data_time: 0.0105  memory: 5266  grad_norm: 148.8401  loss: 17.5713  decode.loss_cls: 0.9250  decode.loss_mask: 0.3338  decode.loss_dice: 0.4264  decode.d0.loss_cls: 1.8233  decode.d0.loss_mask: 0.3393  decode.d0.loss_dice: 0.4433  decode.d1.loss_cls: 1.2062  decode.d1.loss_mask: 0.3251  decode.d1.loss_dice: 0.3853  decode.d2.loss_cls: 0.9041  decode.d2.loss_mask: 0.3230  decode.d2.loss_dice: 0.3702  decode.d3.loss_cls: 0.8899  decode.d3.loss_mask: 0.3235  decode.d3.loss_dice: 0.3872  decode.d4.loss_cls: 0.9057  decode.d4.loss_mask: 0.3333  decode.d4.loss_dice: 0.4004  decode.d5.loss_cls: 0.9535  decode.d5.loss_mask: 0.3183  decode.d5.loss_dice: 0.3729  decode.d6.loss_cls: 0.9091  decode.d6.loss_mask: 0.3043  decode.d6.loss_dice: 0.3698  decode.d7.loss_cls: 0.9657  decode.d7.loss_mask: 0.3196  decode.d7.loss_dice: 0.3780  decode.d8.loss_cls: 0.9388  decode.d8.loss_mask: 0.3134  decode.d8.loss_dice: 0.3830
07/29 23:24:15 - mmengine - INFO - Iter(train) [ 3550/80000]  base_lr: 9.5998e-05 lr: 9.5998e-06  eta: 9:41:16  time: 0.4573  data_time: 0.0107  memory: 5266  grad_norm: 153.6240  loss: 19.6650  decode.loss_cls: 1.0151  decode.loss_mask: 0.4497  decode.loss_dice: 0.4401  decode.d0.loss_cls: 1.9703  decode.d0.loss_mask: 0.3578  decode.d0.loss_dice: 0.4921  decode.d1.loss_cls: 1.2389  decode.d1.loss_mask: 0.3133  decode.d1.loss_dice: 0.4154  decode.d2.loss_cls: 1.0961  decode.d2.loss_mask: 0.3079  decode.d2.loss_dice: 0.4478  decode.d3.loss_cls: 1.0550  decode.d3.loss_mask: 0.3297  decode.d3.loss_dice: 0.4180  decode.d4.loss_cls: 1.0360  decode.d4.loss_mask: 0.3366  decode.d4.loss_dice: 0.4460  decode.d5.loss_cls: 0.9920  decode.d5.loss_mask: 0.3790  decode.d5.loss_dice: 0.4369  decode.d6.loss_cls: 1.0179  decode.d6.loss_mask: 0.4014  decode.d6.loss_dice: 0.4326  decode.d7.loss_cls: 1.0276  decode.d7.loss_mask: 0.3849  decode.d7.loss_dice: 0.4997  decode.d8.loss_cls: 1.0417  decode.d8.loss_mask: 0.4204  decode.d8.loss_dice: 0.4650
07/29 23:24:38 - mmengine - INFO - Iter(train) [ 3600/80000]  base_lr: 9.5942e-05 lr: 9.5942e-06  eta: 9:40:53  time: 0.4570  data_time: 0.0108  memory: 5227  grad_norm: 158.3260  loss: 16.0955  decode.loss_cls: 0.7598  decode.loss_mask: 0.3836  decode.loss_dice: 0.3898  decode.d0.loss_cls: 1.7401  decode.d0.loss_mask: 0.3982  decode.d0.loss_dice: 0.4403  decode.d1.loss_cls: 0.9549  decode.d1.loss_mask: 0.3996  decode.d1.loss_dice: 0.3874  decode.d2.loss_cls: 0.7958  decode.d2.loss_mask: 0.3840  decode.d2.loss_dice: 0.3552  decode.d3.loss_cls: 0.6845  decode.d3.loss_mask: 0.3712  decode.d3.loss_dice: 0.3452  decode.d4.loss_cls: 0.6667  decode.d4.loss_mask: 0.3780  decode.d4.loss_dice: 0.3586  decode.d5.loss_cls: 0.7217  decode.d5.loss_mask: 0.3788  decode.d5.loss_dice: 0.3706  decode.d6.loss_cls: 0.7245  decode.d6.loss_mask: 0.3703  decode.d6.loss_dice: 0.3300  decode.d7.loss_cls: 0.7612  decode.d7.loss_mask: 0.3754  decode.d7.loss_dice: 0.3398  decode.d8.loss_cls: 0.7659  decode.d8.loss_mask: 0.3920  decode.d8.loss_dice: 0.3725
07/29 23:25:01 - mmengine - INFO - Iter(train) [ 3650/80000]  base_lr: 9.5885e-05 lr: 9.5885e-06  eta: 9:40:26  time: 0.4541  data_time: 0.0104  memory: 5283  grad_norm: 300.5083  loss: 20.7471  decode.loss_cls: 1.0378  decode.loss_mask: 0.4056  decode.loss_dice: 0.5366  decode.d0.loss_cls: 1.7790  decode.d0.loss_mask: 0.5138  decode.d0.loss_dice: 0.5996  decode.d1.loss_cls: 1.1268  decode.d1.loss_mask: 0.4352  decode.d1.loss_dice: 0.5702  decode.d2.loss_cls: 1.0781  decode.d2.loss_mask: 0.3943  decode.d2.loss_dice: 0.5110  decode.d3.loss_cls: 1.0007  decode.d3.loss_mask: 0.4039  decode.d3.loss_dice: 0.5172  decode.d4.loss_cls: 1.1380  decode.d4.loss_mask: 0.3971  decode.d4.loss_dice: 0.5268  decode.d5.loss_cls: 1.0680  decode.d5.loss_mask: 0.4136  decode.d5.loss_dice: 0.5131  decode.d6.loss_cls: 1.0456  decode.d6.loss_mask: 0.4119  decode.d6.loss_dice: 0.5168  decode.d7.loss_cls: 1.0117  decode.d7.loss_mask: 0.3963  decode.d7.loss_dice: 0.5095  decode.d8.loss_cls: 0.9836  decode.d8.loss_mask: 0.3907  decode.d8.loss_dice: 0.5148
07/29 23:25:12 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250729_225707
07/29 23:25:23 - mmengine - INFO - Iter(train) [ 3700/80000]  base_lr: 9.5829e-05 lr: 9.5829e-06  eta: 9:40:03  time: 0.4491  data_time: 0.0101  memory: 5230  grad_norm: 165.3396  loss: 18.2012  decode.loss_cls: 0.9603  decode.loss_mask: 0.2684  decode.loss_dice: 0.3956  decode.d0.loss_cls: 1.9177  decode.d0.loss_mask: 0.3231  decode.d0.loss_dice: 0.4269  decode.d1.loss_cls: 1.3274  decode.d1.loss_mask: 0.2934  decode.d1.loss_dice: 0.4041  decode.d2.loss_cls: 0.9808  decode.d2.loss_mask: 0.2723  decode.d2.loss_dice: 0.4221  decode.d3.loss_cls: 0.9569  decode.d3.loss_mask: 0.2680  decode.d3.loss_dice: 0.4135  decode.d4.loss_cls: 1.0242  decode.d4.loss_mask: 0.2825  decode.d4.loss_dice: 0.4377  decode.d5.loss_cls: 1.0404  decode.d5.loss_mask: 0.2706  decode.d5.loss_dice: 0.4087  decode.d6.loss_cls: 1.0094  decode.d6.loss_mask: 0.2657  decode.d6.loss_dice: 0.4032  decode.d7.loss_cls: 1.0565  decode.d7.loss_mask: 0.2758  decode.d7.loss_dice: 0.3982  decode.d8.loss_cls: 1.0150  decode.d8.loss_mask: 0.2732  decode.d8.loss_dice: 0.4094
07/29 23:25:46 - mmengine - INFO - Iter(train) [ 3750/80000]  base_lr: 9.5772e-05 lr: 9.5772e-06  eta: 9:39:41  time: 0.4548  data_time: 0.0105  memory: 5307  grad_norm: 141.2434  loss: 18.5139  decode.loss_cls: 0.9390  decode.loss_mask: 0.3856  decode.loss_dice: 0.4145  decode.d0.loss_cls: 1.6216  decode.d0.loss_mask: 0.4334  decode.d0.loss_dice: 0.5209  decode.d1.loss_cls: 1.0874  decode.d1.loss_mask: 0.3956  decode.d1.loss_dice: 0.4298  decode.d2.loss_cls: 0.8745  decode.d2.loss_mask: 0.4042  decode.d2.loss_dice: 0.4380  decode.d3.loss_cls: 0.9155  decode.d3.loss_mask: 0.4102  decode.d3.loss_dice: 0.4324  decode.d4.loss_cls: 0.9691  decode.d4.loss_mask: 0.3861  decode.d4.loss_dice: 0.4397  decode.d5.loss_cls: 0.9226  decode.d5.loss_mask: 0.3979  decode.d5.loss_dice: 0.4235  decode.d6.loss_cls: 0.9765  decode.d6.loss_mask: 0.3947  decode.d6.loss_dice: 0.4167  decode.d7.loss_cls: 0.9108  decode.d7.loss_mask: 0.3881  decode.d7.loss_dice: 0.4223  decode.d8.loss_cls: 0.9380  decode.d8.loss_mask: 0.3894  decode.d8.loss_dice: 0.4359
07/29 23:26:09 - mmengine - INFO - Iter(train) [ 3800/80000]  base_lr: 9.5716e-05 lr: 9.5716e-06  eta: 9:39:15  time: 0.4494  data_time: 0.0099  memory: 5281  grad_norm: 148.2301  loss: 17.4146  decode.loss_cls: 0.8799  decode.loss_mask: 0.3316  decode.loss_dice: 0.3843  decode.d0.loss_cls: 1.8979  decode.d0.loss_mask: 0.3360  decode.d0.loss_dice: 0.3830  decode.d1.loss_cls: 1.1908  decode.d1.loss_mask: 0.3246  decode.d1.loss_dice: 0.3355  decode.d2.loss_cls: 0.9469  decode.d2.loss_mask: 0.3346  decode.d2.loss_dice: 0.3733  decode.d3.loss_cls: 0.9534  decode.d3.loss_mask: 0.3274  decode.d3.loss_dice: 0.3621  decode.d4.loss_cls: 0.8712  decode.d4.loss_mask: 0.3285  decode.d4.loss_dice: 0.3705  decode.d5.loss_cls: 0.8599  decode.d5.loss_mask: 0.3341  decode.d5.loss_dice: 0.3926  decode.d6.loss_cls: 0.9188  decode.d6.loss_mask: 0.3330  decode.d6.loss_dice: 0.4008  decode.d7.loss_cls: 0.9349  decode.d7.loss_mask: 0.3329  decode.d7.loss_dice: 0.3801  decode.d8.loss_cls: 0.8825  decode.d8.loss_mask: 0.3296  decode.d8.loss_dice: 0.3841
07/29 23:26:31 - mmengine - INFO - Iter(train) [ 3850/80000]  base_lr: 9.5659e-05 lr: 9.5659e-06  eta: 9:38:44  time: 0.4453  data_time: 0.0094  memory: 5266  grad_norm: 148.8072  loss: 16.6143  decode.loss_cls: 0.8881  decode.loss_mask: 0.3094  decode.loss_dice: 0.3526  decode.d0.loss_cls: 1.7861  decode.d0.loss_mask: 0.3282  decode.d0.loss_dice: 0.4529  decode.d1.loss_cls: 1.0133  decode.d1.loss_mask: 0.3103  decode.d1.loss_dice: 0.3597  decode.d2.loss_cls: 0.8883  decode.d2.loss_mask: 0.2992  decode.d2.loss_dice: 0.3783  decode.d3.loss_cls: 0.9072  decode.d3.loss_mask: 0.2998  decode.d3.loss_dice: 0.3421  decode.d4.loss_cls: 0.8921  decode.d4.loss_mask: 0.3136  decode.d4.loss_dice: 0.3440  decode.d5.loss_cls: 0.9248  decode.d5.loss_mask: 0.2977  decode.d5.loss_dice: 0.3537  decode.d6.loss_cls: 0.8903  decode.d6.loss_mask: 0.2984  decode.d6.loss_dice: 0.3437  decode.d7.loss_cls: 0.8717  decode.d7.loss_mask: 0.2996  decode.d7.loss_dice: 0.3445  decode.d8.loss_cls: 0.8563  decode.d8.loss_mask: 0.3108  decode.d8.loss_dice: 0.3576
07/29 23:26:54 - mmengine - INFO - Iter(train) [ 3900/80000]  base_lr: 9.5603e-05 lr: 9.5603e-06  eta: 9:38:12  time: 0.4485  data_time: 0.0098  memory: 5244  grad_norm: 150.8313  loss: 16.8128  decode.loss_cls: 0.7816  decode.loss_mask: 0.3640  decode.loss_dice: 0.4150  decode.d0.loss_cls: 1.9118  decode.d0.loss_mask: 0.3809  decode.d0.loss_dice: 0.4525  decode.d1.loss_cls: 1.0430  decode.d1.loss_mask: 0.3525  decode.d1.loss_dice: 0.4157  decode.d2.loss_cls: 0.7649  decode.d2.loss_mask: 0.3391  decode.d2.loss_dice: 0.3712  decode.d3.loss_cls: 0.7875  decode.d3.loss_mask: 0.3413  decode.d3.loss_dice: 0.3777  decode.d4.loss_cls: 0.8472  decode.d4.loss_mask: 0.3343  decode.d4.loss_dice: 0.3957  decode.d5.loss_cls: 0.8492  decode.d5.loss_mask: 0.3387  decode.d5.loss_dice: 0.3900  decode.d6.loss_cls: 0.8763  decode.d6.loss_mask: 0.3261  decode.d6.loss_dice: 0.3771  decode.d7.loss_cls: 0.7658  decode.d7.loss_mask: 0.3389  decode.d7.loss_dice: 0.4063  decode.d8.loss_cls: 0.7740  decode.d8.loss_mask: 0.3356  decode.d8.loss_dice: 0.3590
07/29 23:27:16 - mmengine - INFO - Iter(train) [ 3950/80000]  base_lr: 9.5546e-05 lr: 9.5546e-06  eta: 9:37:49  time: 0.4570  data_time: 0.0105  memory: 5283  grad_norm: 159.4127  loss: 18.7834  decode.loss_cls: 1.0841  decode.loss_mask: 0.3165  decode.loss_dice: 0.4102  decode.d0.loss_cls: 1.7335  decode.d0.loss_mask: 0.3467  decode.d0.loss_dice: 0.5288  decode.d1.loss_cls: 1.1595  decode.d1.loss_mask: 0.3128  decode.d1.loss_dice: 0.4044  decode.d2.loss_cls: 1.1099  decode.d2.loss_mask: 0.3207  decode.d2.loss_dice: 0.3823  decode.d3.loss_cls: 1.0768  decode.d3.loss_mask: 0.3198  decode.d3.loss_dice: 0.4302  decode.d4.loss_cls: 1.1006  decode.d4.loss_mask: 0.3182  decode.d4.loss_dice: 0.4173  decode.d5.loss_cls: 1.0294  decode.d5.loss_mask: 0.3111  decode.d5.loss_dice: 0.3881  decode.d6.loss_cls: 1.0944  decode.d6.loss_mask: 0.3273  decode.d6.loss_dice: 0.3818  decode.d7.loss_cls: 1.0146  decode.d7.loss_mask: 0.3058  decode.d7.loss_dice: 0.3748  decode.d8.loss_cls: 1.0456  decode.d8.loss_mask: 0.3259  decode.d8.loss_dice: 0.4125
07/29 23:27:39 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250729_225707
07/29 23:27:39 - mmengine - INFO - Iter(train) [ 4000/80000]  base_lr: 9.5490e-05 lr: 9.5490e-06  eta: 9:37:27  time: 0.4580  data_time: 0.0105  memory: 5229  grad_norm: 221.7353  loss: 22.2884  decode.loss_cls: 1.1441  decode.loss_mask: 0.3911  decode.loss_dice: 0.5657  decode.d0.loss_cls: 1.8049  decode.d0.loss_mask: 0.3926  decode.d0.loss_dice: 0.6518  decode.d1.loss_cls: 1.3918  decode.d1.loss_mask: 0.4056  decode.d1.loss_dice: 0.5533  decode.d2.loss_cls: 1.1564  decode.d2.loss_mask: 0.4351  decode.d2.loss_dice: 0.5433  decode.d3.loss_cls: 1.1941  decode.d3.loss_mask: 0.3362  decode.d3.loss_dice: 0.5489  decode.d4.loss_cls: 1.1747  decode.d4.loss_mask: 0.3937  decode.d4.loss_dice: 0.5618  decode.d5.loss_cls: 1.0863  decode.d5.loss_mask: 0.4691  decode.d5.loss_dice: 0.5910  decode.d6.loss_cls: 1.1320  decode.d6.loss_mask: 0.4121  decode.d6.loss_dice: 0.6386  decode.d7.loss_cls: 1.2009  decode.d7.loss_mask: 0.3771  decode.d7.loss_dice: 0.5811  decode.d8.loss_cls: 1.1647  decode.d8.loss_mask: 0.4017  decode.d8.loss_dice: 0.5887
07/29 23:28:02 - mmengine - INFO - Iter(train) [ 4050/80000]  base_lr: 9.5433e-05 lr: 9.5433e-06  eta: 9:37:05  time: 0.4575  data_time: 0.0107  memory: 5266  grad_norm: 136.5506  loss: 15.8323  decode.loss_cls: 0.7747  decode.loss_mask: 0.3440  decode.loss_dice: 0.3440  decode.d0.loss_cls: 1.6034  decode.d0.loss_mask: 0.3683  decode.d0.loss_dice: 0.4577  decode.d1.loss_cls: 1.0356  decode.d1.loss_mask: 0.3705  decode.d1.loss_dice: 0.3847  decode.d2.loss_cls: 0.7409  decode.d2.loss_mask: 0.3494  decode.d2.loss_dice: 0.3588  decode.d3.loss_cls: 0.7139  decode.d3.loss_mask: 0.3414  decode.d3.loss_dice: 0.3588  decode.d4.loss_cls: 0.7552  decode.d4.loss_mask: 0.3355  decode.d4.loss_dice: 0.3532  decode.d5.loss_cls: 0.7296  decode.d5.loss_mask: 0.3444  decode.d5.loss_dice: 0.3585  decode.d6.loss_cls: 0.7189  decode.d6.loss_mask: 0.3394  decode.d6.loss_dice: 0.3683  decode.d7.loss_cls: 0.8183  decode.d7.loss_mask: 0.3377  decode.d7.loss_dice: 0.3593  decode.d8.loss_cls: 0.7833  decode.d8.loss_mask: 0.3386  decode.d8.loss_dice: 0.3462
07/29 23:28:25 - mmengine - INFO - Iter(train) [ 4100/80000]  base_lr: 9.5377e-05 lr: 9.5377e-06  eta: 9:36:42  time: 0.4548  data_time: 0.0102  memory: 5245  grad_norm: 133.2482  loss: 16.1232  decode.loss_cls: 0.7934  decode.loss_mask: 0.3272  decode.loss_dice: 0.3971  decode.d0.loss_cls: 1.6885  decode.d0.loss_mask: 0.3931  decode.d0.loss_dice: 0.4374  decode.d1.loss_cls: 0.9317  decode.d1.loss_mask: 0.3273  decode.d1.loss_dice: 0.3920  decode.d2.loss_cls: 0.7750  decode.d2.loss_mask: 0.3111  decode.d2.loss_dice: 0.3805  decode.d3.loss_cls: 0.8182  decode.d3.loss_mask: 0.3191  decode.d3.loss_dice: 0.3804  decode.d4.loss_cls: 0.7440  decode.d4.loss_mask: 0.3214  decode.d4.loss_dice: 0.3711  decode.d5.loss_cls: 0.7781  decode.d5.loss_mask: 0.3233  decode.d5.loss_dice: 0.3815  decode.d6.loss_cls: 0.7539  decode.d6.loss_mask: 0.3316  decode.d6.loss_dice: 0.3845  decode.d7.loss_cls: 0.7862  decode.d7.loss_mask: 0.3624  decode.d7.loss_dice: 0.3747  decode.d8.loss_cls: 0.8205  decode.d8.loss_mask: 0.3449  decode.d8.loss_dice: 0.3731
07/29 23:28:48 - mmengine - INFO - Iter(train) [ 4150/80000]  base_lr: 9.5320e-05 lr: 9.5320e-06  eta: 9:36:21  time: 0.4525  data_time: 0.0101  memory: 5307  grad_norm: 139.8457  loss: 16.1132  decode.loss_cls: 0.8021  decode.loss_mask: 0.3529  decode.loss_dice: 0.4159  decode.d0.loss_cls: 1.7138  decode.d0.loss_mask: 0.4009  decode.d0.loss_dice: 0.4813  decode.d1.loss_cls: 0.8873  decode.d1.loss_mask: 0.3228  decode.d1.loss_dice: 0.4216  decode.d2.loss_cls: 0.6661  decode.d2.loss_mask: 0.3679  decode.d2.loss_dice: 0.3944  decode.d3.loss_cls: 0.6342  decode.d3.loss_mask: 0.3664  decode.d3.loss_dice: 0.4186  decode.d4.loss_cls: 0.7147  decode.d4.loss_mask: 0.3823  decode.d4.loss_dice: 0.4038  decode.d5.loss_cls: 0.7399  decode.d5.loss_mask: 0.3421  decode.d5.loss_dice: 0.4154  decode.d6.loss_cls: 0.7058  decode.d6.loss_mask: 0.3408  decode.d6.loss_dice: 0.4029  decode.d7.loss_cls: 0.7311  decode.d7.loss_mask: 0.3519  decode.d7.loss_dice: 0.4077  decode.d8.loss_cls: 0.7431  decode.d8.loss_mask: 0.3592  decode.d8.loss_dice: 0.4262
07/29 23:29:11 - mmengine - INFO - Iter(train) [ 4200/80000]  base_lr: 9.5263e-05 lr: 9.5263e-06  eta: 9:35:56  time: 0.4581  data_time: 0.0103  memory: 5264  grad_norm: 98.0509  loss: 12.7679  decode.loss_cls: 0.5689  decode.loss_mask: 0.3222  decode.loss_dice: 0.3158  decode.d0.loss_cls: 1.3628  decode.d0.loss_mask: 0.3165  decode.d0.loss_dice: 0.3916  decode.d1.loss_cls: 0.6526  decode.d1.loss_mask: 0.2999  decode.d1.loss_dice: 0.3285  decode.d2.loss_cls: 0.4912  decode.d2.loss_mask: 0.2961  decode.d2.loss_dice: 0.3380  decode.d3.loss_cls: 0.4683  decode.d3.loss_mask: 0.3000  decode.d3.loss_dice: 0.3296  decode.d4.loss_cls: 0.5539  decode.d4.loss_mask: 0.2912  decode.d4.loss_dice: 0.2950  decode.d5.loss_cls: 0.5893  decode.d5.loss_mask: 0.2964  decode.d5.loss_dice: 0.2995  decode.d6.loss_cls: 0.6088  decode.d6.loss_mask: 0.2967  decode.d6.loss_dice: 0.3012  decode.d7.loss_cls: 0.5959  decode.d7.loss_mask: 0.3161  decode.d7.loss_dice: 0.3192  decode.d8.loss_cls: 0.6006  decode.d8.loss_mask: 0.3005  decode.d8.loss_dice: 0.3216
07/29 23:29:33 - mmengine - INFO - Iter(train) [ 4250/80000]  base_lr: 9.5207e-05 lr: 9.5207e-06  eta: 9:35:33  time: 0.4572  data_time: 0.0104  memory: 5245  grad_norm: 194.0895  loss: 17.6398  decode.loss_cls: 0.9745  decode.loss_mask: 0.3804  decode.loss_dice: 0.3957  decode.d0.loss_cls: 1.7594  decode.d0.loss_mask: 0.3021  decode.d0.loss_dice: 0.4624  decode.d1.loss_cls: 1.1338  decode.d1.loss_mask: 0.2726  decode.d1.loss_dice: 0.4185  decode.d2.loss_cls: 0.9482  decode.d2.loss_mask: 0.2842  decode.d2.loss_dice: 0.3580  decode.d3.loss_cls: 0.8648  decode.d3.loss_mask: 0.3097  decode.d3.loss_dice: 0.3930  decode.d4.loss_cls: 0.8725  decode.d4.loss_mask: 0.3140  decode.d4.loss_dice: 0.4286  decode.d5.loss_cls: 0.9672  decode.d5.loss_mask: 0.2953  decode.d5.loss_dice: 0.3937  decode.d6.loss_cls: 0.9621  decode.d6.loss_mask: 0.3190  decode.d6.loss_dice: 0.4058  decode.d7.loss_cls: 0.9015  decode.d7.loss_mask: 0.3486  decode.d7.loss_dice: 0.4204  decode.d8.loss_cls: 1.0008  decode.d8.loss_mask: 0.3588  decode.d8.loss_dice: 0.3940
07/29 23:29:56 - mmengine - INFO - Iter(train) [ 4300/80000]  base_lr: 9.5150e-05 lr: 9.5150e-06  eta: 9:35:08  time: 0.4533  data_time: 0.0102  memory: 5245  grad_norm: 129.3752  loss: 13.6469  decode.loss_cls: 0.6031  decode.loss_mask: 0.2827  decode.loss_dice: 0.3535  decode.d0.loss_cls: 1.6379  decode.d0.loss_mask: 0.3184  decode.d0.loss_dice: 0.4391  decode.d1.loss_cls: 0.6833  decode.d1.loss_mask: 0.2909  decode.d1.loss_dice: 0.3920  decode.d2.loss_cls: 0.5593  decode.d2.loss_mask: 0.2812  decode.d2.loss_dice: 0.3645  decode.d3.loss_cls: 0.6115  decode.d3.loss_mask: 0.2827  decode.d3.loss_dice: 0.3480  decode.d4.loss_cls: 0.5846  decode.d4.loss_mask: 0.2824  decode.d4.loss_dice: 0.3596  decode.d5.loss_cls: 0.5874  decode.d5.loss_mask: 0.2812  decode.d5.loss_dice: 0.3617  decode.d6.loss_cls: 0.5899  decode.d6.loss_mask: 0.2819  decode.d6.loss_dice: 0.3656  decode.d7.loss_cls: 0.6047  decode.d7.loss_mask: 0.2869  decode.d7.loss_dice: 0.3560  decode.d8.loss_cls: 0.6085  decode.d8.loss_mask: 0.2795  decode.d8.loss_dice: 0.3690
07/29 23:30:19 - mmengine - INFO - Iter(train) [ 4350/80000]  base_lr: 9.5094e-05 lr: 9.5094e-06  eta: 9:34:45  time: 0.4493  data_time: 0.0102  memory: 5283  grad_norm: 145.2580  loss: 13.4882  decode.loss_cls: 0.6345  decode.loss_mask: 0.2847  decode.loss_dice: 0.3565  decode.d0.loss_cls: 1.3979  decode.d0.loss_mask: 0.3102  decode.d0.loss_dice: 0.4259  decode.d1.loss_cls: 0.6983  decode.d1.loss_mask: 0.3002  decode.d1.loss_dice: 0.3990  decode.d2.loss_cls: 0.5682  decode.d2.loss_mask: 0.2955  decode.d2.loss_dice: 0.3869  decode.d3.loss_cls: 0.5838  decode.d3.loss_mask: 0.2814  decode.d3.loss_dice: 0.3741  decode.d4.loss_cls: 0.5250  decode.d4.loss_mask: 0.3078  decode.d4.loss_dice: 0.3658  decode.d5.loss_cls: 0.5474  decode.d5.loss_mask: 0.2895  decode.d5.loss_dice: 0.3728  decode.d6.loss_cls: 0.5702  decode.d6.loss_mask: 0.2944  decode.d6.loss_dice: 0.3730  decode.d7.loss_cls: 0.5862  decode.d7.loss_mask: 0.2992  decode.d7.loss_dice: 0.3859  decode.d8.loss_cls: 0.6168  decode.d8.loss_mask: 0.2943  decode.d8.loss_dice: 0.3629
07/29 23:30:41 - mmengine - INFO - Iter(train) [ 4400/80000]  base_lr: 9.5037e-05 lr: 9.5037e-06  eta: 9:34:21  time: 0.4540  data_time: 0.0104  memory: 5266  grad_norm: 128.0363  loss: 14.8350  decode.loss_cls: 0.6457  decode.loss_mask: 0.4173  decode.loss_dice: 0.4579  decode.d0.loss_cls: 1.3541  decode.d0.loss_mask: 0.3330  decode.d0.loss_dice: 0.4851  decode.d1.loss_cls: 0.6961  decode.d1.loss_mask: 0.3506  decode.d1.loss_dice: 0.4419  decode.d2.loss_cls: 0.5882  decode.d2.loss_mask: 0.3276  decode.d2.loss_dice: 0.4213  decode.d3.loss_cls: 0.5502  decode.d3.loss_mask: 0.3336  decode.d3.loss_dice: 0.3999  decode.d4.loss_cls: 0.6209  decode.d4.loss_mask: 0.3208  decode.d4.loss_dice: 0.4287  decode.d5.loss_cls: 0.6254  decode.d5.loss_mask: 0.3278  decode.d5.loss_dice: 0.4357  decode.d6.loss_cls: 0.5978  decode.d6.loss_mask: 0.3457  decode.d6.loss_dice: 0.4672  decode.d7.loss_cls: 0.5862  decode.d7.loss_mask: 0.3604  decode.d7.loss_dice: 0.4610  decode.d8.loss_cls: 0.5851  decode.d8.loss_mask: 0.3779  decode.d8.loss_dice: 0.4919
07/29 23:31:04 - mmengine - INFO - Iter(train) [ 4450/80000]  base_lr: 9.4981e-05 lr: 9.4981e-06  eta: 9:33:56  time: 0.4534  data_time: 0.0102  memory: 5229  grad_norm: 137.7939  loss: 17.3501  decode.loss_cls: 0.7711  decode.loss_mask: 0.2772  decode.loss_dice: 0.5057  decode.d0.loss_cls: 1.5359  decode.d0.loss_mask: 0.3706  decode.d0.loss_dice: 0.5460  decode.d1.loss_cls: 1.0464  decode.d1.loss_mask: 0.2878  decode.d1.loss_dice: 0.5243  decode.d2.loss_cls: 0.8866  decode.d2.loss_mask: 0.2922  decode.d2.loss_dice: 0.5083  decode.d3.loss_cls: 0.7973  decode.d3.loss_mask: 0.2881  decode.d3.loss_dice: 0.5085  decode.d4.loss_cls: 0.8416  decode.d4.loss_mask: 0.3052  decode.d4.loss_dice: 0.5474  decode.d5.loss_cls: 0.8160  decode.d5.loss_mask: 0.3138  decode.d5.loss_dice: 0.5338  decode.d6.loss_cls: 0.8186  decode.d6.loss_mask: 0.3004  decode.d6.loss_dice: 0.5018  decode.d7.loss_cls: 0.8777  decode.d7.loss_mask: 0.2888  decode.d7.loss_dice: 0.5004  decode.d8.loss_cls: 0.7653  decode.d8.loss_mask: 0.2855  decode.d8.loss_dice: 0.5076
07/29 23:31:27 - mmengine - INFO - Iter(train) [ 4500/80000]  base_lr: 9.4924e-05 lr: 9.4924e-06  eta: 9:33:34  time: 0.4570  data_time: 0.0105  memory: 5266  grad_norm: 108.6549  loss: 13.5895  decode.loss_cls: 0.6296  decode.loss_mask: 0.2926  decode.loss_dice: 0.3418  decode.d0.loss_cls: 1.5351  decode.d0.loss_mask: 0.2805  decode.d0.loss_dice: 0.3678  decode.d1.loss_cls: 0.6872  decode.d1.loss_mask: 0.2836  decode.d1.loss_dice: 0.3281  decode.d2.loss_cls: 0.6328  decode.d2.loss_mask: 0.2811  decode.d2.loss_dice: 0.3386  decode.d3.loss_cls: 0.5986  decode.d3.loss_mask: 0.2879  decode.d3.loss_dice: 0.3469  decode.d4.loss_cls: 0.6144  decode.d4.loss_mask: 0.2772  decode.d4.loss_dice: 0.3289  decode.d5.loss_cls: 0.6989  decode.d5.loss_mask: 0.2741  decode.d5.loss_dice: 0.3497  decode.d6.loss_cls: 0.6360  decode.d6.loss_mask: 0.2787  decode.d6.loss_dice: 0.3481  decode.d7.loss_cls: 0.6800  decode.d7.loss_mask: 0.2777  decode.d7.loss_dice: 0.3439  decode.d8.loss_cls: 0.6328  decode.d8.loss_mask: 0.3039  decode.d8.loss_dice: 0.3132
07/29 23:31:50 - mmengine - INFO - Iter(train) [ 4550/80000]  base_lr: 9.4867e-05 lr: 9.4867e-06  eta: 9:33:13  time: 0.4675  data_time: 0.0105  memory: 5245  grad_norm: 371.6242  loss: 18.5498  decode.loss_cls: 0.8164  decode.loss_mask: 0.3931  decode.loss_dice: 0.4936  decode.d0.loss_cls: 1.8838  decode.d0.loss_mask: 0.3839  decode.d0.loss_dice: 0.5423  decode.d1.loss_cls: 1.1349  decode.d1.loss_mask: 0.3941  decode.d1.loss_dice: 0.5248  decode.d2.loss_cls: 0.9791  decode.d2.loss_mask: 0.3582  decode.d2.loss_dice: 0.4556  decode.d3.loss_cls: 0.8750  decode.d3.loss_mask: 0.3645  decode.d3.loss_dice: 0.4352  decode.d4.loss_cls: 0.8162  decode.d4.loss_mask: 0.3565  decode.d4.loss_dice: 0.4538  decode.d5.loss_cls: 0.8332  decode.d5.loss_mask: 0.3629  decode.d5.loss_dice: 0.4805  decode.d6.loss_cls: 0.8394  decode.d6.loss_mask: 0.3682  decode.d6.loss_dice: 0.4744  decode.d7.loss_cls: 0.9316  decode.d7.loss_mask: 0.3539  decode.d7.loss_dice: 0.4717  decode.d8.loss_cls: 0.8904  decode.d8.loss_mask: 0.3851  decode.d8.loss_dice: 0.4977
07/29 23:32:13 - mmengine - INFO - Iter(train) [ 4600/80000]  base_lr: 9.4811e-05 lr: 9.4811e-06  eta: 9:32:51  time: 0.4571  data_time: 0.0108  memory: 5266  grad_norm: 183.4423  loss: 18.2535  decode.loss_cls: 0.7479  decode.loss_mask: 0.4009  decode.loss_dice: 0.6001  decode.d0.loss_cls: 1.5969  decode.d0.loss_mask: 0.2895  decode.d0.loss_dice: 0.5579  decode.d1.loss_cls: 0.8619  decode.d1.loss_mask: 0.3054  decode.d1.loss_dice: 0.4937  decode.d2.loss_cls: 0.8181  decode.d2.loss_mask: 0.3000  decode.d2.loss_dice: 0.5133  decode.d3.loss_cls: 0.8047  decode.d3.loss_mask: 0.3017  decode.d3.loss_dice: 0.5149  decode.d4.loss_cls: 0.8403  decode.d4.loss_mask: 0.3338  decode.d4.loss_dice: 0.5673  decode.d5.loss_cls: 0.9837  decode.d5.loss_mask: 0.3448  decode.d5.loss_dice: 0.5496  decode.d6.loss_cls: 0.9145  decode.d6.loss_mask: 0.3647  decode.d6.loss_dice: 0.5325  decode.d7.loss_cls: 0.9270  decode.d7.loss_mask: 0.4089  decode.d7.loss_dice: 0.5384  decode.d8.loss_cls: 0.8190  decode.d8.loss_mask: 0.4231  decode.d8.loss_dice: 0.5990
07/29 23:32:36 - mmengine - INFO - Iter(train) [ 4650/80000]  base_lr: 9.4754e-05 lr: 9.4754e-06  eta: 9:32:30  time: 0.4590  data_time: 0.0102  memory: 5245  grad_norm: 178.3401  loss: 17.6975  decode.loss_cls: 0.8379  decode.loss_mask: 0.4522  decode.loss_dice: 0.3932  decode.d0.loss_cls: 1.7260  decode.d0.loss_mask: 0.4229  decode.d0.loss_dice: 0.5021  decode.d1.loss_cls: 1.0054  decode.d1.loss_mask: 0.3978  decode.d1.loss_dice: 0.4382  decode.d2.loss_cls: 0.8478  decode.d2.loss_mask: 0.3759  decode.d2.loss_dice: 0.4018  decode.d3.loss_cls: 0.8011  decode.d3.loss_mask: 0.4006  decode.d3.loss_dice: 0.4116  decode.d4.loss_cls: 0.8739  decode.d4.loss_mask: 0.3875  decode.d4.loss_dice: 0.4304  decode.d5.loss_cls: 0.8765  decode.d5.loss_mask: 0.4085  decode.d5.loss_dice: 0.3983  decode.d6.loss_cls: 0.8682  decode.d6.loss_mask: 0.4059  decode.d6.loss_dice: 0.3903  decode.d7.loss_cls: 0.8421  decode.d7.loss_mask: 0.3812  decode.d7.loss_dice: 0.4106  decode.d8.loss_cls: 0.8105  decode.d8.loss_mask: 0.4066  decode.d8.loss_dice: 0.3924
07/29 23:32:58 - mmengine - INFO - Iter(train) [ 4700/80000]  base_lr: 9.4698e-05 lr: 9.4698e-06  eta: 9:32:08  time: 0.4563  data_time: 0.0103  memory: 5264  grad_norm: 99.1960  loss: 15.0791  decode.loss_cls: 0.7554  decode.loss_mask: 0.3157  decode.loss_dice: 0.3815  decode.d0.loss_cls: 1.5667  decode.d0.loss_mask: 0.3315  decode.d0.loss_dice: 0.4297  decode.d1.loss_cls: 0.7946  decode.d1.loss_mask: 0.3059  decode.d1.loss_dice: 0.3758  decode.d2.loss_cls: 0.7190  decode.d2.loss_mask: 0.3046  decode.d2.loss_dice: 0.3645  decode.d3.loss_cls: 0.6934  decode.d3.loss_mask: 0.3075  decode.d3.loss_dice: 0.3540  decode.d4.loss_cls: 0.6944  decode.d4.loss_mask: 0.3266  decode.d4.loss_dice: 0.3536  decode.d5.loss_cls: 0.7263  decode.d5.loss_mask: 0.3318  decode.d5.loss_dice: 0.3442  decode.d6.loss_cls: 0.6973  decode.d6.loss_mask: 0.3310  decode.d6.loss_dice: 0.3555  decode.d7.loss_cls: 0.8064  decode.d7.loss_mask: 0.3081  decode.d7.loss_dice: 0.3543  decode.d8.loss_cls: 0.7648  decode.d8.loss_mask: 0.3048  decode.d8.loss_dice: 0.3801
07/29 23:33:21 - mmengine - INFO - Iter(train) [ 4750/80000]  base_lr: 9.4641e-05 lr: 9.4641e-06  eta: 9:31:45  time: 0.4539  data_time: 0.0103  memory: 5264  grad_norm: 155.2130  loss: 15.9493  decode.loss_cls: 0.8387  decode.loss_mask: 0.2918  decode.loss_dice: 0.3479  decode.d0.loss_cls: 1.8371  decode.d0.loss_mask: 0.3683  decode.d0.loss_dice: 0.4357  decode.d1.loss_cls: 1.0417  decode.d1.loss_mask: 0.3029  decode.d1.loss_dice: 0.3610  decode.d2.loss_cls: 0.9078  decode.d2.loss_mask: 0.2841  decode.d2.loss_dice: 0.3392  decode.d3.loss_cls: 0.8022  decode.d3.loss_mask: 0.2755  decode.d3.loss_dice: 0.3370  decode.d4.loss_cls: 0.7960  decode.d4.loss_mask: 0.2692  decode.d4.loss_dice: 0.3319  decode.d5.loss_cls: 0.8563  decode.d5.loss_mask: 0.2809  decode.d5.loss_dice: 0.3225  decode.d6.loss_cls: 0.8202  decode.d6.loss_mask: 0.2884  decode.d6.loss_dice: 0.3334  decode.d7.loss_cls: 0.8387  decode.d7.loss_mask: 0.2896  decode.d7.loss_dice: 0.3358  decode.d8.loss_cls: 0.8047  decode.d8.loss_mask: 0.2820  decode.d8.loss_dice: 0.3290
07/29 23:33:44 - mmengine - INFO - Iter(train) [ 4800/80000]  base_lr: 9.4584e-05 lr: 9.4584e-06  eta: 9:31:21  time: 0.4544  data_time: 0.0105  memory: 5245  grad_norm: 110.9400  loss: 14.0494  decode.loss_cls: 0.6764  decode.loss_mask: 0.2807  decode.loss_dice: 0.3094  decode.d0.loss_cls: 1.5497  decode.d0.loss_mask: 0.3031  decode.d0.loss_dice: 0.4008  decode.d1.loss_cls: 0.8207  decode.d1.loss_mask: 0.3195  decode.d1.loss_dice: 0.3480  decode.d2.loss_cls: 0.6978  decode.d2.loss_mask: 0.2971  decode.d2.loss_dice: 0.3439  decode.d3.loss_cls: 0.7118  decode.d3.loss_mask: 0.2923  decode.d3.loss_dice: 0.3321  decode.d4.loss_cls: 0.6341  decode.d4.loss_mask: 0.2937  decode.d4.loss_dice: 0.3430  decode.d5.loss_cls: 0.5994  decode.d5.loss_mask: 0.2960  decode.d5.loss_dice: 0.3153  decode.d6.loss_cls: 0.6338  decode.d6.loss_mask: 0.2943  decode.d6.loss_dice: 0.3306  decode.d7.loss_cls: 0.7056  decode.d7.loss_mask: 0.2943  decode.d7.loss_dice: 0.3541  decode.d8.loss_cls: 0.6526  decode.d8.loss_mask: 0.2856  decode.d8.loss_dice: 0.3340
07/29 23:34:07 - mmengine - INFO - Iter(train) [ 4850/80000]  base_lr: 9.4528e-05 lr: 9.4528e-06  eta: 9:30:58  time: 0.4569  data_time: 0.0104  memory: 5281  grad_norm: 94.8083  loss: 13.6060  decode.loss_cls: 0.6898  decode.loss_mask: 0.2627  decode.loss_dice: 0.3647  decode.d0.loss_cls: 1.5343  decode.d0.loss_mask: 0.2672  decode.d0.loss_dice: 0.4047  decode.d1.loss_cls: 0.7494  decode.d1.loss_mask: 0.2734  decode.d1.loss_dice: 0.3628  decode.d2.loss_cls: 0.6881  decode.d2.loss_mask: 0.2593  decode.d2.loss_dice: 0.3512  decode.d3.loss_cls: 0.6267  decode.d3.loss_mask: 0.2525  decode.d3.loss_dice: 0.3199  decode.d4.loss_cls: 0.5952  decode.d4.loss_mask: 0.2572  decode.d4.loss_dice: 0.3372  decode.d5.loss_cls: 0.6214  decode.d5.loss_mask: 0.2613  decode.d5.loss_dice: 0.3719  decode.d6.loss_cls: 0.7058  decode.d6.loss_mask: 0.2569  decode.d6.loss_dice: 0.3075  decode.d7.loss_cls: 0.6657  decode.d7.loss_mask: 0.2598  decode.d7.loss_dice: 0.3390  decode.d8.loss_cls: 0.6243  decode.d8.loss_mask: 0.2611  decode.d8.loss_dice: 0.3348
07/29 23:34:29 - mmengine - INFO - Iter(train) [ 4900/80000]  base_lr: 9.4471e-05 lr: 9.4471e-06  eta: 9:30:32  time: 0.4502  data_time: 0.0100  memory: 5266  grad_norm: 165.6197  loss: 12.8906  decode.loss_cls: 0.5786  decode.loss_mask: 0.3147  decode.loss_dice: 0.3901  decode.d0.loss_cls: 1.3786  decode.d0.loss_mask: 0.3432  decode.d0.loss_dice: 0.4545  decode.d1.loss_cls: 0.6764  decode.d1.loss_mask: 0.3177  decode.d1.loss_dice: 0.4017  decode.d2.loss_cls: 0.5410  decode.d2.loss_mask: 0.2746  decode.d2.loss_dice: 0.3636  decode.d3.loss_cls: 0.4936  decode.d3.loss_mask: 0.2899  decode.d3.loss_dice: 0.3589  decode.d4.loss_cls: 0.4346  decode.d4.loss_mask: 0.2867  decode.d4.loss_dice: 0.3698  decode.d5.loss_cls: 0.4452  decode.d5.loss_mask: 0.2767  decode.d5.loss_dice: 0.3569  decode.d6.loss_cls: 0.4774  decode.d6.loss_mask: 0.2848  decode.d6.loss_dice: 0.3387  decode.d7.loss_cls: 0.5553  decode.d7.loss_mask: 0.3055  decode.d7.loss_dice: 0.3664  decode.d8.loss_cls: 0.5275  decode.d8.loss_mask: 0.3095  decode.d8.loss_dice: 0.3783
07/29 23:34:52 - mmengine - INFO - Iter(train) [ 4950/80000]  base_lr: 9.4415e-05 lr: 9.4415e-06  eta: 9:30:06  time: 0.4524  data_time: 0.0103  memory: 5245  grad_norm: 134.6005  loss: 15.7489  decode.loss_cls: 0.7589  decode.loss_mask: 0.3897  decode.loss_dice: 0.3497  decode.d0.loss_cls: 1.4565  decode.d0.loss_mask: 0.4009  decode.d0.loss_dice: 0.4050  decode.d1.loss_cls: 0.8961  decode.d1.loss_mask: 0.3610  decode.d1.loss_dice: 0.3695  decode.d2.loss_cls: 0.7596  decode.d2.loss_mask: 0.3656  decode.d2.loss_dice: 0.3606  decode.d3.loss_cls: 0.7313  decode.d3.loss_mask: 0.3593  decode.d3.loss_dice: 0.3462  decode.d4.loss_cls: 0.7390  decode.d4.loss_mask: 0.3749  decode.d4.loss_dice: 0.3480  decode.d5.loss_cls: 0.7143  decode.d5.loss_mask: 0.3698  decode.d5.loss_dice: 0.3547  decode.d6.loss_cls: 0.8265  decode.d6.loss_mask: 0.3497  decode.d6.loss_dice: 0.3379  decode.d7.loss_cls: 0.8274  decode.d7.loss_mask: 0.3563  decode.d7.loss_dice: 0.3641  decode.d8.loss_cls: 0.7591  decode.d8.loss_mask: 0.3680  decode.d8.loss_dice: 0.3495
07/29 23:35:15 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250729_225707
07/29 23:35:15 - mmengine - INFO - Iter(train) [ 5000/80000]  base_lr: 9.4358e-05 lr: 9.4358e-06  eta: 9:29:44  time: 0.4579  data_time: 0.0108  memory: 5266  grad_norm: 109.3346  loss: 12.6940  decode.loss_cls: 0.5004  decode.loss_mask: 0.3254  decode.loss_dice: 0.3211  decode.d0.loss_cls: 1.4944  decode.d0.loss_mask: 0.3188  decode.d0.loss_dice: 0.3466  decode.d1.loss_cls: 0.6798  decode.d1.loss_mask: 0.3300  decode.d1.loss_dice: 0.3497  decode.d2.loss_cls: 0.5438  decode.d2.loss_mask: 0.3277  decode.d2.loss_dice: 0.3255  decode.d3.loss_cls: 0.4764  decode.d3.loss_mask: 0.3104  decode.d3.loss_dice: 0.3058  decode.d4.loss_cls: 0.5214  decode.d4.loss_mask: 0.3096  decode.d4.loss_dice: 0.3051  decode.d5.loss_cls: 0.4944  decode.d5.loss_mask: 0.3161  decode.d5.loss_dice: 0.3102  decode.d6.loss_cls: 0.5303  decode.d6.loss_mask: 0.3173  decode.d6.loss_dice: 0.3147  decode.d7.loss_cls: 0.5193  decode.d7.loss_mask: 0.3208  decode.d7.loss_dice: 0.3244  decode.d8.loss_cls: 0.5039  decode.d8.loss_mask: 0.3253  decode.d8.loss_dice: 0.3256
07/29 23:35:15 - mmengine - INFO - Saving checkpoint at 5000 iterations
07/29 23:35:40 - mmengine - INFO - Iter(train) [ 5050/80000]  base_lr: 9.4301e-05 lr: 9.4301e-06  eta: 9:29:54  time: 0.4568  data_time: 0.0106  memory: 5281  grad_norm: 80.3539  loss: 13.4912  decode.loss_cls: 0.5414  decode.loss_mask: 0.3064  decode.loss_dice: 0.3824  decode.d0.loss_cls: 1.2507  decode.d0.loss_mask: 0.3236  decode.d0.loss_dice: 0.4091  decode.d1.loss_cls: 0.6616  decode.d1.loss_mask: 0.3066  decode.d1.loss_dice: 0.3849  decode.d2.loss_cls: 0.6302  decode.d2.loss_mask: 0.3034  decode.d2.loss_dice: 0.3841  decode.d3.loss_cls: 0.5647  decode.d3.loss_mask: 0.3054  decode.d3.loss_dice: 0.3796  decode.d4.loss_cls: 0.6024  decode.d4.loss_mask: 0.3125  decode.d4.loss_dice: 0.4055  decode.d5.loss_cls: 0.5811  decode.d5.loss_mask: 0.3099  decode.d5.loss_dice: 0.4047  decode.d6.loss_cls: 0.5651  decode.d6.loss_mask: 0.3121  decode.d6.loss_dice: 0.3799  decode.d7.loss_cls: 0.5683  decode.d7.loss_mask: 0.3091  decode.d7.loss_dice: 0.3717  decode.d8.loss_cls: 0.5387  decode.d8.loss_mask: 0.3109  decode.d8.loss_dice: 0.3852
07/29 23:36:03 - mmengine - INFO - Iter(train) [ 5100/80000]  base_lr: 9.4245e-05 lr: 9.4245e-06  eta: 9:29:32  time: 0.4559  data_time: 0.0104  memory: 5244  grad_norm: 157.6978  loss: 17.1917  decode.loss_cls: 0.8718  decode.loss_mask: 0.3185  decode.loss_dice: 0.4368  decode.d0.loss_cls: 1.3709  decode.d0.loss_mask: 0.3862  decode.d0.loss_dice: 0.4868  decode.d1.loss_cls: 1.0176  decode.d1.loss_mask: 0.3560  decode.d1.loss_dice: 0.4578  decode.d2.loss_cls: 0.8365  decode.d2.loss_mask: 0.3421  decode.d2.loss_dice: 0.4459  decode.d3.loss_cls: 0.8281  decode.d3.loss_mask: 0.3376  decode.d3.loss_dice: 0.4152  decode.d4.loss_cls: 0.8795  decode.d4.loss_mask: 0.3277  decode.d4.loss_dice: 0.4689  decode.d5.loss_cls: 0.8759  decode.d5.loss_mask: 0.3294  decode.d5.loss_dice: 0.4259  decode.d6.loss_cls: 0.9620  decode.d6.loss_mask: 0.3276  decode.d6.loss_dice: 0.4466  decode.d7.loss_cls: 0.9203  decode.d7.loss_mask: 0.3333  decode.d7.loss_dice: 0.4507  decode.d8.loss_cls: 0.8009  decode.d8.loss_mask: 0.3283  decode.d8.loss_dice: 0.4067
07/29 23:36:25 - mmengine - INFO - Iter(train) [ 5150/80000]  base_lr: 9.4188e-05 lr: 9.4188e-06  eta: 9:29:08  time: 0.4568  data_time: 0.0106  memory: 5243  grad_norm: 141.4691  loss: 12.8700  decode.loss_cls: 0.4884  decode.loss_mask: 0.3646  decode.loss_dice: 0.3431  decode.d0.loss_cls: 1.3376  decode.d0.loss_mask: 0.3796  decode.d0.loss_dice: 0.3820  decode.d1.loss_cls: 0.5650  decode.d1.loss_mask: 0.3749  decode.d1.loss_dice: 0.3502  decode.d2.loss_cls: 0.5120  decode.d2.loss_mask: 0.3625  decode.d2.loss_dice: 0.3376  decode.d3.loss_cls: 0.4458  decode.d3.loss_mask: 0.3595  decode.d3.loss_dice: 0.3308  decode.d4.loss_cls: 0.4501  decode.d4.loss_mask: 0.3628  decode.d4.loss_dice: 0.3286  decode.d5.loss_cls: 0.4763  decode.d5.loss_mask: 0.3624  decode.d5.loss_dice: 0.3285  decode.d6.loss_cls: 0.4731  decode.d6.loss_mask: 0.3676  decode.d6.loss_dice: 0.3395  decode.d7.loss_cls: 0.4917  decode.d7.loss_mask: 0.3704  decode.d7.loss_dice: 0.3409  decode.d8.loss_cls: 0.5177  decode.d8.loss_mask: 0.3709  decode.d8.loss_dice: 0.3558
07/29 23:36:48 - mmengine - INFO - Iter(train) [ 5200/80000]  base_lr: 9.4132e-05 lr: 9.4132e-06  eta: 9:28:44  time: 0.4533  data_time: 0.0103  memory: 5266  grad_norm: 129.4115  loss: 15.8656  decode.loss_cls: 0.5789  decode.loss_mask: 0.3429  decode.loss_dice: 0.4657  decode.d0.loss_cls: 1.5469  decode.d0.loss_mask: 0.3446  decode.d0.loss_dice: 0.5211  decode.d1.loss_cls: 0.8594  decode.d1.loss_mask: 0.3565  decode.d1.loss_dice: 0.4636  decode.d2.loss_cls: 0.7443  decode.d2.loss_mask: 0.3451  decode.d2.loss_dice: 0.4644  decode.d3.loss_cls: 0.6714  decode.d3.loss_mask: 0.3450  decode.d3.loss_dice: 0.4620  decode.d4.loss_cls: 0.6559  decode.d4.loss_mask: 0.3599  decode.d4.loss_dice: 0.4561  decode.d5.loss_cls: 0.6062  decode.d5.loss_mask: 0.3845  decode.d5.loss_dice: 0.4666  decode.d6.loss_cls: 0.7109  decode.d6.loss_mask: 0.3519  decode.d6.loss_dice: 0.4687  decode.d7.loss_cls: 0.6394  decode.d7.loss_mask: 0.3551  decode.d7.loss_dice: 0.4695  decode.d8.loss_cls: 0.6107  decode.d8.loss_mask: 0.3468  decode.d8.loss_dice: 0.4718
07/29 23:37:11 - mmengine - INFO - Iter(train) [ 5250/80000]  base_lr: 9.4075e-05 lr: 9.4075e-06  eta: 9:28:21  time: 0.4557  data_time: 0.0105  memory: 5244  grad_norm: 107.0379  loss: 11.3835  decode.loss_cls: 0.4623  decode.loss_mask: 0.3329  decode.loss_dice: 0.2932  decode.d0.loss_cls: 1.2876  decode.d0.loss_mask: 0.3369  decode.d0.loss_dice: 0.2940  decode.d1.loss_cls: 0.5401  decode.d1.loss_mask: 0.3174  decode.d1.loss_dice: 0.2779  decode.d2.loss_cls: 0.4035  decode.d2.loss_mask: 0.3211  decode.d2.loss_dice: 0.2745  decode.d3.loss_cls: 0.4221  decode.d3.loss_mask: 0.3177  decode.d3.loss_dice: 0.2823  decode.d4.loss_cls: 0.4409  decode.d4.loss_mask: 0.3131  decode.d4.loss_dice: 0.2802  decode.d5.loss_cls: 0.4203  decode.d5.loss_mask: 0.3134  decode.d5.loss_dice: 0.2761  decode.d6.loss_cls: 0.4193  decode.d6.loss_mask: 0.3223  decode.d6.loss_dice: 0.2812  decode.d7.loss_cls: 0.5073  decode.d7.loss_mask: 0.3162  decode.d7.loss_dice: 0.2768  decode.d8.loss_cls: 0.4549  decode.d8.loss_mask: 0.3236  decode.d8.loss_dice: 0.2743
07/29 23:37:34 - mmengine - INFO - Iter(train) [ 5300/80000]  base_lr: 9.4018e-05 lr: 9.4018e-06  eta: 9:27:58  time: 0.4534  data_time: 0.0104  memory: 5245  grad_norm: 133.2534  loss: 14.4562  decode.loss_cls: 0.5802  decode.loss_mask: 0.3842  decode.loss_dice: 0.3815  decode.d0.loss_cls: 1.5185  decode.d0.loss_mask: 0.4088  decode.d0.loss_dice: 0.3984  decode.d1.loss_cls: 0.7068  decode.d1.loss_mask: 0.3871  decode.d1.loss_dice: 0.3531  decode.d2.loss_cls: 0.5933  decode.d2.loss_mask: 0.3889  decode.d2.loss_dice: 0.3780  decode.d3.loss_cls: 0.6055  decode.d3.loss_mask: 0.3767  decode.d3.loss_dice: 0.3744  decode.d4.loss_cls: 0.5739  decode.d4.loss_mask: 0.3752  decode.d4.loss_dice: 0.3668  decode.d5.loss_cls: 0.5488  decode.d5.loss_mask: 0.3990  decode.d5.loss_dice: 0.3724  decode.d6.loss_cls: 0.5885  decode.d6.loss_mask: 0.3791  decode.d6.loss_dice: 0.3781  decode.d7.loss_cls: 0.5724  decode.d7.loss_mask: 0.3733  decode.d7.loss_dice: 0.3592  decode.d8.loss_cls: 0.5792  decode.d8.loss_mask: 0.3778  decode.d8.loss_dice: 0.3772
07/29 23:37:56 - mmengine - INFO - Iter(train) [ 5350/80000]  base_lr: 9.3962e-05 lr: 9.3962e-06  eta: 9:27:34  time: 0.4550  data_time: 0.0103  memory: 5281  grad_norm: 103.3519  loss: 12.2198  decode.loss_cls: 0.5218  decode.loss_mask: 0.3075  decode.loss_dice: 0.2971  decode.d0.loss_cls: 1.3915  decode.d0.loss_mask: 0.3219  decode.d0.loss_dice: 0.3688  decode.d1.loss_cls: 0.6931  decode.d1.loss_mask: 0.2732  decode.d1.loss_dice: 0.3137  decode.d2.loss_cls: 0.5356  decode.d2.loss_mask: 0.2734  decode.d2.loss_dice: 0.3210  decode.d3.loss_cls: 0.4488  decode.d3.loss_mask: 0.2985  decode.d3.loss_dice: 0.3262  decode.d4.loss_cls: 0.4922  decode.d4.loss_mask: 0.3074  decode.d4.loss_dice: 0.3167  decode.d5.loss_cls: 0.4917  decode.d5.loss_mask: 0.2729  decode.d5.loss_dice: 0.2954  decode.d6.loss_cls: 0.4915  decode.d6.loss_mask: 0.2967  decode.d6.loss_dice: 0.3021  decode.d7.loss_cls: 0.5132  decode.d7.loss_mask: 0.3049  decode.d7.loss_dice: 0.3021  decode.d8.loss_cls: 0.5747  decode.d8.loss_mask: 0.2684  decode.d8.loss_dice: 0.2976
07/29 23:38:19 - mmengine - INFO - Iter(train) [ 5400/80000]  base_lr: 9.3905e-05 lr: 9.3905e-06  eta: 9:27:10  time: 0.4553  data_time: 0.0105  memory: 5266  grad_norm: 102.6455  loss: 13.7937  decode.loss_cls: 0.6569  decode.loss_mask: 0.3190  decode.loss_dice: 0.3418  decode.d0.loss_cls: 1.5410  decode.d0.loss_mask: 0.3160  decode.d0.loss_dice: 0.3439  decode.d1.loss_cls: 0.7866  decode.d1.loss_mask: 0.3207  decode.d1.loss_dice: 0.3356  decode.d2.loss_cls: 0.6477  decode.d2.loss_mask: 0.3043  decode.d2.loss_dice: 0.3466  decode.d3.loss_cls: 0.5478  decode.d3.loss_mask: 0.3121  decode.d3.loss_dice: 0.3715  decode.d4.loss_cls: 0.5683  decode.d4.loss_mask: 0.3219  decode.d4.loss_dice: 0.3459  decode.d5.loss_cls: 0.6173  decode.d5.loss_mask: 0.3216  decode.d5.loss_dice: 0.3282  decode.d6.loss_cls: 0.6305  decode.d6.loss_mask: 0.3090  decode.d6.loss_dice: 0.3363  decode.d7.loss_cls: 0.6253  decode.d7.loss_mask: 0.3056  decode.d7.loss_dice: 0.3343  decode.d8.loss_cls: 0.6210  decode.d8.loss_mask: 0.3075  decode.d8.loss_dice: 0.3294
07/29 23:38:42 - mmengine - INFO - Iter(train) [ 5450/80000]  base_lr: 9.3848e-05 lr: 9.3848e-06  eta: 9:26:45  time: 0.4516  data_time: 0.0102  memory: 5245  grad_norm: 122.5693  loss: 16.5663  decode.loss_cls: 0.8343  decode.loss_mask: 0.3252  decode.loss_dice: 0.4636  decode.d0.loss_cls: 1.4880  decode.d0.loss_mask: 0.3414  decode.d0.loss_dice: 0.4714  decode.d1.loss_cls: 1.0280  decode.d1.loss_mask: 0.3232  decode.d1.loss_dice: 0.4448  decode.d2.loss_cls: 0.7221  decode.d2.loss_mask: 0.3331  decode.d2.loss_dice: 0.4532  decode.d3.loss_cls: 0.7666  decode.d3.loss_mask: 0.3149  decode.d3.loss_dice: 0.4501  decode.d4.loss_cls: 0.7968  decode.d4.loss_mask: 0.3188  decode.d4.loss_dice: 0.4515  decode.d5.loss_cls: 0.7987  decode.d5.loss_mask: 0.3154  decode.d5.loss_dice: 0.4067  decode.d6.loss_cls: 0.8325  decode.d6.loss_mask: 0.3092  decode.d6.loss_dice: 0.3893  decode.d7.loss_cls: 0.8816  decode.d7.loss_mask: 0.3253  decode.d7.loss_dice: 0.4392  decode.d8.loss_cls: 0.8270  decode.d8.loss_mask: 0.3251  decode.d8.loss_dice: 0.3892
07/29 23:39:05 - mmengine - INFO - Iter(train) [ 5500/80000]  base_lr: 9.3792e-05 lr: 9.3792e-06  eta: 9:26:23  time: 0.4580  data_time: 0.0105  memory: 5244  grad_norm: 114.4545  loss: 12.9502  decode.loss_cls: 0.5250  decode.loss_mask: 0.3322  decode.loss_dice: 0.3695  decode.d0.loss_cls: 1.3035  decode.d0.loss_mask: 0.3505  decode.d0.loss_dice: 0.4163  decode.d1.loss_cls: 0.6103  decode.d1.loss_mask: 0.3324  decode.d1.loss_dice: 0.3616  decode.d2.loss_cls: 0.5020  decode.d2.loss_mask: 0.3300  decode.d2.loss_dice: 0.3172  decode.d3.loss_cls: 0.5556  decode.d3.loss_mask: 0.3234  decode.d3.loss_dice: 0.3228  decode.d4.loss_cls: 0.5559  decode.d4.loss_mask: 0.3384  decode.d4.loss_dice: 0.3703  decode.d5.loss_cls: 0.5301  decode.d5.loss_mask: 0.3617  decode.d5.loss_dice: 0.3412  decode.d6.loss_cls: 0.5212  decode.d6.loss_mask: 0.3268  decode.d6.loss_dice: 0.3005  decode.d7.loss_cls: 0.5382  decode.d7.loss_mask: 0.3329  decode.d7.loss_dice: 0.3506  decode.d8.loss_cls: 0.5046  decode.d8.loss_mask: 0.3286  decode.d8.loss_dice: 0.2970
07/29 23:39:27 - mmengine - INFO - Iter(train) [ 5550/80000]  base_lr: 9.3735e-05 lr: 9.3735e-06  eta: 9:25:59  time: 0.4540  data_time: 0.0104  memory: 5307  grad_norm: 143.6220  loss: 14.3825  decode.loss_cls: 0.4568  decode.loss_mask: 0.5183  decode.loss_dice: 0.4220  decode.d0.loss_cls: 1.3976  decode.d0.loss_mask: 0.4855  decode.d0.loss_dice: 0.4024  decode.d1.loss_cls: 0.6279  decode.d1.loss_mask: 0.4605  decode.d1.loss_dice: 0.3995  decode.d2.loss_cls: 0.5191  decode.d2.loss_mask: 0.4563  decode.d2.loss_dice: 0.3710  decode.d3.loss_cls: 0.4352  decode.d3.loss_mask: 0.4644  decode.d3.loss_dice: 0.3867  decode.d4.loss_cls: 0.4317  decode.d4.loss_mask: 0.4483  decode.d4.loss_dice: 0.3972  decode.d5.loss_cls: 0.4567  decode.d5.loss_mask: 0.4506  decode.d5.loss_dice: 0.3612  decode.d6.loss_cls: 0.4441  decode.d6.loss_mask: 0.4652  decode.d6.loss_dice: 0.3786  decode.d7.loss_cls: 0.5166  decode.d7.loss_mask: 0.4590  decode.d7.loss_dice: 0.3820  decode.d8.loss_cls: 0.4821  decode.d8.loss_mask: 0.4932  decode.d8.loss_dice: 0.4127
07/29 23:39:50 - mmengine - INFO - Iter(train) [ 5600/80000]  base_lr: 9.3678e-05 lr: 9.3678e-06  eta: 9:25:36  time: 0.4546  data_time: 0.0104  memory: 5264  grad_norm: 128.5604  loss: 12.9950  decode.loss_cls: 0.6572  decode.loss_mask: 0.2863  decode.loss_dice: 0.3507  decode.d0.loss_cls: 1.4155  decode.d0.loss_mask: 0.2750  decode.d0.loss_dice: 0.3727  decode.d1.loss_cls: 0.7252  decode.d1.loss_mask: 0.2801  decode.d1.loss_dice: 0.3557  decode.d2.loss_cls: 0.6243  decode.d2.loss_mask: 0.2687  decode.d2.loss_dice: 0.3515  decode.d3.loss_cls: 0.5584  decode.d3.loss_mask: 0.2676  decode.d3.loss_dice: 0.3429  decode.d4.loss_cls: 0.5597  decode.d4.loss_mask: 0.2706  decode.d4.loss_dice: 0.3302  decode.d5.loss_cls: 0.6133  decode.d5.loss_mask: 0.2647  decode.d5.loss_dice: 0.3213  decode.d6.loss_cls: 0.4969  decode.d6.loss_mask: 0.2681  decode.d6.loss_dice: 0.3441  decode.d7.loss_cls: 0.5761  decode.d7.loss_mask: 0.2618  decode.d7.loss_dice: 0.3208  decode.d8.loss_cls: 0.6177  decode.d8.loss_mask: 0.2706  decode.d8.loss_dice: 0.3472
07/29 23:40:13 - mmengine - INFO - Iter(train) [ 5650/80000]  base_lr: 9.3622e-05 lr: 9.3622e-06  eta: 9:25:12  time: 0.4559  data_time: 0.0105  memory: 5266  grad_norm: 121.5040  loss: 15.2319  decode.loss_cls: 0.6416  decode.loss_mask: 0.3018  decode.loss_dice: 0.4313  decode.d0.loss_cls: 1.6160  decode.d0.loss_mask: 0.2883  decode.d0.loss_dice: 0.4659  decode.d1.loss_cls: 0.9350  decode.d1.loss_mask: 0.2966  decode.d1.loss_dice: 0.4507  decode.d2.loss_cls: 0.7438  decode.d2.loss_mask: 0.2901  decode.d2.loss_dice: 0.4558  decode.d3.loss_cls: 0.6801  decode.d3.loss_mask: 0.2884  decode.d3.loss_dice: 0.4167  decode.d4.loss_cls: 0.6371  decode.d4.loss_mask: 0.2910  decode.d4.loss_dice: 0.4228  decode.d5.loss_cls: 0.6191  decode.d5.loss_mask: 0.2928  decode.d5.loss_dice: 0.4241  decode.d6.loss_cls: 0.6153  decode.d6.loss_mask: 0.2887  decode.d6.loss_dice: 0.4354  decode.d7.loss_cls: 0.6734  decode.d7.loss_mask: 0.2886  decode.d7.loss_dice: 0.4393  decode.d8.loss_cls: 0.7533  decode.d8.loss_mask: 0.3016  decode.d8.loss_dice: 0.4472
07/29 23:40:35 - mmengine - INFO - Iter(train) [ 5700/80000]  base_lr: 9.3565e-05 lr: 9.3565e-06  eta: 9:24:47  time: 0.4492  data_time: 0.0100  memory: 5266  grad_norm: 106.7043  loss: 14.9618  decode.loss_cls: 0.5529  decode.loss_mask: 0.3687  decode.loss_dice: 0.4107  decode.d0.loss_cls: 1.5280  decode.d0.loss_mask: 0.4172  decode.d0.loss_dice: 0.5294  decode.d1.loss_cls: 0.6346  decode.d1.loss_mask: 0.3697  decode.d1.loss_dice: 0.4156  decode.d2.loss_cls: 0.6609  decode.d2.loss_mask: 0.3680  decode.d2.loss_dice: 0.3951  decode.d3.loss_cls: 0.5558  decode.d3.loss_mask: 0.3653  decode.d3.loss_dice: 0.3876  decode.d4.loss_cls: 0.6277  decode.d4.loss_mask: 0.3597  decode.d4.loss_dice: 0.3822  decode.d5.loss_cls: 0.6184  decode.d5.loss_mask: 0.3740  decode.d5.loss_dice: 0.4039  decode.d6.loss_cls: 0.6256  decode.d6.loss_mask: 0.3685  decode.d6.loss_dice: 0.3801  decode.d7.loss_cls: 0.6681  decode.d7.loss_mask: 0.3644  decode.d7.loss_dice: 0.3961  decode.d8.loss_cls: 0.6642  decode.d8.loss_mask: 0.3630  decode.d8.loss_dice: 0.4064
07/29 23:40:58 - mmengine - INFO - Iter(train) [ 5750/80000]  base_lr: 9.3508e-05 lr: 9.3508e-06  eta: 9:24:24  time: 0.4511  data_time: 0.0099  memory: 5266  grad_norm: 141.2258  loss: 15.4668  decode.loss_cls: 0.8637  decode.loss_mask: 0.2901  decode.loss_dice: 0.3734  decode.d0.loss_cls: 1.6402  decode.d0.loss_mask: 0.2616  decode.d0.loss_dice: 0.4524  decode.d1.loss_cls: 0.9711  decode.d1.loss_mask: 0.2548  decode.d1.loss_dice: 0.3706  decode.d2.loss_cls: 0.8368  decode.d2.loss_mask: 0.2729  decode.d2.loss_dice: 0.3921  decode.d3.loss_cls: 0.7294  decode.d3.loss_mask: 0.2689  decode.d3.loss_dice: 0.3636  decode.d4.loss_cls: 0.7348  decode.d4.loss_mask: 0.2643  decode.d4.loss_dice: 0.3746  decode.d5.loss_cls: 0.7304  decode.d5.loss_mask: 0.2962  decode.d5.loss_dice: 0.3853  decode.d6.loss_cls: 0.6816  decode.d6.loss_mask: 0.2704  decode.d6.loss_dice: 0.3486  decode.d7.loss_cls: 0.8328  decode.d7.loss_mask: 0.2937  decode.d7.loss_dice: 0.3682  decode.d8.loss_cls: 0.8639  decode.d8.loss_mask: 0.2894  decode.d8.loss_dice: 0.3912
07/29 23:41:21 - mmengine - INFO - Iter(train) [ 5800/80000]  base_lr: 9.3452e-05 lr: 9.3452e-06  eta: 9:23:59  time: 0.4515  data_time: 0.0101  memory: 5264  grad_norm: 164.4463  loss: 11.3244  decode.loss_cls: 0.3885  decode.loss_mask: 0.3667  decode.loss_dice: 0.3386  decode.d0.loss_cls: 1.1871  decode.d0.loss_mask: 0.3523  decode.d0.loss_dice: 0.3296  decode.d1.loss_cls: 0.4176  decode.d1.loss_mask: 0.3494  decode.d1.loss_dice: 0.3515  decode.d2.loss_cls: 0.3207  decode.d2.loss_mask: 0.3633  decode.d2.loss_dice: 0.3418  decode.d3.loss_cls: 0.3213  decode.d3.loss_mask: 0.3606  decode.d3.loss_dice: 0.3224  decode.d4.loss_cls: 0.3356  decode.d4.loss_mask: 0.3714  decode.d4.loss_dice: 0.3184  decode.d5.loss_cls: 0.3352  decode.d5.loss_mask: 0.3681  decode.d5.loss_dice: 0.3176  decode.d6.loss_cls: 0.3340  decode.d6.loss_mask: 0.3627  decode.d6.loss_dice: 0.3276  decode.d7.loss_cls: 0.3486  decode.d7.loss_mask: 0.3659  decode.d7.loss_dice: 0.3369  decode.d8.loss_cls: 0.3885  decode.d8.loss_mask: 0.3719  decode.d8.loss_dice: 0.3305
07/29 23:41:43 - mmengine - INFO - Iter(train) [ 5850/80000]  base_lr: 9.3395e-05 lr: 9.3395e-06  eta: 9:23:33  time: 0.4488  data_time: 0.0099  memory: 5244  grad_norm: 75.8702  loss: 10.3894  decode.loss_cls: 0.3433  decode.loss_mask: 0.2756  decode.loss_dice: 0.2888  decode.d0.loss_cls: 1.3048  decode.d0.loss_mask: 0.2856  decode.d0.loss_dice: 0.3537  decode.d1.loss_cls: 0.4535  decode.d1.loss_mask: 0.2721  decode.d1.loss_dice: 0.3169  decode.d2.loss_cls: 0.3846  decode.d2.loss_mask: 0.2675  decode.d2.loss_dice: 0.3153  decode.d3.loss_cls: 0.4048  decode.d3.loss_mask: 0.2625  decode.d3.loss_dice: 0.2851  decode.d4.loss_cls: 0.3769  decode.d4.loss_mask: 0.2657  decode.d4.loss_dice: 0.2874  decode.d5.loss_cls: 0.4163  decode.d5.loss_mask: 0.2682  decode.d5.loss_dice: 0.2979  decode.d6.loss_cls: 0.3235  decode.d6.loss_mask: 0.2667  decode.d6.loss_dice: 0.2825  decode.d7.loss_cls: 0.3437  decode.d7.loss_mask: 0.2678  decode.d7.loss_dice: 0.2955  decode.d8.loss_cls: 0.3279  decode.d8.loss_mask: 0.2672  decode.d8.loss_dice: 0.2881
07/29 23:42:06 - mmengine - INFO - Iter(train) [ 5900/80000]  base_lr: 9.3338e-05 lr: 9.3338e-06  eta: 9:23:06  time: 0.4481  data_time: 0.0098  memory: 5266  grad_norm: 178.8482  loss: 11.5903  decode.loss_cls: 0.4118  decode.loss_mask: 0.3069  decode.loss_dice: 0.3070  decode.d0.loss_cls: 1.2721  decode.d0.loss_mask: 0.3385  decode.d0.loss_dice: 0.3519  decode.d1.loss_cls: 0.4651  decode.d1.loss_mask: 0.3223  decode.d1.loss_dice: 0.3333  decode.d2.loss_cls: 0.4224  decode.d2.loss_mask: 0.3249  decode.d2.loss_dice: 0.3557  decode.d3.loss_cls: 0.4364  decode.d3.loss_mask: 0.3032  decode.d3.loss_dice: 0.3263  decode.d4.loss_cls: 0.4334  decode.d4.loss_mask: 0.3271  decode.d4.loss_dice: 0.3289  decode.d5.loss_cls: 0.3841  decode.d5.loss_mask: 0.3209  decode.d5.loss_dice: 0.3179  decode.d6.loss_cls: 0.3675  decode.d6.loss_mask: 0.3422  decode.d6.loss_dice: 0.3184  decode.d7.loss_cls: 0.4511  decode.d7.loss_mask: 0.3175  decode.d7.loss_dice: 0.2974  decode.d8.loss_cls: 0.5050  decode.d8.loss_mask: 0.3025  decode.d8.loss_dice: 0.2985
07/29 23:42:29 - mmengine - INFO - Iter(train) [ 5950/80000]  base_lr: 9.3282e-05 lr: 9.3282e-06  eta: 9:22:42  time: 0.4543  data_time: 0.0105  memory: 5266  grad_norm: 81.8768  loss: 11.5858  decode.loss_cls: 0.4001  decode.loss_mask: 0.3024  decode.loss_dice: 0.3072  decode.d0.loss_cls: 1.2197  decode.d0.loss_mask: 0.3223  decode.d0.loss_dice: 0.3531  decode.d1.loss_cls: 0.5803  decode.d1.loss_mask: 0.3054  decode.d1.loss_dice: 0.3091  decode.d2.loss_cls: 0.4304  decode.d2.loss_mask: 0.3056  decode.d2.loss_dice: 0.3335  decode.d3.loss_cls: 0.4413  decode.d3.loss_mask: 0.3120  decode.d3.loss_dice: 0.3383  decode.d4.loss_cls: 0.3883  decode.d4.loss_mask: 0.2988  decode.d4.loss_dice: 0.3572  decode.d5.loss_cls: 0.4299  decode.d5.loss_mask: 0.3015  decode.d5.loss_dice: 0.3380  decode.d6.loss_cls: 0.4399  decode.d6.loss_mask: 0.2997  decode.d6.loss_dice: 0.3167  decode.d7.loss_cls: 0.4508  decode.d7.loss_mask: 0.3102  decode.d7.loss_dice: 0.3255  decode.d8.loss_cls: 0.4247  decode.d8.loss_mask: 0.3023  decode.d8.loss_dice: 0.3416
07/29 23:42:51 - mmengine - INFO - Exp name: mask2former_r50_8xb2-80k_MYDATA-512x1024_20250729_225707
07/29 23:42:51 - mmengine - INFO - Iter(train) [ 6000/80000]  base_lr: 9.3225e-05 lr: 9.3225e-06  eta: 9:22:18  time: 0.4548  data_time: 0.0104  memory: 5245  grad_norm: 116.7555  loss: 13.9382  decode.loss_cls: 0.4500  decode.loss_mask: 0.3926  decode.loss_dice: 0.4048  decode.d0.loss_cls: 1.3549  decode.d0.loss_mask: 0.4387  decode.d0.loss_dice: 0.4510  decode.d1.loss_cls: 0.6881  decode.d1.loss_mask: 0.4114  decode.d1.loss_dice: 0.4337  decode.d2.loss_cls: 0.5333  decode.d2.loss_mask: 0.4082  decode.d2.loss_dice: 0.3800  decode.d3.loss_cls: 0.4988  decode.d3.loss_mask: 0.3986  decode.d3.loss_dice: 0.3869  decode.d4.loss_cls: 0.4625  decode.d4.loss_mask: 0.3960  decode.d4.loss_dice: 0.3946  decode.d5.loss_cls: 0.4556  decode.d5.loss_mask: 0.3824  decode.d5.loss_dice: 0.3828  decode.d6.loss_cls: 0.4434  decode.d6.loss_mask: 0.3896  decode.d6.loss_dice: 0.4001  decode.d7.loss_cls: 0.3949  decode.d7.loss_mask: 0.4343  decode.d7.loss_dice: 0.4297  decode.d8.loss_cls: 0.4761  decode.d8.loss_mask: 0.4289  decode.d8.loss_dice: 0.4364
07/29 23:43:14 - mmengine - INFO - Iter(train) [ 6050/80000]  base_lr: 9.3168e-05 lr: 9.3168e-06  eta: 9:21:55  time: 0.4577  data_time: 0.0105  memory: 5245  grad_norm: 147.6066  loss: 11.9447  decode.loss_cls: 0.4658  decode.loss_mask: 0.3028  decode.loss_dice: 0.3272  decode.d0.loss_cls: 1.2722  decode.d0.loss_mask: 0.3400  decode.d0.loss_dice: 0.3582  decode.d1.loss_cls: 0.5195  decode.d1.loss_mask: 0.3273  decode.d1.loss_dice: 0.3512  decode.d2.loss_cls: 0.4383  decode.d2.loss_mask: 0.3184  decode.d2.loss_dice: 0.3647  decode.d3.loss_cls: 0.4238  decode.d3.loss_mask: 0.2992  decode.d3.loss_dice: 0.3382  decode.d4.loss_cls: 0.4096  decode.d4.loss_mask: 0.3012  decode.d4.loss_dice: 0.3415  decode.d5.loss_cls: 0.4455  decode.d5.loss_mask: 0.3062  decode.d5.loss_dice: 0.3447  decode.d6.loss_cls: 0.4457  decode.d6.loss_mask: 0.3092  decode.d6.loss_dice: 0.3335  decode.d7.loss_cls: 0.4618  decode.d7.loss_mask: 0.3055  decode.d7.loss_dice: 0.3312  decode.d8.loss_cls: 0.4535  decode.d8.loss_mask: 0.3391  decode.d8.loss_dice: 0.3697
07/29 23:43:37 - mmengine - INFO - Iter(train) [ 6100/80000]  base_lr: 9.3112e-05 lr: 9.3112e-06  eta: 9:21:32  time: 0.4537  data_time: 0.0102  memory: 5266  grad_norm: 161.6676  loss: 13.7432  decode.loss_cls: 0.6475  decode.loss_mask: 0.2956  decode.loss_dice: 0.3849  decode.d0.loss_cls: 1.6016  decode.d0.loss_mask: 0.2705  decode.d0.loss_dice: 0.4371  decode.d1.loss_cls: 0.6129  decode.d1.loss_mask: 0.3122  decode.d1.loss_dice: 0.3822  decode.d2.loss_cls: 0.5541  decode.d2.loss_mask: 0.2947  decode.d2.loss_dice: 0.3950  decode.d3.loss_cls: 0.5275  decode.d3.loss_mask: 0.2571  decode.d3.loss_dice: 0.3853  decode.d4.loss_cls: 0.5362  decode.d4.loss_mask: 0.3107  decode.d4.loss_dice: 0.3714  decode.d5.loss_cls: 0.5526  decode.d5.loss_mask: 0.3064  decode.d5.loss_dice: 0.3848  decode.d6.loss_cls: 0.6031  decode.d6.loss_mask: 0.3143  decode.d6.loss_dice: 0.3854  decode.d7.loss_cls: 0.6064  decode.d7.loss_mask: 0.3217  decode.d7.loss_dice: 0.4140  decode.d8.loss_cls: 0.6152  decode.d8.loss_mask: 0.2854  decode.d8.loss_dice: 0.3774
